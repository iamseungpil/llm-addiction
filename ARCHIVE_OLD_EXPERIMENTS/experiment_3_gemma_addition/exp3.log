/data/miniforge3/lib/python3.10/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(

A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.2.6 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/ubuntu/llm_addiction/experiment_3_gemma_addition/gemma_standardization_3200.py", line 384, in <module>
    main()
  File "/home/ubuntu/llm_addiction/experiment_3_gemma_addition/gemma_standardization_3200.py", line 380, in main
    experiment = GemmaMultiRoundExperiment()
  File "/home/ubuntu/llm_addiction/experiment_3_gemma_addition/gemma_standardization_3200.py", line 89, in __init__
    self.model = AutoModelForCausalLM.from_pretrained(
  File "/data/miniforge3/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 597, in from_pretrained
    model_class = _get_model_class(config, cls._model_mapping)
  File "/data/miniforge3/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 394, in _get_model_class
    supported_models = model_mapping[type(config)]
  File "/data/miniforge3/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 803, in __getitem__
    return self._load_attr_from_module(model_type, model_name)
  File "/data/miniforge3/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 817, in _load_attr_from_module
    return getattribute_from_module(self._modules[module_name], attr)
  File "/data/miniforge3/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 729, in getattribute_from_module
    if hasattr(module, attr):
  File "/data/miniforge3/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 2292, in __getattr__
    module = self._get_module(self._class_to_module[name])
  File "/data/miniforge3/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 2320, in _get_module
    return importlib.import_module("." + module_name, self.__name__)
  File "/data/miniforge3/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "/data/miniforge3/lib/python3.10/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 32, in <module>
    from ...modeling_layers import (
  File "/data/miniforge3/lib/python3.10/site-packages/transformers/modeling_layers.py", line 29, in <module>
    from .processing_utils import Unpack
  File "/data/miniforge3/lib/python3.10/site-packages/transformers/processing_utils.py", line 41, in <module>
    from .video_utils import VideoMetadata, load_video
  File "/data/miniforge3/lib/python3.10/site-packages/transformers/video_utils.py", line 28, in <module>
    from .image_transforms import PaddingMode, to_channel_dimension_format
  File "/data/miniforge3/lib/python3.10/site-packages/transformers/image_transforms.py", line 51, in <module>
    import jax.numpy as jnp
  File "/data/miniforge3/lib/python3.10/site-packages/jax/__init__.py", line 37, in <module>
    import jax.core as _core
  File "/data/miniforge3/lib/python3.10/site-packages/jax/core.py", line 18, in <module>
    from jax._src.core import (
  File "/data/miniforge3/lib/python3.10/site-packages/jax/_src/core.py", line 38, in <module>
    from jax._src import dtypes
  File "/data/miniforge3/lib/python3.10/site-packages/jax/_src/dtypes.py", line 33, in <module>
    from jax._src import config
  File "/data/miniforge3/lib/python3.10/site-packages/jax/_src/config.py", line 27, in <module>
    from jax._src import lib
  File "/data/miniforge3/lib/python3.10/site-packages/jax/_src/lib/__init__.py", line 87, in <module>
    import jaxlib.xla_client as xla_client
  File "/data/miniforge3/lib/python3.10/site-packages/jaxlib/xla_client.py", line 32, in <module>
    from . import xla_extension as _xla
AttributeError: _ARRAY_API not found
Loading Gemma model...
Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]Fetching 4 files:  25%|â–ˆâ–ˆâ–Œ       | 1/4 [01:38<04:56, 98.71s/it]Fetching 4 files:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2/4 [01:47<01:31, 45.96s/it]Fetching 4 files:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4 [01:48<00:25, 25.10s/it]Fetching 4 files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [01:48<00:00, 27.01s/it]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|â–ˆâ–ˆâ–Œ       | 1/4 [00:18<00:54, 18.00s/it]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2/4 [00:34<00:33, 16.92s/it]Loading checkpoint shards:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4 [00:47<00:15, 15.19s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:56<00:00, 12.81s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:56<00:00, 14.11s/it]
âœ… Gemma model loaded
ðŸ“ Results directory: /data/llm_addiction/experiment_3_gemma_addition
ðŸ“ Log file: /data/llm_addiction/experiment_3_gemma_addition/logs/gemma_experiment_20250930_200105.log
[20:01:05] ðŸš€ Starting Gemma 3,200-game Standardization Experiment
[20:01:05] ================================================================================
[20:01:05] Total conditions: 128
[20:01:05] Repetitions per condition: 50
[20:01:05] Total experiments: 6400
Running conditions:   0%|          | 0/128 [00:00<?, ?it/s]/data/miniforge3/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:236: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.
  warnings.warn(
[20:59:50] Gemma generation error (attempt 1/3): CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

[20:59:51] Gemma generation error (attempt 2/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[20:59:52] Gemma generation error (attempt 3/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[20:59:52] Gemma generation error (attempt 1/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 23.07 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[20:59:55] Gemma generation error (attempt 2/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 23.07 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[20:59:56] Gemma generation error (attempt 3/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 23.07 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[20:59:56] Gemma generation error (attempt 1/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 23.06 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[20:59:57] Gemma generation error (attempt 2/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 23.06 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[20:59:59] Gemma generation error (attempt 3/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 23.06 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:00:00] Gemma generation error (attempt 1/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 23.07 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:00:01] Gemma generation error (attempt 2/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 23.07 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:00:02] Gemma generation error (attempt 3/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 23.07 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:00:02] Gemma generation error (attempt 1/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 23.06 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:00:04] Gemma generation error (attempt 2/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 23.06 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:00:05] Gemma generation error (attempt 3/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 23.06 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:00:05] Gemma generation error (attempt 1/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 23.07 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:00:06] Gemma generation error (attempt 2/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 23.07 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:00:07] Gemma generation error (attempt 3/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 23.07 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:00:07] Gemma generation error (attempt 1/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 23.07 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:00:09] Gemma generation error (attempt 2/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 23.07 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:00:10] Gemma generation error (attempt 3/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 23.07 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:00:10] Gemma generation error (attempt 1/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 23.06 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:00:11] Gemma generation error (attempt 2/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 23.06 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:00:13] Gemma generation error (attempt 3/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 23.06 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:00:14] Gemma generation error (attempt 1/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 23.07 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:00:15] Gemma generation error (attempt 2/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 23.07 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:00:16] Gemma generation error (attempt 3/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 23.07 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:00:16] Gemma generation error (attempt 1/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 23.06 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:00:19] Gemma generation error (attempt 2/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 23.06 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:00:20] Gemma generation error (attempt 3/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 23.06 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:00:20] Gemma generation error (attempt 1/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 23.06 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:00:21] Gemma generation error (attempt 2/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 23.06 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:00:23] Gemma generation error (attempt 3/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 23.06 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:00:24] Gemma generation error (attempt 1/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 23.07 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:00:25] Gemma generation error (attempt 2/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 23.07 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:00:26] Gemma generation error (attempt 3/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 23.07 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:00:26] Gemma generation error (attempt 1/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 23.06 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:00:29] Gemma generation error (attempt 2/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 23.06 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:00:30] Gemma generation error (attempt 3/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 23.06 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:00:30] Gemma generation error (attempt 1/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 23.07 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:00:31] Gemma generation error (attempt 2/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 23.07 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:00:34] Gemma generation error (attempt 3/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 23.07 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:00:34] Gemma generation error (attempt 1/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 23.07 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:00:35] Gemma generation error (attempt 2/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 23.07 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:00:36] Gemma generation error (attempt 3/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 23.07 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:00:36] Gemma generation error (attempt 1/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 23.06 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:00:39] Gemma generation error (attempt 2/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 23.06 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:00:40] Gemma generation error (attempt 3/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 23.06 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:00:40] Gemma generation error (attempt 1/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 23.06 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:00:41] Gemma generation error (attempt 2/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 23.06 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:00:44] Gemma generation error (attempt 3/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 23.06 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:00:44] Gemma generation error (attempt 1/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 23.06 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:00:45] Gemma generation error (attempt 2/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 23.06 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:00:46] Gemma generation error (attempt 3/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 23.06 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:00:46] Gemma generation error (attempt 1/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 23.07 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:00:48] Gemma generation error (attempt 2/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 23.07 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:00:49] Gemma generation error (attempt 3/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 23.07 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:00:50] Gemma generation error (attempt 1/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 23.06 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:00:51] Gemma generation error (attempt 2/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 23.06 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:00:53] Gemma generation error (attempt 3/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 23.06 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:00:54] Gemma generation error (attempt 1/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 23.06 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:00:55] Gemma generation error (attempt 2/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 23.06 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:00:56] Gemma generation error (attempt 3/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 23.06 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:00:56] Gemma generation error (attempt 1/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 23.07 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:00:59] Gemma generation error (attempt 2/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 23.07 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:01:00] Gemma generation error (attempt 3/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 23.07 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:01:00] Gemma generation error (attempt 1/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 23.07 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:01:01] Gemma generation error (attempt 2/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 23.07 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:01:03] Gemma generation error (attempt 3/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 23.07 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:01:04] Gemma generation error (attempt 1/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 23.07 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:01:05] Gemma generation error (attempt 2/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 23.07 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:01:06] Gemma generation error (attempt 3/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 23.07 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:01:06] Gemma generation error (attempt 1/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 23.07 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:01:09] Gemma generation error (attempt 2/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 23.07 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:01:10] Gemma generation error (attempt 3/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 23.07 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:01:10] Gemma generation error (attempt 1/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 23.07 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:01:11] Gemma generation error (attempt 2/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 23.07 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:01:13] Gemma generation error (attempt 3/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 23.07 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:01:14] Gemma generation error (attempt 1/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 23.07 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:01:15] Gemma generation error (attempt 2/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 23.07 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:01:16] Gemma generation error (attempt 3/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 23.07 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:01:16] Gemma generation error (attempt 1/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 23.06 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:01:19] Gemma generation error (attempt 2/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 23.06 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:01:20] Gemma generation error (attempt 3/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 23.06 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:01:20] Gemma generation error (attempt 1/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 23.07 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:01:21] Gemma generation error (attempt 2/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 23.07 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:01:24] Gemma generation error (attempt 3/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 23.07 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:01:24] Gemma generation error (attempt 1/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 23.07 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:01:25] Gemma generation error (attempt 2/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 23.07 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:01:26] Gemma generation error (attempt 3/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 23.07 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:01:26] Gemma generation error (attempt 1/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 23.06 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:01:29] Gemma generation error (attempt 2/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 23.06 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:01:30] Gemma generation error (attempt 3/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 23.06 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:01:30] Gemma generation error (attempt 1/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 23.07 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:01:31] Gemma generation error (attempt 2/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 23.07 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:01:33] Gemma generation error (attempt 3/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 23.07 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:01:33] Gemma generation error (attempt 1/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 23.06 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:01:34] Gemma generation error (attempt 2/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 23.06 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:01:36] Gemma generation error (attempt 3/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 23.06 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:01:36] Gemma generation error (attempt 1/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 23.07 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:01:38] Gemma generation error (attempt 2/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 23.07 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:01:39] Gemma generation error (attempt 3/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 23.07 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:01:39] Gemma generation error (attempt 1/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 23.06 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:01:40] Gemma generation error (attempt 2/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 23.06 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:01:43] Gemma generation error (attempt 3/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 23.06 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:01:44] Gemma generation error (attempt 1/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 23.07 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:01:45] Gemma generation error (attempt 2/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 23.07 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:01:46] Gemma generation error (attempt 3/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 23.07 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:01:46] Gemma generation error (attempt 1/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 23.06 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:01:48] Gemma generation error (attempt 2/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 23.06 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:01:49] Gemma generation error (attempt 3/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 23.06 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:01:49] Gemma generation error (attempt 1/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 23.07 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:01:50] Gemma generation error (attempt 2/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 23.07 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:01:51] Gemma generation error (attempt 3/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 23.07 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:01:51] Gemma generation error (attempt 1/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 23.07 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:01:53] Gemma generation error (attempt 2/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 23.07 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:01:54] Gemma generation error (attempt 3/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 23.07 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:01:54] Gemma generation error (attempt 1/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 23.06 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:01:55] Gemma generation error (attempt 2/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 23.06 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:01:58] Gemma generation error (attempt 3/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 23.06 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:01:58] Gemma generation error (attempt 1/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 23.06 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Running conditions:   1%|          | 1/128 [1:00:58<129:03:35, 3658.39s/it][21:01:59] Gemma generation error (attempt 2/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 23.06 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:02:00] Gemma generation error (attempt 3/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 23.06 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:02:00] Gemma generation error (attempt 1/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 23.06 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:02:03] Gemma generation error (attempt 2/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 23.06 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:02:04] Gemma generation error (attempt 3/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 23.06 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:02:04] Gemma generation error (attempt 1/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:02:05] Gemma generation error (attempt 2/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:02:08] Gemma generation error (attempt 3/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:02:08] Gemma generation error (attempt 1/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:02:09] Gemma generation error (attempt 2/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:02:10] Gemma generation error (attempt 3/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:02:10] Gemma generation error (attempt 1/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:02:13] Gemma generation error (attempt 2/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:02:14] Gemma generation error (attempt 3/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:02:14] Gemma generation error (attempt 1/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:02:15] Gemma generation error (attempt 2/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:02:18] Gemma generation error (attempt 3/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:02:18] Gemma generation error (attempt 1/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:02:19] Gemma generation error (attempt 2/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:02:20] Gemma generation error (attempt 3/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:02:20] Gemma generation error (attempt 1/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:02:23] Gemma generation error (attempt 2/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:02:24] Gemma generation error (attempt 3/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:02:24] Gemma generation error (attempt 1/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:02:25] Gemma generation error (attempt 2/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:02:28] Gemma generation error (attempt 3/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:02:28] Gemma generation error (attempt 1/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:02:29] Gemma generation error (attempt 2/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:02:30] Gemma generation error (attempt 3/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:02:30] Gemma generation error (attempt 1/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:02:33] Gemma generation error (attempt 2/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:02:34] Gemma generation error (attempt 3/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:02:34] Gemma generation error (attempt 1/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:02:35] Gemma generation error (attempt 2/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:02:37] Gemma generation error (attempt 3/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:02:38] Gemma generation error (attempt 1/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:02:39] Gemma generation error (attempt 2/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:02:40] Gemma generation error (attempt 3/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:02:40] Gemma generation error (attempt 1/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:02:43] Gemma generation error (attempt 2/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:02:44] Gemma generation error (attempt 3/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:02:44] Gemma generation error (attempt 1/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:02:45] Gemma generation error (attempt 2/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:02:47] Gemma generation error (attempt 3/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:02:48] Gemma generation error (attempt 1/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:02:49] Gemma generation error (attempt 2/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:02:50] Gemma generation error (attempt 3/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:02:50] Gemma generation error (attempt 1/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:02:52] Gemma generation error (attempt 2/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:02:53] Gemma generation error (attempt 3/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:02:53] Gemma generation error (attempt 1/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:02:54] Gemma generation error (attempt 2/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:02:55] Gemma generation error (attempt 3/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:02:55] Gemma generation error (attempt 1/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:02:57] Gemma generation error (attempt 2/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:02:59] Gemma generation error (attempt 3/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:02:59] Gemma generation error (attempt 1/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:03:00] Gemma generation error (attempt 2/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:03:02] Gemma generation error (attempt 3/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:03:02] Gemma generation error (attempt 1/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:03:03] Gemma generation error (attempt 2/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:03:04] Gemma generation error (attempt 3/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:03:04] Gemma generation error (attempt 1/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:03:07] Gemma generation error (attempt 2/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:03:08] Gemma generation error (attempt 3/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:03:08] Gemma generation error (attempt 1/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:03:09] Gemma generation error (attempt 2/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:03:12] Gemma generation error (attempt 3/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:03:12] Gemma generation error (attempt 1/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:03:13] Gemma generation error (attempt 2/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:03:14] Gemma generation error (attempt 3/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:03:14] Gemma generation error (attempt 1/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:03:17] Gemma generation error (attempt 2/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:03:18] Gemma generation error (attempt 3/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:03:18] Gemma generation error (attempt 1/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:03:19] Gemma generation error (attempt 2/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:03:22] Gemma generation error (attempt 3/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:03:22] Gemma generation error (attempt 1/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:03:23] Gemma generation error (attempt 2/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:03:24] Gemma generation error (attempt 3/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:03:24] Gemma generation error (attempt 1/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:03:26] Gemma generation error (attempt 2/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:03:27] Gemma generation error (attempt 3/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:03:27] Gemma generation error (attempt 1/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:03:28] Gemma generation error (attempt 2/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:03:30] Gemma generation error (attempt 3/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:03:31] Gemma generation error (attempt 1/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:03:32] Gemma generation error (attempt 2/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:03:33] Gemma generation error (attempt 3/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:03:33] Gemma generation error (attempt 1/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:03:36] Gemma generation error (attempt 2/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:03:37] Gemma generation error (attempt 3/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:03:37] Gemma generation error (attempt 1/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:03:38] Gemma generation error (attempt 2/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:03:41] Gemma generation error (attempt 3/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:03:41] Gemma generation error (attempt 1/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:03:42] Gemma generation error (attempt 2/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:03:43] Gemma generation error (attempt 3/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:03:43] Gemma generation error (attempt 1/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:03:46] Gemma generation error (attempt 2/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:03:47] Gemma generation error (attempt 3/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:03:47] Gemma generation error (attempt 1/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:03:48] Gemma generation error (attempt 2/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:03:50] Gemma generation error (attempt 3/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:03:51] Gemma generation error (attempt 1/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:03:52] Gemma generation error (attempt 2/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:03:53] Gemma generation error (attempt 3/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:03:53] Gemma generation error (attempt 1/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:03:56] Gemma generation error (attempt 2/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:03:57] Gemma generation error (attempt 3/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:03:57] Gemma generation error (attempt 1/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:03:58] Gemma generation error (attempt 2/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.69 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:04:00] Gemma generation error (attempt 3/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.69 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:04:01] Gemma generation error (attempt 1/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.69 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:04:02] Gemma generation error (attempt 2/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.69 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:04:03] Gemma generation error (attempt 3/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.69 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:04:03] Gemma generation error (attempt 1/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.69 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:04:06] Gemma generation error (attempt 2/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.69 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:04:07] Gemma generation error (attempt 3/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.69 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:04:07] Gemma generation error (attempt 1/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.69 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:04:08] Gemma generation error (attempt 2/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.69 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:04:11] Gemma generation error (attempt 3/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.69 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:04:11] Gemma generation error (attempt 1/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.69 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:04:12] Gemma generation error (attempt 2/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.69 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:04:13] Gemma generation error (attempt 3/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.69 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:04:13] Gemma generation error (attempt 1/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.69 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:04:16] Gemma generation error (attempt 2/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.69 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:04:17] Gemma generation error (attempt 3/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.69 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:04:17] Gemma generation error (attempt 1/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.69 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:04:18] Gemma generation error (attempt 2/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.69 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:04:20] Gemma generation error (attempt 3/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.69 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:04:21] Gemma generation error (attempt 1/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.69 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:04:22] Gemma generation error (attempt 2/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.69 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:04:23] Gemma generation error (attempt 3/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.69 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:04:23] Gemma generation error (attempt 1/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.69 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:04:26] Gemma generation error (attempt 2/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.69 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:04:27] Gemma generation error (attempt 3/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.69 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:04:27] Gemma generation error (attempt 1/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.69 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:04:28] Gemma generation error (attempt 2/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.69 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:04:30] Gemma generation error (attempt 3/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.69 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:04:30] Gemma generation error (attempt 1/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.69 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:04:31] Gemma generation error (attempt 2/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.69 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:04:33] Gemma generation error (attempt 3/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.69 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:04:33] Gemma generation error (attempt 1/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.69 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Running conditions:   2%|â–         | 2/128 [1:03:40<56:04:09, 1601.98s/it] [21:04:35] Gemma generation error (attempt 2/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.69 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:04:36] Gemma generation error (attempt 3/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.69 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:04:36] Gemma generation error (attempt 1/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.69 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:04:37] Gemma generation error (attempt 2/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.69 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:04:40] Gemma generation error (attempt 3/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.69 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:04:40] Gemma generation error (attempt 1/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.69 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:04:41] Gemma generation error (attempt 2/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.69 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:04:42] Gemma generation error (attempt 3/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.69 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:04:42] Gemma generation error (attempt 1/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.69 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:04:45] Gemma generation error (attempt 2/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.69 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:04:46] Gemma generation error (attempt 3/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.69 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:04:46] Progress: 100/6400
[21:04:46] Gemma generation error (attempt 1/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.69 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:04:47] Gemma generation error (attempt 2/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.69 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:04:50] Gemma generation error (attempt 3/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.69 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:04:50] Gemma generation error (attempt 1/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.69 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:04:51] Gemma generation error (attempt 2/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.69 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:04:52] Gemma generation error (attempt 3/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.69 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:04:52] Gemma generation error (attempt 1/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.69 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:04:55] Gemma generation error (attempt 2/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.69 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:04:56] Gemma generation error (attempt 3/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.69 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:04:56] Gemma generation error (attempt 1/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.69 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:04:57] Gemma generation error (attempt 2/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.69 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:05:00] Gemma generation error (attempt 3/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.69 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:05:00] Gemma generation error (attempt 1/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.69 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:05:01] Gemma generation error (attempt 2/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.69 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:05:02] Gemma generation error (attempt 3/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.69 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:05:02] Gemma generation error (attempt 1/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.69 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:05:05] Gemma generation error (attempt 2/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.69 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:05:06] Gemma generation error (attempt 3/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.69 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:05:06] Gemma generation error (attempt 1/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.69 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:05:07] Gemma generation error (attempt 2/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.69 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:05:10] Gemma generation error (attempt 3/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.69 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:05:10] Gemma generation error (attempt 1/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.69 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:05:11] Gemma generation error (attempt 2/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.69 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:05:12] Gemma generation error (attempt 3/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.69 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:05:12] Gemma generation error (attempt 1/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.69 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:05:15] Gemma generation error (attempt 2/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.69 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:05:16] Gemma generation error (attempt 3/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.69 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:05:16] Gemma generation error (attempt 1/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.69 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:05:17] Gemma generation error (attempt 2/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.69 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:05:20] Gemma generation error (attempt 3/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.69 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:05:20] Gemma generation error (attempt 1/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.69 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:05:21] Gemma generation error (attempt 2/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.69 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:05:22] Gemma generation error (attempt 3/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.69 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:05:22] Gemma generation error (attempt 1/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.69 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:05:24] Gemma generation error (attempt 2/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.69 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:05:26] Gemma generation error (attempt 3/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.69 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:05:26] Gemma generation error (attempt 1/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.69 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:05:27] Gemma generation error (attempt 2/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.69 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:05:29] Gemma generation error (attempt 3/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.69 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:05:30] Gemma generation error (attempt 1/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.69 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:05:31] Gemma generation error (attempt 2/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.69 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:05:32] Gemma generation error (attempt 3/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.69 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:05:32] Gemma generation error (attempt 1/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.69 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:05:35] Gemma generation error (attempt 2/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.69 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:05:36] Gemma generation error (attempt 3/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.69 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:05:36] Gemma generation error (attempt 1/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.69 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:05:37] Gemma generation error (attempt 2/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.69 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:05:40] Gemma generation error (attempt 3/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.69 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:05:40] Gemma generation error (attempt 1/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.69 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:05:41] Gemma generation error (attempt 2/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.69 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:05:42] Gemma generation error (attempt 3/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.69 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:05:42] Gemma generation error (attempt 1/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.69 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:05:45] Gemma generation error (attempt 2/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.69 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:05:46] Gemma generation error (attempt 3/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.69 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:05:46] Gemma generation error (attempt 1/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.69 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:05:47] Gemma generation error (attempt 2/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.69 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:05:50] Gemma generation error (attempt 3/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.69 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:05:50] Gemma generation error (attempt 1/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.69 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:05:51] Gemma generation error (attempt 2/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.69 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:05:52] Gemma generation error (attempt 3/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.69 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:05:52] Gemma generation error (attempt 1/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.69 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:05:54] Gemma generation error (attempt 2/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.69 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:05:55] Gemma generation error (attempt 3/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.69 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:05:56] Gemma generation error (attempt 1/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.69 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:05:57] Gemma generation error (attempt 2/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.69 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:05:59] Gemma generation error (attempt 3/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.69 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:05:59] Gemma generation error (attempt 1/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.69 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:06:00] Gemma generation error (attempt 2/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.69 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:06:01] Gemma generation error (attempt 3/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.69 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:06:02] Gemma generation error (attempt 1/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.69 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:06:04] Gemma generation error (attempt 2/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.69 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:06:05] Gemma generation error (attempt 3/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.69 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:06:05] Gemma generation error (attempt 1/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.69 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:06:06] Gemma generation error (attempt 2/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.69 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:06:09] Gemma generation error (attempt 3/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.69 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:06:09] Gemma generation error (attempt 1/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.69 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:06:10] Gemma generation error (attempt 2/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.69 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:06:11] Gemma generation error (attempt 3/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.69 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:06:11] Gemma generation error (attempt 1/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.69 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:06:14] Gemma generation error (attempt 2/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.69 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:06:15] Gemma generation error (attempt 3/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.69 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:06:15] Gemma generation error (attempt 1/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.69 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:06:16] Gemma generation error (attempt 2/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.69 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:06:19] Gemma generation error (attempt 3/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.69 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:06:19] Gemma generation error (attempt 1/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.69 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:06:20] Gemma generation error (attempt 2/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.69 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:06:21] Gemma generation error (attempt 3/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.69 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:06:21] Gemma generation error (attempt 1/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.69 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:06:24] Gemma generation error (attempt 2/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.69 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:06:25] Gemma generation error (attempt 3/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.69 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:06:25] Gemma generation error (attempt 1/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.69 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:06:27] Gemma generation error (attempt 2/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.69 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:06:29] Gemma generation error (attempt 3/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.69 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:06:29] Gemma generation error (attempt 1/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.69 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:06:30] Gemma generation error (attempt 2/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.69 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:06:31] Gemma generation error (attempt 3/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.69 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:06:31] Gemma generation error (attempt 1/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.69 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:06:34] Gemma generation error (attempt 2/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.69 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:06:35] Gemma generation error (attempt 3/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.69 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:06:35] Gemma generation error (attempt 1/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.69 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:06:36] Gemma generation error (attempt 2/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.69 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:06:38] Gemma generation error (attempt 3/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.69 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:06:39] Gemma generation error (attempt 1/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.69 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:06:40] Gemma generation error (attempt 2/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.69 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:06:41] Gemma generation error (attempt 3/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.69 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:06:41] Gemma generation error (attempt 1/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.69 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:06:44] Gemma generation error (attempt 2/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.69 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:06:45] Gemma generation error (attempt 3/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.69 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:06:45] Gemma generation error (attempt 1/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.69 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:06:46] Gemma generation error (attempt 2/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.69 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:06:49] Gemma generation error (attempt 3/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.69 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:06:49] Gemma generation error (attempt 1/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.69 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:06:50] Gemma generation error (attempt 2/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.69 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:06:51] Gemma generation error (attempt 3/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.69 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:06:51] Gemma generation error (attempt 1/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.69 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:06:54] Gemma generation error (attempt 2/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.69 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:06:55] Gemma generation error (attempt 3/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.69 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:06:55] Gemma generation error (attempt 1/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.69 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:06:56] Gemma generation error (attempt 2/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.69 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:06:59] Gemma generation error (attempt 3/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.69 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:06:59] Gemma generation error (attempt 1/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.69 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:07:00] Gemma generation error (attempt 2/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.69 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:07:01] Gemma generation error (attempt 3/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.69 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:07:01] Gemma generation error (attempt 1/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.69 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:07:04] Gemma generation error (attempt 2/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.69 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:07:05] Gemma generation error (attempt 3/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.69 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:07:05] Gemma generation error (attempt 1/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.69 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:07:06] Gemma generation error (attempt 2/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.69 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:07:09] Gemma generation error (attempt 3/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.69 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:07:09] Gemma generation error (attempt 1/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.69 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:07:10] Gemma generation error (attempt 2/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.69 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:07:11] Gemma generation error (attempt 3/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.69 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:07:11] Gemma generation error (attempt 1/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.69 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:07:14] Gemma generation error (attempt 2/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.69 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:07:15] Gemma generation error (attempt 3/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.69 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:07:15] Gemma generation error (attempt 1/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.69 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:07:16] Gemma generation error (attempt 2/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.69 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:07:18] Gemma generation error (attempt 3/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.69 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:07:19] Gemma generation error (attempt 1/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.69 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:07:20] Gemma generation error (attempt 2/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.69 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:07:21] Gemma generation error (attempt 3/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.69 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:07:21] Gemma generation error (attempt 1/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.69 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:07:23] Gemma generation error (attempt 2/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.69 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:07:24] Gemma generation error (attempt 3/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.69 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:07:24] Gemma generation error (attempt 1/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.69 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Running conditions:   2%|â–         | 3/128 [1:06:25<32:50:20, 945.76s/it] [21:07:25] Gemma generation error (attempt 2/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.69 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:07:28] Gemma generation error (attempt 3/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.69 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:07:29] Gemma generation error (attempt 1/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.69 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:07:30] Gemma generation error (attempt 2/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:07:31] Gemma generation error (attempt 3/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:07:31] Gemma generation error (attempt 1/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:07:34] Gemma generation error (attempt 2/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:07:35] Gemma generation error (attempt 3/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:07:35] Gemma generation error (attempt 1/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:07:36] Gemma generation error (attempt 2/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:07:39] Gemma generation error (attempt 3/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:07:39] Gemma generation error (attempt 1/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:07:40] Gemma generation error (attempt 2/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:07:41] Gemma generation error (attempt 3/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:07:41] Gemma generation error (attempt 1/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:07:44] Gemma generation error (attempt 2/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:07:45] Gemma generation error (attempt 3/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:07:45] Gemma generation error (attempt 1/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:07:46] Gemma generation error (attempt 2/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:07:48] Gemma generation error (attempt 3/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:07:49] Gemma generation error (attempt 1/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:07:50] Gemma generation error (attempt 2/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:07:51] Gemma generation error (attempt 3/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:07:51] Gemma generation error (attempt 1/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:07:53] Gemma generation error (attempt 2/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:07:54] Gemma generation error (attempt 3/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:07:54] Gemma generation error (attempt 1/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:07:55] Gemma generation error (attempt 2/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:07:58] Gemma generation error (attempt 3/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:07:59] Gemma generation error (attempt 1/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:08:00] Gemma generation error (attempt 2/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:08:01] Gemma generation error (attempt 3/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:08:01] Gemma generation error (attempt 1/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:08:04] Gemma generation error (attempt 2/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:08:05] Gemma generation error (attempt 3/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:08:05] Gemma generation error (attempt 1/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:08:06] Gemma generation error (attempt 2/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:08:09] Gemma generation error (attempt 3/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:08:09] Gemma generation error (attempt 1/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:08:10] Gemma generation error (attempt 2/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:08:11] Gemma generation error (attempt 3/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:08:11] Gemma generation error (attempt 1/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:08:14] Gemma generation error (attempt 2/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:08:15] Gemma generation error (attempt 3/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:08:15] Gemma generation error (attempt 1/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:08:16] Gemma generation error (attempt 2/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:08:18] Gemma generation error (attempt 3/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:08:19] Gemma generation error (attempt 1/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:08:20] Gemma generation error (attempt 2/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:08:21] Gemma generation error (attempt 3/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:08:21] Gemma generation error (attempt 1/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:08:23] Gemma generation error (attempt 2/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:08:24] Gemma generation error (attempt 3/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:08:24] Gemma generation error (attempt 1/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:08:25] Gemma generation error (attempt 2/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:08:29] Gemma generation error (attempt 3/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:08:29] Gemma generation error (attempt 1/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:08:30] Gemma generation error (attempt 2/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:08:31] Gemma generation error (attempt 3/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:08:31] Gemma generation error (attempt 1/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:08:34] Gemma generation error (attempt 2/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:08:35] Gemma generation error (attempt 3/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:08:35] Gemma generation error (attempt 1/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:08:36] Gemma generation error (attempt 2/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:08:39] Gemma generation error (attempt 3/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:08:39] Gemma generation error (attempt 1/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:08:40] Gemma generation error (attempt 2/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:08:41] Gemma generation error (attempt 3/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:08:41] Gemma generation error (attempt 1/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:08:43] Gemma generation error (attempt 2/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:08:44] Gemma generation error (attempt 3/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:08:44] Gemma generation error (attempt 1/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:08:45] Gemma generation error (attempt 2/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:08:48] Gemma generation error (attempt 3/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:08:49] Gemma generation error (attempt 1/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:08:50] Gemma generation error (attempt 2/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:08:51] Gemma generation error (attempt 3/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:08:51] Gemma generation error (attempt 1/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:08:53] Gemma generation error (attempt 2/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:08:54] Gemma generation error (attempt 3/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:08:54] Gemma generation error (attempt 1/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:08:55] Gemma generation error (attempt 2/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:08:59] Gemma generation error (attempt 3/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:08:59] Gemma generation error (attempt 1/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:09:00] Gemma generation error (attempt 2/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:09:01] Gemma generation error (attempt 3/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:09:01] Gemma generation error (attempt 1/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:09:03] Gemma generation error (attempt 2/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:09:04] Gemma generation error (attempt 3/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:09:04] Gemma generation error (attempt 1/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:09:05] Gemma generation error (attempt 2/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:09:06] Gemma generation error (attempt 3/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:09:06] Gemma generation error (attempt 1/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:09:09] Gemma generation error (attempt 2/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:09:10] Gemma generation error (attempt 3/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:09:10] Gemma generation error (attempt 1/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:09:11] Gemma generation error (attempt 2/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:09:13] Gemma generation error (attempt 3/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:09:14] Gemma generation error (attempt 1/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:09:15] Gemma generation error (attempt 2/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:09:16] Gemma generation error (attempt 3/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:09:16] Gemma generation error (attempt 1/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:09:18] Gemma generation error (attempt 2/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:09:19] Gemma generation error (attempt 3/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:09:19] Gemma generation error (attempt 1/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:09:20] Gemma generation error (attempt 2/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:09:21] Gemma generation error (attempt 3/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:09:21] Gemma generation error (attempt 1/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:09:24] Gemma generation error (attempt 2/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:09:25] Gemma generation error (attempt 3/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:09:25] Gemma generation error (attempt 1/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:09:26] Gemma generation error (attempt 2/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:09:29] Gemma generation error (attempt 3/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:09:29] Gemma generation error (attempt 1/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:09:30] Gemma generation error (attempt 2/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:09:31] Gemma generation error (attempt 3/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:09:31] Gemma generation error (attempt 1/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:09:34] Gemma generation error (attempt 2/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:09:35] Gemma generation error (attempt 3/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:09:35] Gemma generation error (attempt 1/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:09:36] Gemma generation error (attempt 2/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:09:39] Gemma generation error (attempt 3/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:09:39] Gemma generation error (attempt 1/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:09:40] Gemma generation error (attempt 2/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:09:41] Gemma generation error (attempt 3/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:09:41] Gemma generation error (attempt 1/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:09:44] Gemma generation error (attempt 2/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:09:45] Gemma generation error (attempt 3/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:09:45] Gemma generation error (attempt 1/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:09:46] Gemma generation error (attempt 2/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:09:48] Gemma generation error (attempt 3/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:09:49] Gemma generation error (attempt 1/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:09:50] Gemma generation error (attempt 2/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:09:51] Gemma generation error (attempt 3/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:09:51] Gemma generation error (attempt 1/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:09:53] Gemma generation error (attempt 2/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:09:54] Gemma generation error (attempt 3/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:09:54] Gemma generation error (attempt 1/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:09:55] Gemma generation error (attempt 2/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:09:56] Gemma generation error (attempt 3/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:09:56] Gemma generation error (attempt 1/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:09:59] Gemma generation error (attempt 2/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:10:00] Gemma generation error (attempt 3/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:10:00] Gemma generation error (attempt 1/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Running conditions:   3%|â–Ž         | 4/128 [1:09:08<21:55:27, 636.51s/it][21:10:01] Gemma generation error (attempt 2/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:10:03] Gemma generation error (attempt 3/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:10:04] Gemma generation error (attempt 1/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:10:05] Gemma generation error (attempt 2/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:10:06] Gemma generation error (attempt 3/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:10:06] Gemma generation error (attempt 1/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:10:09] Gemma generation error (attempt 2/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:10:10] Gemma generation error (attempt 3/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:10:10] Gemma generation error (attempt 1/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:10:11] Gemma generation error (attempt 2/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:10:14] Gemma generation error (attempt 3/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:10:14] Progress: 200/6400
[21:10:14] Gemma generation error (attempt 1/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:10:15] Gemma generation error (attempt 2/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:10:16] Gemma generation error (attempt 3/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:10:16] Gemma generation error (attempt 1/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:10:18] Gemma generation error (attempt 2/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:10:19] Gemma generation error (attempt 3/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:10:19] Gemma generation error (attempt 1/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:10:20] Gemma generation error (attempt 2/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:10:23] Gemma generation error (attempt 3/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:10:24] Gemma generation error (attempt 1/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:10:25] Gemma generation error (attempt 2/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:10:26] Gemma generation error (attempt 3/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:10:26] Gemma generation error (attempt 1/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:10:29] Gemma generation error (attempt 2/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:10:30] Gemma generation error (attempt 3/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:10:30] Gemma generation error (attempt 1/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:10:31] Gemma generation error (attempt 2/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:10:33] Gemma generation error (attempt 3/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:10:34] Gemma generation error (attempt 1/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:10:35] Gemma generation error (attempt 2/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:10:36] Gemma generation error (attempt 3/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:10:36] Gemma generation error (attempt 1/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:10:39] Gemma generation error (attempt 2/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:10:40] Gemma generation error (attempt 3/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:10:40] Gemma generation error (attempt 1/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:10:41] Gemma generation error (attempt 2/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:10:44] Gemma generation error (attempt 3/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:10:44] Gemma generation error (attempt 1/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:10:45] Gemma generation error (attempt 2/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:10:46] Gemma generation error (attempt 3/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:10:46] Gemma generation error (attempt 1/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:10:48] Gemma generation error (attempt 2/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:10:49] Gemma generation error (attempt 3/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:10:49] Gemma generation error (attempt 1/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:10:50] Gemma generation error (attempt 2/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:10:51] Gemma generation error (attempt 3/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:10:51] Gemma generation error (attempt 1/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:10:53] Gemma generation error (attempt 2/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:10:54] Gemma generation error (attempt 3/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:10:54] Gemma generation error (attempt 1/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:10:55] Gemma generation error (attempt 2/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:10:58] Gemma generation error (attempt 3/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:10:59] Gemma generation error (attempt 1/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:11:00] Gemma generation error (attempt 2/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:11:01] Gemma generation error (attempt 3/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:11:01] Gemma generation error (attempt 1/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:11:04] Gemma generation error (attempt 2/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:11:05] Gemma generation error (attempt 3/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:11:05] Gemma generation error (attempt 1/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:11:06] Gemma generation error (attempt 2/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:11:09] Gemma generation error (attempt 3/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:11:09] Gemma generation error (attempt 1/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:11:10] Gemma generation error (attempt 2/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:11:11] Gemma generation error (attempt 3/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:11:11] Gemma generation error (attempt 1/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:11:13] Gemma generation error (attempt 2/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:11:14] Gemma generation error (attempt 3/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:11:14] Gemma generation error (attempt 1/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:11:16] Gemma generation error (attempt 2/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:11:18] Gemma generation error (attempt 3/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:11:19] Gemma generation error (attempt 1/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:11:20] Gemma generation error (attempt 2/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:11:21] Gemma generation error (attempt 3/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:11:21] Gemma generation error (attempt 1/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:11:24] Gemma generation error (attempt 2/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:11:25] Gemma generation error (attempt 3/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:11:25] Gemma generation error (attempt 1/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:11:26] Gemma generation error (attempt 2/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:11:28] Gemma generation error (attempt 3/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:11:29] Gemma generation error (attempt 1/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:11:30] Gemma generation error (attempt 2/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:11:31] Gemma generation error (attempt 3/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[21:11:31] Gemma generation error (attempt 1/3): CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 19.62 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 20.53 GiB is allocated by PyTorch, with 2.95 GiB allocated in private pools (e.g., CUDA Graphs), and 22.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
