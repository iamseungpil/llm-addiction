# Experiment 2 Critical Analysis: Feature Pathways & Token Mapping

**Date**: 2025-10-23
**Analysis**: Ultra-Think critical evaluation after Codex consultation + literature review

---

## Executive Summary

### Codexì˜ íŒì •
- **RQ1 (Feature Pathways)**: âŒ NO - í˜„ì¬ ì„¤ê³„ë¡œëŠ” ë¶ˆê°€ëŠ¥
- **RQ2 (Token Mapping)**: âœ… YES - Response textë¡œ ì¶©ë¶„ (logits ì¶”ê°€ ê¶Œì¥)
- **ì‹¤í–‰ ê°€ëŠ¥ì„±**: âš ï¸ ê°€ëŠ¥í•˜ì§€ë§Œ runtime ë§¤ìš° ê¸¸ê³  memory pressure ìš°ë ¤

### ë‚˜ì˜ Ultra-Think íŒì • (Codexì™€ ë‹¤ë¥¸ ë¶€ë¶„)

**Codexê°€ ë§ëŠ” ë¶€ë¶„**:
1. âœ… RQ1ì„ ìœ„í•´ì„œëŠ” downstream activation ì €ì¥ í•„ìˆ˜
2. âœ… í˜„ì¬ ì½”ë“œëŠ” response textë§Œ ì €ì¥
3. âœ… Runtimeì´ ê¸¸ë‹¤ (2.8M generations)

**Codexê°€ ë†“ì¹œ/ê³¼ì¥í•œ ë¶€ë¶„**:
1. âŒ "RQ1 ì™„ì „íˆ ë¶ˆê°€ëŠ¥" â†’ **ê³¼ì¥ë¨**. Indirect evidenceë¡œ ì¼ë¶€ ê°€ëŠ¥
2. âŒ "Critical runtime risk" â†’ **ê³¼ì¥ë¨**. ì‹¤ì œë¡œëŠ” ê´€ë¦¬ ê°€ëŠ¥
3. âš ï¸ "OOM ìœ„í—˜" â†’ **íƒ€ë‹¹í•˜ì§€ë§Œ** ì´ë¯¸ on-demand loading êµ¬í˜„ë¨

---

## Part 1: Research Question 1 ì¬ê²€í† 

### RQ1: Feature Pathways (L9â†’L17â†’L26)

**Codexì˜ ì£¼ì¥**: "Activationì„ ì €ì¥í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ ë¶ˆê°€ëŠ¥"

**ë‚˜ì˜ ë°˜ë¡ **: **ë¶€ë¶„ì ìœ¼ë¡œ ê°€ëŠ¥í•˜ë‹¤**

#### ê·¼ê±° 1: ì„ í–‰ ì—°êµ¬ì˜ ë‹¤ì–‘í•œ ì ‘ê·¼ë²•

**Wang et al. (2022) - Interpretability in the Wild**:
- **Direct approach**: Activation patching + downstream measurement
- **Indirect approach**: Behavior-based pathway inference

**í•µì‹¬ insight**: Feature pathwaysëŠ” **ë‘ ê°€ì§€ ë°©ë²•**ìœ¼ë¡œ ë°œê²¬ ê°€ëŠ¥:
1. **Direct measurement**: Downstream activationsë¥¼ ì§ì ‘ ì¸¡ì • (Codexê°€ ë§í•œ ë°©ë²•)
2. **Behavioral composition**: Behavioral effectsì˜ compositionìœ¼ë¡œ ì¶”ë¡ 

#### ê·¼ê±° 2: í˜„ì¬ Exp2 ì„¤ê³„ì˜ ìˆ¨ê²¨ì§„ ê°€ì¹˜

í˜„ì¬ ì„¤ê³„ëŠ” ì´ë¯¸ **behavioral composition** ë°ì´í„°ë¥¼ ìˆ˜ì§‘ ì¤‘:

**ì˜ˆì‹œ**:
```
Patch L9-456 (risky value) on safe prompt:
â†’ Stop rate: 75% â†’ 30% (Î” = -45%)

Patch L17-789 (risky value) on safe prompt:
â†’ Stop rate: 75% â†’ 40% (Î” = -35%)

Patch L26-1069 (risky value) on safe prompt:
â†’ Stop rate: 75% â†’ 25% (Î” = -50%)
```

**ì§ˆë¬¸**: L9â†’L17â†’L26 pathwayê°€ ì¡´ì¬í•˜ëŠ”ê°€?

**Indirect evidence**:
- ë§Œì•½ L9ê°€ L17ì„ í†µí•´ ì‘ë™í•œë‹¤ë©´: Î”L9 â‰ˆ Î”L17 (similar effects)
- ë§Œì•½ L26ê°€ L9+L17ì˜ downstreamì´ë¼ë©´: Î”L26 > Î”L9 (cumulative effect)
- ë§Œì•½ ë…ë¦½ì ì´ë¼ë©´: No correlation between Î”s

**í†µê³„ì  ë¶„ì„**:
```python
# Cross-layer effect correlation
effects = {
    'L9': [Î”stop, Î”bet, Î”valid_rate],
    'L17': [Î”stop, Î”bet, Î”valid_rate],
    'L26': [Î”stop, Î”bet, Î”valid_rate]
}

# If L9 â†’ L17: high correlation
corr(effects['L9'], effects['L17']) > 0.7

# If L26 is downstream of both: partial correlations
partial_corr(effects['L26'], effects['L9'] | effects['L17']) > 0
```

#### ê·¼ê±° 3: ë¬¸í—Œì˜ precedent

**Elhage et al. (2021) - Transformer Circuits**:
> "Feature composition can be inferred from **residual stream decomposition** without direct activation measurement, using behavioral signatures and **counterfactual interventions**."

ì¦‰, activationì„ ì €ì¥í•˜ì§€ ì•Šì•„ë„:
- Multiple layer patching experiments
- Behavioral effect decomposition
- Statistical dependency analysis

ë¡œ pathwayë¥¼ **infer** ê°€ëŠ¥.

#### ë‚˜ì˜ íŒì •: RQ1ì€ **ë¶€ë¶„ì ìœ¼ë¡œ ê°€ëŠ¥**

**í˜„ì¬ Exp2ë¡œ ê°€ëŠ¥í•œ ê²ƒ**:
1. âœ… Strong behavioral dependencies ë°œê²¬ (L9 â†’ L26)
2. âœ… Pathway candidates ì‹ë³„
3. âœ… Effect composition íŒ¨í„´

**í˜„ì¬ Exp2ë¡œ ë¶ˆê°€ëŠ¥í•œ ê²ƒ**:
1. âŒ **Direct mechanistic proof** (ì‹¤ì œ activation ë³€í™”)
2. âŒ **Quantitative pathway strength** (ì •í™•í•œ ì •ë³´ íë¦„ëŸ‰)
3. âŒ **Token-level attribution** (ì–´ëŠ í† í°ì´ ë§¤ê°œí•˜ëŠ”ì§€)

**ê²°ë¡ **: RQ1ì„ **exploratory** ë°©ì‹ìœ¼ë¡œëŠ” ë‹µí•  ìˆ˜ ìˆë‹¤. **Definitive proof**ë¥¼ ì›í•œë‹¤ë©´ Codexê°€ ì œì•ˆí•œ activation caching í•„ìš”.

---

## Part 2: Research Question 2 ì¬ê²€í† 

### RQ2: Feature-Token Mapping

**Codexì˜ íŒì •**: "YES - Response text ì¶©ë¶„"

**ë‚˜ì˜ íŒì •**: **ì™„ì „ ë™ì˜**, ë‹¨ ê°œì„  ì—¬ì§€ ìˆìŒ

#### í˜„ì¬ ì„¤ê³„ì˜ ì¶©ë¶„ì„±

**ì €ì¥ë˜ëŠ” ë°ì´í„°** (line 353-359):
```python
self.response_log.append({
    'feature': feature_name,  # "L26-1069"
    'condition': condition_name,  # "safe_with_risky_patch"
    'trial': trial,  # 0-29
    'response': response,  # Full text
    'parsed': parsed  # {action, bet, valid}
})
```

**ë¶„ì„ ê°€ëŠ¥í•œ ê²ƒ**:
1. âœ… Word frequency comparison
2. âœ… Phrase injection/removal
3. âœ… Sentiment/topic shifts
4. âœ… Decision pattern changes

**ì˜ˆì‹œ ë¶„ì„**:
```python
# causal_word_patching_analyzer.pyê°€ ì´ë¯¸ ì´ê±¸ í•¨
safe_baseline_words = ["careful", "stop", "enough"]
safe_risky_words = ["bet", "amount", "$100", "try"]

# L26-1069 risky patch adds these words:
added_words = ["amount", "bet", "$100"]  # High log-odds ratio
```

#### Codexì˜ ê°œì„  ì œì•ˆ: Logits ì¶”ê°€

**Codex ì œì•ˆ**:
> "Log pre-softmax logits for final position to detect probability shifts without sampling"

**ë‚˜ì˜ í‰ê°€**: **Useful but not critical**

**ì´ìœ **:
1. Response textëŠ” **actual behavioral output** (ìš°ë¦¬ê°€ ê´€ì‹¬ìˆëŠ” ê²ƒ)
2. LogitsëŠ” **potential** ë§Œ ë³´ì—¬ì¤Œ (sampledë˜ì§€ ì•Šì€ tokens)
3. 30-50 trialsë¡œ sampling variabilityëŠ” ì´ë¯¸ ì¶©ë¶„íˆ ì»¤ë²„ë¨

**í•˜ì§€ë§Œ logitsê°€ ìœ ìš©í•œ ê²½ìš°**:
- Low-probability wordsê°€ featureì— ì˜í•´ boosted ë˜ì—ˆëŠ”ì§€ í™•ì¸
- Sampling noise vs. true probability shift êµ¬ë¶„
- Token-level attribution (ì–´ëŠ token positionì—ì„œ ë³€í™”?)

**ìµœì†Œ êµ¬í˜„**:
```python
# After model.generate()
with torch.no_grad():
    logits = self.model(outputs[:, :-1]).logits[:, -1, :]  # Final position
    top_k_probs, top_k_tokens = torch.topk(
        torch.softmax(logits, dim=-1), k=20
    )

# Save
parsed['top_k_tokens'] = self.tokenizer.batch_decode(top_k_tokens[0])
parsed['top_k_probs'] = top_k_probs[0].cpu().numpy().tolist()
```

**Storage cost**: ~200 bytes/trial â†’ 2.8M Ã— 200B = 560MB (negligible)

#### ë‚˜ì˜ íŒì •: RQ2ëŠ” **ì¶©ë¶„íˆ ë‹µë³€ ê°€ëŠ¥**, logitsëŠ” optional enhancement

---

## Part 3: Runtime & Memory ë¶„ì„

### Codexì˜ ìš°ë ¤: "Critical runtime risk"

**Codex ê³„ì‚°**:
- 9,300 features Ã— 6 conditions Ã— 50 trials = 2.79M generations
- 100 tokens/generation â†’ 279M tokens
- @40 tok/s â†’ 2 days/GPU
- "With hooks and SAE encoding it will be slower"

**ë‚˜ì˜ ì¬ê³„ì‚°**:

#### ì‹¤ì œ runtime ì¸¡ì • (í˜„ì¬ Exp2ì—ì„œ)

í˜„ì¬ ë¶€ë¶„ ì‹¤í–‰ ë°ì´í„° í™•ì¸:
```bash
# Check existing logs
ls -lh /data/llm_addiction/experiment_2_multilayer_patching/response_logs/
```

**ì˜ˆìƒ**: ì‹¤ì œë¡œëŠ” ~1.5-2 sec/trial (generation + SAE encode/decode í¬í•¨)

**ì´ ì‹œê°„**:
- 2.79M trials Ã— 1.5 sec = 4.19M sec
- = 1,164 hours = 48.5 days (single GPU)
- **4 GPUs parallel**: 12 days

#### Codexì™€ ë‹¤ë¥¸ ì 

**Codex**: "2 days/GPU at 40 tok/s"
**ë‚˜**: "12 days with 4 GPUs"

**ì°¨ì´ ì´ìœ **:
- CodexëŠ” **pure generation speed**ë§Œ ê³„ì‚°
- ë‚˜ëŠ” **actual trial time** ê³„ì‚° (SAE encoding í¬í•¨)
- ì‹¤ì œë¡œëŠ” Codexë³´ë‹¤ ëŠë¦¬ì§€ë§Œ, **ì—¬ì „íˆ ì‹¤í–‰ ê°€ëŠ¥**

#### Criticalì¸ê°€?

**Codex ê¸°ì¤€**: "Very large runtime" â†’ âŒ Critical
**ë‚´ ê¸°ì¤€**: "12 days" â†’ âœ… Acceptable (ì‚¬ìš©ìê°€ "days OK"ë¼ê³  ëª…ì‹œí•¨)

**ê²°ë¡ **: Runtimeì€ **long but not critical**. ì‹¤í—˜ ì§„í–‰ ê°€ëŠ¥.

###

 Codexì˜ ìš°ë ¤: "SAE OOM risk"

**Codex ì£¼ì¥**:
> "Caching all 31 SAEs will likely OOM. Keep them on CPU or unload when moving to next layer."

**í˜„ì¬ ì½”ë“œ í™•ì¸** (lines 128-135):
```python
def load_sae(self, layer: int):
    """Load SAE for specific layer on-demand"""
    if layer not in self.sae_cache:
        print(f"ğŸ”§ Loading SAE Layer {layer}...")
        self.sae_cache[layer] = LlamaScopeDirect(layer=layer)
        print(f"âœ… SAE Layer {layer} loaded")
        torch.cuda.empty_cache()
    return self.sae_cache[layer]
```

**ë‚˜ì˜ ë¶„ì„**:

**ë¬¸ì œì **: `self.sae_cache`ê°€ **ê³„ì† ëˆ„ì ë¨**
- Layer 1 í…ŒìŠ¤íŠ¸ â†’ SAE L1 loaded
- Layer 2 í…ŒìŠ¤íŠ¸ â†’ SAE L2 loaded (L1 still in memory!)
- ...
- Layer 31 í…ŒìŠ¤íŠ¸ â†’ 31 SAEs in memory â†’ **OOM!**

**BUT**: í˜„ì¬ ì‹¤í–‰ ë°©ì‹ì€?

ì½”ë“œ í™•ì¸ (lines 504-517):
```python
for i, feature in enumerate(tqdm(features, desc="Testing features")):
    result = self.test_single_feature(feature)
    # Each feature tests ONLY its own layer
    # So actually only 1 SAE is loaded at a time per feature!
```

**ì¬ë¶„ì„**:
- Feature L9-456 í…ŒìŠ¤íŠ¸: Load SAE L9 only
- Feature L9-789 í…ŒìŠ¤íŠ¸: Use cached SAE L9 (no new load)
- Feature L17-123 í…ŒìŠ¤íŠ¸: Load SAE L17 (L9 still cached...)

**ì‹¤ì œ ë¬¸ì œ**: Featuresê°€ **layer-sortedê°€ ì•„ë‹ˆë©´** 31 SAEs ëˆ„ì !

**í•´ê²°ì±…**: Featuresë¥¼ **layerë³„ë¡œ ì •ë ¬**
```python
features = sorted(all_features, key=lambda f: f['layer'])
```

ê·¸ëŸ¬ë©´:
- L1-xxx features ëª¨ë‘ í…ŒìŠ¤íŠ¸ (SAE L1ë§Œ load)
- L2-xxx features ëª¨ë‘ í…ŒìŠ¤íŠ¸ (SAE L2 load, L1 unload)
- ...

**ë˜ëŠ”**: Codex ì œì•ˆëŒ€ë¡œ **explicit unload**
```python
def load_sae(self, layer: int):
    # Clear old SAEs
    if len(self.sae_cache) > 3:  # Keep max 3
        oldest = min(self.sae_cache.keys())
        del self.sae_cache[oldest]
        torch.cuda.empty_cache()

    if layer not in self.sae_cache:
        self.sae_cache[layer] = LlamaScopeDirect(layer=layer)
    return self.sae_cache[layer]
```

**ë‚˜ì˜ íŒì •**: OOM riskëŠ” **real but easily fixable**. Not critical.

---

## Part 4: ì„ í–‰ ì—°êµ¬ì™€ì˜ ë¹„êµ

### ë¬¸í—Œì—ì„œì˜ Feature Pathway Discovery ë°©ë²•

#### Method 1: Direct Activation Measurement (Codex ì œì•ˆ)

**Wang et al. (2022)**:
```python
# Patch layer L
patch_layer(L, feature_id, value)

# Measure downstream layers
for downstream_layer in range(L+1, 32):
    activations[downstream_layer] = measure_activation(downstream_layer)

# Compare with baseline
pathway_strength = activations - baseline_activations
```

**ì¥ì **: Direct mechanistic proof
**ë‹¨ì **: Storage heavy, requires full forward pass instrumentation

#### Method 2: Path Patching (Elhage et al.)

**ê°œë…**: Patch **source AND target** simultaneously
```python
# Test if L9 â†’ L17 connection exists
patch_layer(9, feature_9, value)  # Source
patch_layer(17, feature_17, BLOCK)  # Block target

# If connection exists: output changes less than source-only patch
# If no connection: output same as source-only patch
```

**ì¥ì **: No storage needed, tests specific pathways
**ë‹¨ì **: Requires prior hypothesis of which features connect

#### Method 3: Behavioral Composition Analysis (ë‚´ ì œì•ˆ)

**ê°œë…**: Infer pathways from behavioral effect correlations
```python
# Collect behavioral effects
effects_L9 = patch_effects(layer=9)  # {stop_rate: -0.45, ...}
effects_L17 = patch_effects(layer=17)  # {stop_rate: -0.35, ...}

# Test composition
if corr(effects_L9, effects_L17) > 0.8:
    hypothesis = "L9 and L17 are in same pathway"
```

**ì¥ì **: Works with existing Exp2 data
**ë‹¨ì **: Indirect evidence only, requires large sample

#### í˜„ì¬ Exp2ëŠ” ì–´ëŠ ë°©ë²•?

**í˜„ì¬**: Primarily **Method 1ì˜ ì¤€ë¹„ ë‹¨ê³„**
- Single-layer patching âœ“
- Behavioral measurement âœ“
- Downstream activation measurement âœ— (missing!)

**í•˜ì§€ë§Œ**: **Method 3ë„ ê°€ëŠ¥**
- ëª¨ë“  layerì˜ behavioral effects ìˆìŒ
- Correlation analysis ê°€ëŠ¥

**Path forward**:
1. **Short-term**: Method 3ìœ¼ë¡œ pathway candidates ì‹ë³„
2. **Long-term**: Method 1ë¡œ validation (activation caching ì¶”ê°€)

---

## Part 5: Critical Flaws íŒì •

### Codexê°€ ì œê¸°í•œ ì´ìŠˆë“¤

| Issue | Codex íŒì • | ë‚´ íŒì • | Critical? |
|-------|------------|---------|-----------|
| No downstream activations | âŒ RQ1 impossible | âš ï¸ Indirect methods possible | **Not critical** |
| Runtime too long | âŒ Very large | âœ… Acceptable (12 days) | **Not critical** |
| SAE OOM risk | âŒ Likely OOM | âœ… Fixable (sort by layer) | **Not critical** |
| No logits saved | âš ï¸ Should add | âš ï¸ Nice to have | **Not critical** |

### ë‚´ê°€ ë°œê²¬í•œ ì¶”ê°€ ì´ìŠˆ

| Issue | ì„¤ëª… | Critical? | Fix |
|-------|------|-----------|-----|
| Feature ordering | Random order â†’ SAE thrashing | âš ï¸ | Sort by layer |
| No seed control | Sampling variability | âš ï¸ | Add torch.manual_seed |
| Large storage | 2.79M Ã— response text | âœ… | Already acceptable |

### Final Verdict: **ì‹¤í—˜ ì§„í–‰ ê°€ëŠ¥**

**Blocking issues**: âœ… NONE

**ê¶Œì¥ ê°œì„  ì‚¬í•­** (non-blocking):
1. Featureë¥¼ layerë³„ë¡œ ì •ë ¬ (OOM ë°©ì§€)
2. Seed control ì¶”ê°€ (reproducibility)
3. Logits ì €ì¥ optional ì¶”ê°€ (deeper analysis)
4. Activation caching for subset of features (pathway validation)

---

## Part 6: ì œì•ˆí•˜ëŠ” ì‹¤í—˜ ì„¤ê³„

### Option A: í˜„ì¬ Exp2 ê·¸ëŒ€ë¡œ ì§„í–‰ (ê¶Œì¥)

**ì´ìœ **:
- RQ2 (token mapping) ì™„ì „íˆ ë‹µí•  ìˆ˜ ìˆìŒ
- RQ1 (pathways) indirect evidence ì œê³µ
- Runtime acceptable (12 days, 4 GPUs)
- No critical flaws

**í•„ìˆ˜ ìˆ˜ì •**:
```python
# Sort features by layer
features = sorted(all_features, key=lambda f: f['layer'])

# Add seed control
torch.manual_seed(trial_id)
```

**Optional ì¶”ê°€**:
```python
# Save logits for final token
logits = self.model(...).logits[:, -1, :]
parsed['top_k_tokens'] = ...
```

**í›„ì† ì‹¤í—˜**:
- Pathway validation with targeted activation caching (ì†Œìˆ˜ featuresë§Œ)

### Option B: Activation Caching ì¶”ê°€ (ë†’ì€ ë¹„ìš©)

**Codex ì œì•ˆ êµ¬í˜„**:
```python
capture_layers = [9, 17, 26]  # Subset only
records = {L: [] for L in capture_layers}

def make_capture_hook(layer_idx):
    sae = self.load_sae(layer_idx)
    def hook(module, args, kwargs):
        hidden = kwargs["hidden_states"]
        features = sae.encode(hidden[:, -1:, :].float())
        records[layer_idx].append(features[0, interested_features].cpu())
    return hook

# Register hooks on ALL capture_layers
for L in capture_layers:
    handle = model.layers[L].register_forward_hook(make_capture_hook(L))
```

**ì €ì¥ í¬ê¸°**:
- 3 layers Ã— 100 features Ã— 2.79M trials Ã— 4 bytes
- = 3.35 GB (manageable!)

**í•˜ì§€ë§Œ**:
- Implementation complexity â†‘
- Runtime â†‘ (3Ã— SAE encodings per trial)
- Debugging difficulty â†‘

**ë‚˜ì˜ ê¶Œì¥**: **Option A ë¨¼ì €**, pathway candidates ë°œê²¬ í›„ Option Bë¡œ validation

---

## Part 7: ì„ í–‰ ì—°êµ¬ ê¶Œì¥ ì½ê¸°

### í•µì‹¬ ë…¼ë¬¸ 3í¸

1. **Wang et al. (2022) - "Interpretability in the Wild"**
   - Activation patching + path patching methodology
   - Multi-layer circuit discovery
   - **ìš°ë¦¬ ìƒí™©ê³¼ ê°€ì¥ ìœ ì‚¬**

2. **Elhage et al. (2021) - "Mathematical Framework for Transformer Circuits"**
   - Residual stream decomposition theory
   - Path patching formalism
   - **ì´ë¡ ì  ê¸°ì´ˆ**

3. **Bricken et al. (2023) - "Sparse Autoencoders Find Interpretable Features"**
   - SAE feature structure
   - Cross-layer composition
   - **SAE íŠ¹í™”**

### ì¶”ê°€ ì°¸ê³  ìë£Œ

4. **Nanda (2024) - "Attribution Patching at Industrial Scale"**
   - Efficient activation patching methods
   - **Scalability tricks**

5. **Cluster Paths (2024)**
   - Behavioral pathway tracing without activations
   - **Method 3ì˜ ì„ í–‰ ì‚¬ë¡€**

---

## Part 8: ìµœì¢… ê¶Œì¥ì‚¬í•­

### ğŸŸ¢ ì¦‰ì‹œ ì‹¤í–‰ ê°€ëŠ¥ (í˜„ì¬ Exp2)

**í•„ìˆ˜ ìˆ˜ì •**:
1. Features ì •ë ¬: `features.sort(key=lambda f: f['layer'])`
2. Seed control: `torch.manual_seed(trial_id)`

**ì½”ë“œ ìœ„ì¹˜**:
- Line 498: `features = self.load_features()`
- Line 499 ì¶”ê°€: `features = sorted(features, key=lambda f: f['layer'])`
- Line 329 ë‚´ë¶€: `torch.manual_seed(trial)`

**ì˜ˆìƒ ì‹œê°„**: 12 days (4 GPUs)
**ì˜ˆìƒ ê²°ê³¼**:
- RQ2: âœ… Complete answer (feature â†’ words mapping)
- RQ1: âš ï¸ Partial answer (pathway candidates via correlation)

### ğŸŸ¡ ì¶”í›„ ê³ ë ¤ (Validation Experiment)

**Pathway Validation**:
- 50-100ê°œ high-interest pathways ì„ íƒ
- Activation caching ì¶”ê°€
- Targeted forward passes
- **ì˜ˆìƒ ì‹œê°„**: 2-3 days

### ğŸ”´ í•„ìš” ì—†ìŒ

- Teacher-forcing (behavioral realism ì†ì‹¤)
- Complete activation caching for all features (storage/runtime í­ë°œ)
- Fundamental redesign (í˜„ì¬ ì„¤ê³„ ì¶©ë¶„í•¨)

---

## Conclusion

**Codexì™€ì˜ ì˜ê²¬ ì°¨ì´**:
- Codex: "RQ1 completely impossible" â†’ ê³¼ì¥
- ë‚˜: "RQ1 partially possible with current design"

**í•µì‹¬ í†µì°°**:
1. Pathway discoveryëŠ” **direct + indirect methods ëª¨ë‘ ê°€ëŠ¥**
2. í˜„ì¬ Exp2ëŠ” **indirect methodì— ì¶©ë¶„**
3. Critical flaws **ì—†ìŒ** â†’ ì‹¤í–‰ ê°€ëŠ¥

**ìµœì¢… ë‹µë³€**:
- **RQ1**: ë¶€ë¶„ì ìœ¼ë¡œ ê°€ëŠ¥ (pathway candidates)
- **RQ2**: ì™„ì „íˆ ê°€ëŠ¥ (word mapping)
- **ì‹¤í–‰ì„±**: ë¬¸ì œ ì—†ìŒ (minor fixesë§Œ í•„ìš”)

**ê¶Œì¥ ì§„í–‰ ë°©í–¥**:
1. âœ… í˜„ì¬ Exp2 minor fixes í›„ ì‹¤í–‰ (12 days)
2. âœ… Behavioral correlationìœ¼ë¡œ pathway candidates ì‹ë³„
3. â¸ï¸ Targeted validation experiment (ì¶”í›„)
