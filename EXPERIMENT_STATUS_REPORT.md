# LLM Addiction - 5ê°œ ì‹¤í—˜ ìƒíƒœ ë³´ê³ ì„œ
**ë‚ ì§œ**: 2025-10-02 04:50 KST

## ğŸ“Š ì „ì²´ ìš”ì•½

| ì‹¤í—˜ | ìƒíƒœ | ì§„í–‰ë¥  | GPU | ë¹„ê³  |
|------|------|--------|-----|------|
| **Exp0 LLaMA** | âœ… ì‹¤í–‰ ì¤‘ | 6/128 (4.7%) | GPU 0 | ì •ìƒ ì‘ë™ |
| **Exp0 Gemma** | âŒ ë©ˆì¶¤ | 0/128 (0%) | GPU 1 | ê²Œì„ ìƒì„± ë¯¸ì‹œì‘ |
| **Exp2 Patching** | âš ï¸ ë¡œë”© ì¤‘ | 0% | GPU 2,5,6,7 | SAE ë¡œë”© ë‹¨ê³„ ì •ì§€ |
| **Exp1 Pathway** | âœ… ì™„ë£Œ | 100% | - | 2.7GB ê²°ê³¼ íŒŒì¼ |
| **Exp3 Word** | âœ… ì™„ë£Œ | 100% | - | 441 features ë¶„ì„ ì™„ë£Œ |

---

## ğŸ” ìƒì„¸ ë¶„ì„

### 1. âœ… Exp0 LLaMA (ì •ìƒ ì‹¤í–‰ ì¤‘)

**ìœ„ì¹˜**: `/home/ubuntu/llm_addiction/experiment_0_llama_gemma_restart/`
**ì½”ë“œ**: `experiment_0_restart.py`
**ë¡œê·¸**: `logs/exp0_llama.log`
**tmux ì„¸ì…˜**: `exp0_llama`

**ì§„í–‰ ìƒí™©**:
- 6/128 ì¡°ê±´ ì™„ë£Œ (4.7%)
- ì˜ˆìƒ ì™„ë£Œ ì‹œê°„: ~4ì‹œê°„ (125ì´ˆ/ì¡°ê±´)
- GPU 0: 15.9GB ì‚¬ìš©
- CPU: ì •ìƒ

**ì˜ˆìƒ ê²°ê³¼ íŒŒì¼**: `/data/llm_addiction/experiment_0_llama_restart/`

---

### 2. âŒ Exp0 Gemma (ë©ˆì¶¤ - ì§„ë‹¨ í•„ìš”)

**ìœ„ì¹˜**: `/home/ubuntu/llm_addiction/experiment_0_llama_gemma_restart/`
**ì½”ë“œ**: `experiment_0_restart.py --model gemma`
**ë¡œê·¸**: `logs/exp0_gemma.log`
**tmux ì„¸ì…˜**: `exp0_gemma`

**ë¬¸ì œ ìƒí™©**:
- ëª¨ë¸ ë¡œë”© ì„±ê³µ (âœ… GEMMA loaded successfully)
- 0/128 ì¡°ê±´ì—ì„œ ë©ˆì¶¤ (ê²Œì„ ì‹œì‘ ì•ˆ ë¨)
- GPU 1: 59GB ì‚¬ìš© (ë§¤ìš° ë†’ìŒ)
- JAX ì œê±°í–ˆìœ¼ë‚˜ ì—¬ì „íˆ ë¬¸ì œ

**ê°€ëŠ¥í•œ ì›ì¸**:
1. Gemma-2-9b-itì˜ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ ë„ˆë¬´ ë†’ìŒ (59GB)
2. Chat template ë˜ëŠ” generation ì„¤ì • ë¬¸ì œ
3. Empty response ë¬´í•œ ë£¨í”„ ê°€ëŠ¥ì„±

**í•´ê²° ë°©ì•ˆ**:
- Gemma í”„ë¡œì„¸ìŠ¤ ì¬ì‹œì‘ í•„ìš”
- ë©”ëª¨ë¦¬ ì„¤ì • ìµœì í™” í•„ìš”
- Generation íŒŒë¼ë¯¸í„° ì¡°ì • í•„ìš”

---

### 3. âš ï¸ Exp2 Patching (SAE ë¡œë”© ë‹¨ê³„ ì •ì§€)

**ìœ„ì¹˜**: `/home/ubuntu/llm_addiction/experiment_2_multilayer_patching_L1_31/`
**ì½”ë“œ**: `experiment_2_L1_31_top300.py`
**ë¡œê·¸**: `logs/exp2_L1_8_gpu2.log` (ë“± 4ê°œ)
**tmux ì„¸ì…˜**: `exp2_p1`, `exp2_p2`, `exp2_p3`, `exp2_p4`

**ë¬¸ì œ ìƒí™©**:
```
   Loading W_E (encoder.weight)... converted... âœ… (torch.Size([4096, 32768]))
   Loading b_E (encoder.bias)... converted... âœ… (torch.Size([32768]))
   Loading W_D (decoder.weight)... converted... âœ… (torch.Size([32768, 4096]))
   Loading b_D (decoder.bias)... converted... â† ì—¬ê¸°ì„œ ë©ˆì¶¤
```

- ëª¨ë“  4ê°œ í”„ë¡œì„¸ìŠ¤ê°€ ë™ì¼í•œ ìœ„ì¹˜ì—ì„œ ì •ì§€
- CPU ì‚¬ìš©ë¥  102-105% (ì‹¤í–‰ì€ ë˜ê³  ìˆìŒ)
- GPU ë©”ëª¨ë¦¬ í• ë‹¹ ì™„ë£Œ (16-17GB per GPU)
- "converted..." ì¶œë ¥ í›„ "âœ… (torch.Size...)" ë¯¸ì¶œë ¥

**ê¸°ìˆ ì  ì›ì¸**:
- llama_scope_working.py line 228ì˜ print ë²„í¼ë§
- line 227 `new_state_dict[target_name] = weight` ë§¤ìš° ëŠë¦¼
- ë˜ëŠ” line 235+ ë‹¨ê³„ë¡œ ë„˜ì–´ê°”ìœ¼ë‚˜ ì¶œë ¥ ì—†ìŒ

**ì‹œë„í•œ í•´ê²°ì±…**:
1. âœ… `.float()` â†’ `.to(torch.float32)` ë³€ê²½
2. âœ… Staggered launch (30ì´ˆ ì§€ì—°)
3. âœ… í”„ë¡œì„¸ìŠ¤ ìˆ˜ ê°ì†Œ (12ê°œ â†’ 4ê°œ)
4. âŒ ì—¬ì „íˆ ë™ì¼ ì§€ì ì—ì„œ ì •ì§€

**ë‹¤ìŒ ë‹¨ê³„**:
1. Python unbuffered modeë¡œ ì‹¤í–‰ (`python -u`)
2. ëª¨ë“  printì— `flush=True` ì¶”ê°€
3. SAE ë¡œë”© ì½”ë“œ ë” ë‹¨ìˆœí™”
4. Checkpointë¥¼ ë¯¸ë¦¬ ë©”ëª¨ë¦¬ì— ë¡œë“œ

---

### 4. âœ… Exp1 Pathway (ì™„ë£Œ)

**ìœ„ì¹˜**: `/data/llm_addiction/experiment_1_pathway_L1_31/`
**ê²°ê³¼ íŒŒì¼**: `final_pathway_L1_31_20251001_165207.json` (2.7GB)

**ì™„ë£Œ ë‚´ì—­**:
- 50 games ë¶„ì„
- L1-31 (ì „ì²´ 31 layers) tracking ì™„ë£Œ
- Voluntary stop vs Bankruptcy ê·¸ë£¹ ë¹„êµ

---

### 5. âœ… Exp3 Word Analysis (ì™„ë£Œ)

**ìœ„ì¹˜**: `/data/llm_addiction/experiment_4_feature_word_analysis/`
**ê²°ê³¼ íŒŒì¼**: `feature_word_analysis_20251001_000025.json`

**ì™„ë£Œ ë‚´ì—­**:
- 441 features (Layer 25) ë¶„ì„ ì™„ë£Œ
- Decoder weight analysisë¡œ ì˜í–¥ë°›ëŠ” ë‹¨ì–´ ë¶„ì„
- Bankrupt vs Safe group delta ê³„ì‚°

---

## ğŸ› ï¸ í•´ê²°í•œ ë¬¸ì œë“¤

### 1. âœ… device_map ì˜¤ë¥˜
- **ë¬¸ì œ**: `device_map={'': self.gpu_id}` â†’ GPU ID ì¶©ëŒ
- **í•´ê²°**: `device_map={'': 0}` (CUDA_VISIBLE_DEVICES ì‚¬ìš© ì‹œ)
- **íŒŒì¼**: `experiment_2_L1_31_top300.py:119`

### 2. âœ… JAX ì˜ì¡´ì„± ì œê±°
- **ë¬¸ì œ**: Gemma ë¡œë”© ì‹œ `AttributeError: _ARRAY_API not found`
- **í•´ê²°**: `pip uninstall -y jax jaxlib`
- **ê²°ê³¼**: Gemma ëª¨ë¸ ë¡œë”© ì„±ê³µ

### 3. âœ… CUDA_VISIBLE_DEVICES ì „íŒŒ
- **ë¬¸ì œ**: tmux ëª…ë ¹ì–´ì—ì„œ í™˜ê²½ë³€ìˆ˜ ë¯¸ì „ë‹¬
- **í•´ê²°**: `env CUDA_VISIBLE_DEVICES=X python ...` í˜•ì‹ ì‚¬ìš©
- **ì ìš©**: ëª¨ë“  launcher ìŠ¤í¬ë¦½íŠ¸

---

## ğŸ“ ì‹¤í—˜ ì½”ë“œ ë° ê²°ê³¼ ê²½ë¡œ

### ì‹¤í—˜ ì½”ë“œ
```
/home/ubuntu/llm_addiction/
â”œâ”€â”€ experiment_0_llama_gemma_restart/
â”‚   â”œâ”€â”€ experiment_0_restart.py â† Exp0 ì½”ë“œ
â”‚   â””â”€â”€ logs/
â”‚       â”œâ”€â”€ exp0_llama.log â† LLaMA ë¡œê·¸
â”‚       â””â”€â”€ exp0_gemma.log â† Gemma ë¡œê·¸
â”œâ”€â”€ experiment_2_multilayer_patching_L1_31/
â”‚   â”œâ”€â”€ experiment_2_L1_31_top300.py â† Exp2 ì½”ë“œ
â”‚   â”œâ”€â”€ launch_safe.sh â† ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
â”‚   â””â”€â”€ logs/
â”‚       â”œâ”€â”€ exp2_L1_8_gpu2.log
â”‚       â”œâ”€â”€ exp2_L9_16_gpu5.log
â”‚       â”œâ”€â”€ exp2_L17_24_gpu6.log
â”‚       â””â”€â”€ exp2_L25_31_gpu7.log
â”œâ”€â”€ experiment_1_layer_pathway_L1_31/
â”‚   â””â”€â”€ experiment_1_pathway.py â† Exp1 ì½”ë“œ (ì™„ë£Œ)
â””â”€â”€ experiment_3_feature_word_6400/
    â””â”€â”€ experiment_3_feature_word.py â† Exp3 ì½”ë“œ (ì™„ë£Œ)
```

### ê²°ê³¼ ë°ì´í„°
```
/data/llm_addiction/
â”œâ”€â”€ experiment_0_llama_restart/ â† Exp0 LLaMA ê²°ê³¼ (ì§„í–‰ ì¤‘)
â”œâ”€â”€ experiment_0_gemma_restart/ â† Exp0 Gemma ê²°ê³¼ (ë©ˆì¶¤)
â”œâ”€â”€ experiment_2_multilayer_patching/ â† Exp2 ê²°ê³¼ (ì§„í–‰ ì¤‘)
â”œâ”€â”€ experiment_1_pathway_L1_31/
â”‚   â””â”€â”€ final_pathway_L1_31_20251001_165207.json â† Exp1 ì™„ë£Œ (2.7GB)
â””â”€â”€ experiment_4_feature_word_analysis/
    â””â”€â”€ feature_word_analysis_20251001_000025.json â† Exp3 ì™„ë£Œ
```

### Feature ë°ì´í„°
```
/data/llm_addiction/experiment_1_L1_31_extraction/
â””â”€â”€ L1_31_features_FINAL_20250930_220003.json â† 87,012 features (29MB)
```

---

## ğŸ¯ ë‹¤ìŒ ë‹¨ê³„ ê¶Œì¥ì‚¬í•­

### ì¦‰ì‹œ ì¡°ì¹˜ í•„ìš”:
1. **Exp0 Gemma**: ì¬ì‹œì‘ í•„ìš” (ë©”ëª¨ë¦¬ ìµœì í™”)
2. **Exp2 Patching**: Python unbuffered modeë¡œ ì¬ì‹¤í–‰

### ì¥ê¸° í•´ê²°ì±…:
1. SAE loaderë¥¼ shared memory ë°©ì‹ìœ¼ë¡œ ë³€ê²½
2. Gemma generation íŒŒë¼ë¯¸í„° ìµœì í™”
3. Checkpoint pre-loading ë©”ì»¤ë‹ˆì¦˜ ì¶”ê°€

---

## ğŸ“Š ì˜ˆìƒ ì™„ë£Œ ì‹œê°„

| ì‹¤í—˜ | í˜„ì¬ ìƒíƒœ | ì˜ˆìƒ ì™„ë£Œ | ì†Œìš” ì‹œê°„ |
|------|-----------|-----------|-----------|
| Exp0 LLaMA | 6/128 (4.7%) | ~4ì‹œê°„ | 125s/ì¡°ê±´ |
| Exp0 Gemma | ë©ˆì¶¤ | ì¬ì‹œì‘ í›„ 4ì‹œê°„ | - |
| Exp2 Patching | ë¡œë”© ì¤‘ | í•´ê²° í›„ 12-15ì‹œê°„ | - |
| Exp1 Pathway | ì™„ë£Œ | - | - |
| Exp3 Word | ì™„ë£Œ | - | - |

**ì „ì²´ ì™„ë£Œ ì˜ˆìƒ**: ë¬¸ì œ í•´ê²° í›„ ì•½ 15-20ì‹œê°„

---

*ë³´ê³ ì„œ ìƒì„±: 2025-10-02 04:50 KST*
