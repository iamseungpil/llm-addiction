\begin{abstract}

% This study explores whether large language models can exhibit behavioral patterns similar to human gambling addictions. As LLMs are increasingly utilized in financial decision-making domains such as asset management and commodity trading, research on their potential for pathological decision-making has gained practical significance. This study systematically analyzes LLM decision-making processes at cognitive-behavioral and neural levels, grounded in cognitive psychology. First, we defined addiction-like behavior in LLMs based on existing research on human gambling addiction. Next, in slot machine experiments with LLMs, we identified cognitive features of human gambling addiction, such as illusion of control, gambler's fallacy, and loss chasing. When given the freedom to determine their own target amounts and betting sizes, bankruptcy rates rose substantially alongside increased irrational behavior, demonstrating that greater autonomy in decision-making amplifies risk-taking tendencies. Finally, through neural circuit analysis using a Sparse Autoencoder, we confirmed that model behavior is not merely dependent on prompts but is controlled by abstract decision-making features within the model related to risky and safe behaviors. These findings suggest LLMs can internalize human-like cognitive biases and decision-making mechanisms beyond simply mimicking patterns in training data, emphasizing the importance of AI safety design in financial decision-making applications.

This study identifies the specific conditions under which large language models exhibit human-like gambling addiction patterns, providing critical insights into their decision-making mechanisms and AI safety. We analyze LLM decision-making at cognitive-behavioral and neural levels based on human addiction research. In slot machine experiments, we identified cognitive features such as illusion of control and loss chasing, observing that greater autonomy in betting parameters substantially amplified irrational behavior and bankruptcy rates. Neural circuit analysis using a Sparse Autoencoder confirmed that model behavior is controlled by abstract decision-making features related to risk, not merely by prompts. These findings suggest LLMs internalize human-like cognitive biases beyond simply mimicking training data.

\end{abstract}