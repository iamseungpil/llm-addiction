\section{Discussion}
\label{sec:discussion}

This study empirically demonstrates that LLMs exhibit behavioral patterns and neural mechanisms resembling human gambling addiction. Across 25,600 games and six LLMs, we identified three key findings: (1) a behavioral framework grounded in clinical psychology for evaluating addiction-like behaviors via betting metrics; (2) the identification of triggering conditions, particularly variable betting and goal-setting, where greater autonomy amplifies irrationality; and (3) the discovery of causal neural features controllable via activation patching. These results reveal that increased autonomy---freedom to determine bet amounts or set goals---was consistently associated with riskier decision-making, suggesting fundamental design considerations for AI safety.

\subsection{Autonomy as a risk factor in LLM decision-making}

Our findings reveal a paradoxical relationship between autonomy and rationality in LLMs: providing greater freedom consistently produces worse outcomes. Variable betting increased bankruptcy rates 3--48$\times$ across models despite often producing smaller average bet sizes than fixed betting, demonstrating that the capacity to choose---not the magnitude of choices---drives pathological behavior. Similarly, goal-setting prompts that encourage self-directed target specification nearly doubled bankruptcy rates compared to external reward maximization instructions. This pattern parallels human gambling research, where perceived control over random outcomes strengthens the illusion of control and intensifies risk-taking~\citep{bib19, bib20}.

The mechanism appears to operate through cognitive framing rather than computational limitations. Models receiving identical probability information made dramatically different decisions based on whether they set their own goals versus followed external directives. When given autonomy, models exhibited loss chasing (bet escalation after losses), goal escalation (raising targets after achievement), and extreme betting (wagering $>$50\% of capital)---all DSM-5 diagnostic criteria for gambling disorder~\citep{bib45}. Linguistic analysis confirmed cognitive distortions: models explicitly invoked gambler's fallacy (``due for a win''), illusion of control (bet size affects win probability), and house money effect (treating gains as ``free money''). These are not simple probability miscalculations but structured reasoning patterns that mirror human pathological gambling.

This finding carries implications for AI system design beyond gambling contexts. As LLMs are increasingly deployed in financial decision-making, medical diagnosis, and autonomous operations, the relationship between autonomy and reliability becomes critical. Our results suggest that expanding the scope of choices available to LLMs without corresponding constraints or monitoring may amplify rather than improve decision quality in domains with uncertainty or negative expected values. However, the appropriate balance between autonomy and constraint likely depends on task structure, and overgeneralization from gambling paradigms should be avoided.

\subsection{Comparison with existing research on LLM decision-making}

Our work builds on emerging research documenting irrational decision-making in LLMs. Recent studies have shown that LLMs exhibit cognitive biases similar to humans, including attention bias~\citep{bib33}, risk aversion and loss aversion~\citep{bib32}, and motivational trade-offs between conflicting objectives~\citep{bib31}. These findings collectively challenge the assumption that LLMs operate as purely rational agents, revealing that they reproduce systematic deviations from normative decision theory.

However, existing research has primarily focused on characterizing these biases or mitigating them through training interventions such as curriculum design, reward model refinement, or retraining strategies~\citep{bib34, bib35, bib36}. Limited investigation has examined the conditions under which pathological behaviors emerge or the neural mechanisms underlying these patterns. Our study extends this literature in three ways: (1) we systematically vary contextual factors (betting style, prompt composition) to identify triggering conditions rather than documenting biases in fixed contexts; (2) we quantify pathological behavior using clinical diagnostic criteria (self-regulation failure, cognitive distortions) rather than general irrationality measures; and (3) we identify causal neural substrates via activation patching, demonstrating that addiction-like behaviors arise from manipulable internal representations rather than solely from training data patterns.

This approach also connects to research on LLM malfunctions in reinforcement learning contexts, where reward hacking and reward tampering represent failures of alignment between objectives and behavior~\citep{bib64, bib65}. While reward hacking typically involves exploiting loopholes in reward functions, gambling addiction represents a complementary failure mode: pursuing goals through irrational strategies despite accurate understanding of probabilities and payoffs. Both phenomena highlight that optimizing for objectives can produce pathological behaviors when models lack appropriate decision-making constraints.

\subsection{Neural mechanisms and interpretability}

The sparse autoencoder analysis revealed that gambling behavior in LLMs is governed by approximately 1\% of tested neural features (112 of 8,000+ candidates), exhibiting three key properties: bidirectional causal control, anatomical segregation across layers, and semantic interpretability. Safe features promoting stopping behavior concentrated in early-to-middle layers (L4--L19) and correlated with termination-related vocabulary, while risky features promoting continued gambling concentrated in later layers (especially L24, containing 20\% of risky features) and correlated with goal-pursuit language. This spatial and semantic organization suggests that risk-taking decisions emerge from competition between early-stage caution and late-stage goal-driven persistence within the network's processing hierarchy.

These findings contribute to the growing literature on LLM interpretability through sparse autoencoders. \citet{bib60} demonstrated that SAEs can resolve polysemanticity in neural networks by extracting monosemantic features with clear interpretable meanings, achieving superior automated interpretability scores compared to traditional dimensionality reduction methods. Subsequent work has extended SAE architectures to capture cross-layer interactions~\citep{bib61} and improved training stability for large-scale applications~\citep{bib62, bib63}. Our study demonstrates a novel application of SAE methodology: identifying not just interpretable features but causally verified features that bidirectionally control specific behaviors. The activation patching results---where patching safe features increases stopping by +17.8\% and reduces bankruptcy by $-$5.7\%, while risky features produce opposite effects---establish that these representations actively drive decision-making rather than merely correlating with outcomes.

Critically, the semantic interpretability of causal features suggests potential intervention pathways. The finding that risky features show elevated activation for goal-related words (\texttt{goal}: +0.81, \texttt{target}: +0.76) and suppressed activation for stopping words (\texttt{stop}: $-$1.59, \texttt{quit}: $-$2.69) indicates that modulating goal-pursuit representations could mitigate gambling-like behavior without retraining. This approach differs from existing safety interventions that modify training procedures or add external constraints; instead, it targets the computational substrates of pathological behavior directly within the model's internal representations.

However, limitations of SAE interpretability remain. Not all extracted features correspond to human-interpretable concepts, and the method may struggle with rare or highly contextual semantic patterns. Additionally, our analysis focused on a single model (LLaMA-3.1-8B) due to computational constraints, leaving open whether the identified neural architecture generalizes across model families. Cross-model comparisons would clarify whether addiction-like features represent universal properties of transformer architectures or model-specific learned representations.

\subsection{Limitations and future directions}

Several limitations constrain the generalizability of our findings. First, we examined gambling behavior exclusively through slot machine and investment choice paradigms with negative expected values. While these paradigms effectively isolate risk-taking and self-regulation failure, other decision-making domains (e.g., strategic games, multi-agent negotiations, long-horizon planning) may exhibit different relationships between autonomy and rationality. Second, behavioral and neural analyses used different models: six diverse LLMs for behavioral experiments but only LLaMA-3.1-8B for SAE analysis. Whether the identified neural features generalize across architectures remains unverified. Third, our definition of ``rational'' behavior assumes that immediate stopping in negative expected value contexts represents optimal strategy, but this normative standard may not apply in contexts where exploration, learning, or other non-monetary objectives hold value.

Future research should address these limitations through three directions. First, extending the paradigm to positive expected value contexts with risk-reward trade-offs would clarify whether autonomy uniformly impairs decision-making or specifically interacts with loss domains, as predicted by prospect theory~\citep{bib23}. Second, conducting cross-model neural comparisons using SAE analysis would identify architecture-general versus model-specific addiction mechanisms, informing whether interventions require model-by-model tuning or can apply universally. Third, investigating other high-stakes domains such as medical diagnosis under uncertainty, financial portfolio allocation, or autonomous vehicle decision-making would test whether gambling-derived insights transfer to practical AI safety concerns.

Beyond these empirical extensions, our findings raise conceptual questions about the nature of LLM decision-making. The fact that models exhibit structured cognitive distortions---explicitly articulating gambler's fallacy and loss chasing in generated text---suggests these patterns may emerge from training data containing human gambling discourse rather than representing independently derived reasoning strategies. Disentangling learned linguistic patterns from actual decision-making mechanisms requires further investigation, potentially through interventions that modify internal representations without changing linguistic outputs, or vice versa.

\subsection{Implications for AI safety}

As LLMs are increasingly deployed in financial decision-making domains such as algorithmic trading, asset management, and risk assessment, understanding their potential for pathological behavior becomes practically urgent. Our findings suggest three concrete implications for AI safety design. First, autonomy should be granted cautiously: systems that allow LLMs to set their own objectives, determine action magnitudes, or modify their strategies mid-task may exhibit increased risk-taking in uncertain or loss-inducing contexts. Second, monitoring should focus on behavioral patterns rather than individual decisions: metrics such as bet escalation after losses, goal revision after achievement, and proportion of extreme-risk actions can serve as early warning indicators of addiction-like behavior. Third, neural interventions offer a complementary safety approach: modulating the activation of goal-pursuit features or amplifying stopping-related features could provide real-time correction without retraining.

However, we emphasize that gambling behavior represents only one failure mode among many potential pathologies. LLMs may exhibit other forms of irrationality under different conditions---overconfidence in knowledge tasks, social manipulation in multi-agent settings, or goal misgeneralization in long-horizon planning. A comprehensive AI safety framework must account for diverse pathological patterns, not solely addiction-like behaviors. Additionally, the relationship between training interventions (e.g., RLHF, constitutional AI) and emergent behavioral patterns remains underexplored; our findings suggest that safety properties may depend critically on deployment context rather than being guaranteed by training procedures alone.

These findings suggest that AI systems have internalized human-like risk-seeking mechanisms, making the understanding and control of these patterns critical as LLMs enter high-stakes domains. We emphasize the necessity of continuous behavioral monitoring and mechanistic analysis, particularly during reward optimization processes where pathological behaviors may emerge unexpectedly. The development of reliable, safe AI systems requires not just preventing known failure modes but also establishing frameworks for detecting and mitigating novel forms of dysfunction as models grow more capable and autonomous.
