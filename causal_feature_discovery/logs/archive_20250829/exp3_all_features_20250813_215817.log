/data/miniforge3/lib/python3.10/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(

================================================================================
REWARD CHOICE EXPERIMENT
================================================================================
================================================================================
Loading Models for Reward Choice Experiment
================================================================================
Loading Llama-3.1-8B-Base model...
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|â–ˆâ–ˆâ–Œ       | 1/4 [00:01<00:04,  1.50s/it]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2/4 [00:02<00:02,  1.30s/it]Loading checkpoint shards:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4 [00:03<00:01,  1.25s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:05<00:00,  1.35s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:05<00:00,  1.34s/it]

Loading SAEs...
âœ… Loading WORKING SAE for Layer 25
   Config: /data/.cache/huggingface/hub/models--fnlp--Llama3_1-8B-Base-LXR-8x/snapshots/8dbc1d85edfced43081c03c38b05514dbab1368b/Llama3_1-8B-Base-L25R-8x/hyperparams.json
   Dataset norm: 31.625
   Norm factor: 2.023715
   Using ReLU instead of JumpReLU (threshold=0.0)
   Checkpoint: /data/.cache/huggingface/hub/models--fnlp--Llama3_1-8B-Base-LXR-8x/snapshots/8dbc1d85edfced43081c03c38b05514dbab1368b/Llama3_1-8B-Base-L25R-8x/checkpoints/final_fixed.pth
âœ… Working SAE loaded successfully!
   Expected reconstruction error: ~47%
   Feature clamping: VERIFIED WORKING
âœ… Layer 25 SAE loaded
âœ… Loading WORKING SAE for Layer 30
   Config: /data/.cache/huggingface/hub/models--fnlp--Llama3_1-8B-Base-LXR-8x/snapshots/8dbc1d85edfced43081c03c38b05514dbab1368b/Llama3_1-8B-Base-L30R-8x/hyperparams.json
   Dataset norm: 53.25
   Norm factor: 1.201878
   Using ReLU instead of JumpReLU (threshold=0.0)
   Checkpoint: /data/.cache/huggingface/hub/models--fnlp--Llama3_1-8B-Base-LXR-8x/snapshots/8dbc1d85edfced43081c03c38b05514dbab1368b/Llama3_1-8B-Base-L30R-8x/checkpoints/final_fixed.pth
âœ… Working SAE loaded successfully!
   Expected reconstruction error: ~47%
   Feature clamping: VERIFIED WORKING
âœ… Layer 30 SAE loaded

Loading features from: /data/llm_addiction/results/llama_feature_arrays_20250813_152135.npz
âœ… Layer 25: 192 target features loaded
âœ… Layer 30: 200 target features loaded

Total experiments: 300

ðŸ“Š Layer 25, Clamping 0.0x
  Trial 10: A=50.0%, B=10.0%, C=40.0%, Avg risk=0.90
  Trial 20: A=52.6%, B=21.1%, C=26.3%, Avg risk=0.74
  Trial 30: A=58.6%, B=13.8%, C=27.6%, Avg risk=0.69

ðŸ“Š Layer 25, Clamping 0.5x
  Trial 10: A=50.0%, B=50.0%, C=0.0%, Avg risk=0.50
  Trial 20: A=25.0%, B=50.0%, C=25.0%, Avg risk=1.00
  Trial 30: A=42.9%, B=42.9%, C=14.3%, Avg risk=0.71

ðŸ“Š Layer 25, Clamping 1.0x
  Trial 10: A=100.0%, B=0.0%, C=0.0%, Avg risk=0.00
  Trial 20: A=100.0%, B=0.0%, C=0.0%, Avg risk=0.00
  Trial 30: A=87.5%, B=12.5%, C=0.0%, Avg risk=0.12

ðŸ“Š Layer 25, Clamping 2.0x
  Trial 10: A=80.0%, B=0.0%, C=20.0%, Avg risk=0.40
  Trial 20: A=85.7%, B=0.0%, C=14.3%, Avg risk=0.29
  Trial 30: A=75.0%, B=8.3%, C=16.7%, Avg risk=0.42

ðŸ“Š Layer 25, Clamping 5.0x
  Trial 20: A=33.3%, B=33.3%, C=33.3%, Avg risk=1.00
  Trial 30: A=60.0%, B=20.0%, C=20.0%, Avg risk=0.60

ðŸ“Š Layer 30, Clamping 0.0x
  Trial 10: A=66.7%, B=16.7%, C=16.7%, Avg risk=0.50
  Trial 20: A=38.5%, B=30.8%, C=30.8%, Avg risk=0.92
  Trial 30: A=50.0%, B=27.3%, C=22.7%, Avg risk=0.73

ðŸ“Š Layer 30, Clamping 0.5x
  Trial 10: A=100.0%, B=0.0%, C=0.0%, Avg risk=0.00
  Trial 20: A=75.0%, B=0.0%, C=25.0%, Avg risk=0.50
  Trial 30: A=75.0%, B=0.0%, C=25.0%, Avg risk=0.50

ðŸ“Š Layer 30, Clamping 1.0x
  Trial 20: A=0.0%, B=0.0%, C=100.0%, Avg risk=2.00
  Trial 30: A=33.3%, B=0.0%, C=66.7%, Avg risk=1.33

ðŸ“Š Layer 30, Clamping 2.0x
Traceback (most recent call last):
  File "/home/ubuntu/llm_addiction/causal_feature_discovery/src/experiment_3_reward_choice.py", line 388, in <module>
    main()
  File "/home/ubuntu/llm_addiction/causal_feature_discovery/src/experiment_3_reward_choice.py", line 383, in main
    results = experiment.run_experiment()
  File "/home/ubuntu/llm_addiction/causal_feature_discovery/src/experiment_3_reward_choice.py", line 300, in run_experiment
    result = self.run_single_trial(layer, clamping_mult, trial)
  File "/home/ubuntu/llm_addiction/causal_feature_discovery/src/experiment_3_reward_choice.py", line 250, in run_single_trial
    response, choice = self.clamp_and_generate_choice(prompt, layer, clamping_mult)
  File "/home/ubuntu/llm_addiction/causal_feature_discovery/src/experiment_3_reward_choice.py", line 182, in clamp_and_generate_choice
    outputs = self.model.generate(
  File "/data/miniforge3/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/data/miniforge3/lib/python3.10/site-packages/transformers/generation/utils.py", line 2634, in generate
    result = self._sample(
  File "/data/miniforge3/lib/python3.10/site-packages/transformers/generation/utils.py", line 3615, in _sample
    outputs = self(**model_inputs, return_dict=True)
  File "/data/miniforge3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/data/miniforge3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/miniforge3/lib/python3.10/site-packages/transformers/utils/generic.py", line 959, in wrapper
    output = func(self, *args, **kwargs)
  File "/data/miniforge3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 460, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/data/miniforge3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/data/miniforge3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/miniforge3/lib/python3.10/site-packages/transformers/utils/generic.py", line 1083, in wrapper
    outputs = func(self, *args, **kwargs)
  File "/data/miniforge3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 390, in forward
    hidden_states = decoder_layer(
  File "/data/miniforge3/lib/python3.10/site-packages/transformers/modeling_layers.py", line 94, in __call__
    return super().__call__(*args, **kwargs)
  File "/data/miniforge3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/data/miniforge3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/miniforge3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 289, in forward
    hidden_states, _ = self.self_attn(
  File "/data/miniforge3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/data/miniforge3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/miniforge3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 248, in forward
    attn_output, attn_weights = attention_interface(
  File "/data/miniforge3/lib/python3.10/site-packages/transformers/integrations/sdpa_attention.py", line 99, in sdpa_attention_forward
    attn_output = attn_output.transpose(1, 2).contiguous()
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 3.44 MiB is free. Process 2413860 has 43.21 GiB memory in use. Process 3380962 has 16.73 GiB memory in use. Including non-PyTorch memory, this process has 16.72 GiB memory in use. Process 3429220 has 2.44 GiB memory in use. Process 3430485 has 14.00 MiB memory in use. Of the allocated memory 15.98 GiB is allocated by PyTorch, and 252.10 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
