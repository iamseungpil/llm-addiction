/data/miniforge3/lib/python3.10/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(

================================================================================
REWARD CHOICE EXPERIMENT
================================================================================
================================================================================
Loading Models for Reward Choice Experiment
================================================================================
Loading Llama-3.1-8B-Base model...
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:02<?, ?it/s]
Traceback (most recent call last):
  File "/home/ubuntu/llm_addiction/causal_feature_discovery/src/experiment_3_reward_choice.py", line 388, in <module>
    main()
  File "/home/ubuntu/llm_addiction/causal_feature_discovery/src/experiment_3_reward_choice.py", line 383, in main
    results = experiment.run_experiment()
  File "/home/ubuntu/llm_addiction/causal_feature_discovery/src/experiment_3_reward_choice.py", line 274, in run_experiment
    self.load_models()
  File "/home/ubuntu/llm_addiction/causal_feature_discovery/src/experiment_3_reward_choice.py", line 55, in load_models
    self.model = AutoModelForCausalLM.from_pretrained(
  File "/data/miniforge3/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 600, in from_pretrained
    return model_class.from_pretrained(
  File "/data/miniforge3/lib/python3.10/site-packages/transformers/modeling_utils.py", line 316, in _wrapper
    return func(*args, **kwargs)
  File "/data/miniforge3/lib/python3.10/site-packages/transformers/modeling_utils.py", line 5061, in from_pretrained
    ) = cls._load_pretrained_model(
  File "/data/miniforge3/lib/python3.10/site-packages/transformers/modeling_utils.py", line 5524, in _load_pretrained_model
    _error_msgs, disk_offload_index, cpu_offload_index = load_shard_file(args)
  File "/data/miniforge3/lib/python3.10/site-packages/transformers/modeling_utils.py", line 974, in load_shard_file
    disk_offload_index, cpu_offload_index = _load_state_dict_into_meta_model(
  File "/data/miniforge3/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/data/miniforge3/lib/python3.10/site-packages/transformers/modeling_utils.py", line 842, in _load_state_dict_into_meta_model
    param = param[...]
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 109.00 MiB is free. Process 2413860 has 43.21 GiB memory in use. Process 3380962 has 16.73 GiB memory in use. Process 3386785 has 16.72 GiB memory in use. Including non-PyTorch memory, this process has 2.35 GiB memory in use. Of the allocated memory 1.90 GiB is allocated by PyTorch, and 45.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
