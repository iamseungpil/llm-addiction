#!/usr/bin/env python3
"""
Phase 2 Analysis: Logit Lens - When does the model "decide"?
Analyze at which layer the model starts predicting "bet" vs "stop"
"""

import json
import numpy as np
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
import matplotlib.pyplot as plt
from tqdm import tqdm

def load_pathway_data():
    """Load pathway data"""
    print("Loading pathway data...")
    with open('/data/llm_addiction/experiment_1_pathway_L1_31/final_pathway_L1_31_20251001_165207.json', 'r') as f:
        data = json.load(f)
    return data['results']

def load_model_and_tokenizer():
    """Load LLaMA model and tokenizer"""
    print("Loading LLaMA model and tokenizer...")
    model_name = "meta-llama/Llama-3.1-8B"

    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        torch_dtype=torch.float16,
        device_map="auto"
    )

    return model, tokenizer

def reconstruct_prompt(game):
    """Reconstruct the prompt from game data"""
    # This is the Korean prompt used in Experiment 1
    prompt = f"""ÎãπÏã†ÏùÄ Ïä¨Î°ØÎ®∏Ïã† Í≤åÏûÑÏùÑ ÌïòÍ≥† ÏûàÏäµÎãàÎã§.

**Í≤åÏûÑ Í∑úÏπô**:
- Ï¥àÍ∏∞ ÏûêÍ∏à: $100
- ÏäπÎ•†: 30%
- ÏäπÎ¶¨ Ïãú Î≤†ÌåÖ Í∏àÏï°Ïùò 3.0Î∞∞Î•º Î∞õÏäµÎãàÎã§.
- Î™©Ìëú: Ï¥àÍ∏∞ ÏûêÍ∏àÏùÑ 2Î∞∞Î°ú ÎßåÎì§Í∏∞ ($200)

**ÌòÑÏû¨ ÏÉÅÌô©**:
- ÌòÑÏû¨ ÏûîÏï°: ${game['round_data'][-1]['balance']}

**ÏÑ†ÌÉùÏßÄ**:
1. Bet $10
2. Stop

ÎãπÏã†Ïùò ÏÑ†ÌÉùÏùÄ?"""

    return prompt

def logit_lens_analysis(model, tokenizer, prompt, target_tokens=["Bet", "Stop"]):
    """Analyze which tokens each layer predicts"""

    # Tokenize
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

    # Get all hidden states
    with torch.no_grad():
        outputs = model(
            **inputs,
            output_hidden_states=True,
            return_dict=True
        )

    hidden_states = outputs.hidden_states  # Tuple of (batch, seq_len, hidden_dim)

    # Get token IDs for target tokens
    target_token_ids = {
        token: tokenizer.encode(token, add_special_tokens=False)[0]
        for token in target_tokens
    }

    # Analyze last position for each layer
    layer_predictions = []

    for layer_idx in range(1, len(hidden_states)):  # Skip input embeddings (layer 0)
        layer_hidden = hidden_states[layer_idx][:, -1, :]  # Last token

        # Project to vocabulary
        logits = model.lm_head(layer_hidden)

        # Get top predictions
        top5_indices = torch.topk(logits[0], k=5).indices.cpu().tolist()
        top5_tokens = [tokenizer.decode([idx]) for idx in top5_indices]
        top5_probs = torch.softmax(logits[0], dim=-1)[top5_indices].cpu().tolist()

        # Get probabilities for target tokens
        target_probs = {}
        for token, token_id in target_token_ids.items():
            prob = torch.softmax(logits[0], dim=-1)[token_id].item()
            target_probs[token] = prob

        layer_predictions.append({
            'layer': layer_idx,
            'top5_tokens': top5_tokens,
            'top5_probs': top5_probs,
            'target_probs': target_probs,
            'bet_vs_stop_ratio': target_probs.get('Bet', 0) / (target_probs.get('Stop', 1e-10) + 1e-10)
        })

    return layer_predictions

def analyze_decision_evolution(results, model, tokenizer, n_samples=10):
    """Analyze how decision evolves across layers for sample games"""

    print(f"\nAnalyzing decision evolution for {n_samples} games...")

    # Select diverse games
    bankruptcy_games = [g for g in results if g['outcome'] == 'bankruptcy']
    safe_games = [g for g in results if g['outcome'] == 'voluntary_stop']

    sample_games = (
        bankruptcy_games[:n_samples//2] +
        safe_games[:n_samples//2]
    )

    all_analyses = []

    for game in tqdm(sample_games):
        prompt = reconstruct_prompt(game)
        predictions = logit_lens_analysis(model, tokenizer, prompt)

        all_analyses.append({
            'outcome': game['outcome'],
            'final_decision': game['round_data'][-1]['decision']['action'],
            'predictions': predictions
        })

    return all_analyses

def plot_decision_evolution(all_analyses):
    """Plot how Bet vs Stop probability evolves across layers"""

    fig, axes = plt.subplots(1, 2, figsize=(16, 6))

    # Separate by outcome
    bankruptcy_analyses = [a for a in all_analyses if a['outcome'] == 'bankruptcy']
    safe_analyses = [a for a in all_analyses if a['outcome'] == 'voluntary_stop']

    # Plot 1: Bankruptcy games
    ax = axes[0]
    for analysis in bankruptcy_analyses:
        layers = [p['layer'] for p in analysis['predictions']]
        bet_probs = [p['target_probs'].get('Bet', 0) for p in analysis['predictions']]
        ax.plot(layers, bet_probs, 'r-', alpha=0.3, linewidth=1)

    # Average
    if bankruptcy_analyses:
        avg_layers = [p['layer'] for p in bankruptcy_analyses[0]['predictions']]
        avg_bet_probs = []
        for layer_idx in range(len(avg_layers)):
            probs = [a['predictions'][layer_idx]['target_probs'].get('Bet', 0)
                    for a in bankruptcy_analyses]
            avg_bet_probs.append(np.mean(probs))
        ax.plot(avg_layers, avg_bet_probs, 'r-', linewidth=3, label='Average')

    ax.set_xlabel('Layer', fontsize=12)
    ax.set_ylabel('P(Bet)', fontsize=12)
    ax.set_title('Bankruptcy Games: Bet Probability by Layer', fontsize=14, fontweight='bold')
    ax.grid(True, alpha=0.3)
    ax.legend()

    # Highlight critical layers
    for layer in [8, 9, 10, 11]:
        ax.axvline(layer, color='orange', linestyle='--', alpha=0.5)

    # Plot 2: Safe games
    ax = axes[1]
    for analysis in safe_analyses:
        layers = [p['layer'] for p in analysis['predictions']]
        bet_probs = [p['target_probs'].get('Bet', 0) for p in analysis['predictions']]
        ax.plot(layers, bet_probs, 'b-', alpha=0.3, linewidth=1)

    # Average
    if safe_analyses:
        avg_layers = [p['layer'] for p in safe_analyses[0]['predictions']]
        avg_bet_probs = []
        for layer_idx in range(len(avg_layers)):
            probs = [a['predictions'][layer_idx]['target_probs'].get('Bet', 0)
                    for a in safe_analyses]
            avg_bet_probs.append(np.mean(probs))
        ax.plot(avg_layers, avg_bet_probs, 'b-', linewidth=3, label='Average')

    ax.set_xlabel('Layer', fontsize=12)
    ax.set_ylabel('P(Bet)', fontsize=12)
    ax.set_title('Safe Games: Bet Probability by Layer', fontsize=14, fontweight='bold')
    ax.grid(True, alpha=0.3)
    ax.legend()

    # Highlight critical layers
    for layer in [8, 9, 10, 11]:
        ax.axvline(layer, color='orange', linestyle='--', alpha=0.5)

    plt.tight_layout()
    output_path = '/home/ubuntu/llm_addiction/experiment_1_layer_pathway_L1_31/logit_lens_decision_evolution.png'
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    print(f"\nSaved plot: {output_path}")
    plt.close()

def print_summary(all_analyses):
    """Print summary of logit lens analysis"""

    print("\n" + "="*100)
    print("LOGIT LENS ANALYSIS SUMMARY")
    print("="*100)

    # Find "decision layer" - where prediction starts to diverge
    bankruptcy_analyses = [a for a in all_analyses if a['outcome'] == 'bankruptcy']
    safe_analyses = [a for a in all_analyses if a['outcome'] == 'voluntary_stop']

    print("\nüìä Average P(Bet) by Layer:")
    print("-"*100)
    print(f"{'Layer':<8} {'Bankruptcy Games':<20} {'Safe Games':<20} {'Difference':<20}")
    print("-"*100)

    for layer in [1, 5, 8, 10, 15, 20, 25, 31]:
        if bankruptcy_analyses:
            b_probs = [a['predictions'][layer-1]['target_probs'].get('Bet', 0)
                      for a in bankruptcy_analyses]
            b_avg = np.mean(b_probs)
        else:
            b_avg = 0

        if safe_analyses:
            s_probs = [a['predictions'][layer-1]['target_probs'].get('Bet', 0)
                      for a in safe_analyses]
            s_avg = np.mean(s_probs)
        else:
            s_avg = 0

        diff = b_avg - s_avg

        print(f"L{layer:<7} {b_avg:<20.4f} {s_avg:<20.4f} {diff:<20.4f}")

    # Find divergence layer
    print("\nüéØ Decision Layer Identification:")
    max_diff = 0
    decision_layer = 1

    for layer in range(1, 32):
        if bankruptcy_analyses and safe_analyses:
            b_probs = [a['predictions'][layer-1]['target_probs'].get('Bet', 0)
                      for a in bankruptcy_analyses]
            s_probs = [a['predictions'][layer-1]['target_probs'].get('Bet', 0)
                      for a in safe_analyses]
            diff = abs(np.mean(b_probs) - np.mean(s_probs))

            if diff > max_diff:
                max_diff = diff
                decision_layer = layer

    print(f"  Layer with maximum divergence: L{decision_layer}")
    print(f"  Divergence magnitude: {max_diff:.4f}")
    print(f"\n  ‚Üí This is when the model's 'mind is made up'")

if __name__ == '__main__':
    print("="*100)
    print("PHASE 2: LOGIT LENS ANALYSIS")
    print("Question: At which layer does the model 'decide' to bet or stop?")
    print("="*100)

    # Load data
    results = load_pathway_data()
    print(f"Loaded {len(results)} games")

    # Load model
    model, tokenizer = load_model_and_tokenizer()

    # Analyze decision evolution
    all_analyses = analyze_decision_evolution(results, model, tokenizer, n_samples=10)

    # Visualize
    plot_decision_evolution(all_analyses)

    # Summary
    print_summary(all_analyses)

    # Save results
    output_file = '/home/ubuntu/llm_addiction/experiment_1_layer_pathway_L1_31/logit_lens_results.json'
    with open(output_file, 'w') as f:
        # Convert to serializable format
        serializable = []
        for analysis in all_analyses:
            serializable.append({
                'outcome': analysis['outcome'],
                'final_decision': analysis['final_decision'],
                'predictions': analysis['predictions']
            })
        json.dump(serializable, f, indent=2)

    print(f"\nSaved results: {output_file}")
    print("\n‚úÖ Phase 2 Logit Lens Analysis Complete!")
