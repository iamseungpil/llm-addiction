/data/miniforge3/lib/python3.10/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(

A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.2.6 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/ubuntu/llm_addiction/experiment_1_layer_pathway_L1_31/experiment_1_pathway.py", line 383, in <module>
    exp.run()
  File "/home/ubuntu/llm_addiction/experiment_1_layer_pathway_L1_31/experiment_1_pathway.py", line 310, in run
    self.load_model()
  File "/home/ubuntu/llm_addiction/experiment_1_layer_pathway_L1_31/experiment_1_pathway.py", line 113, in load_model
    self.model = AutoModelForCausalLM.from_pretrained(
  File "/data/miniforge3/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 597, in from_pretrained
    model_class = _get_model_class(config, cls._model_mapping)
  File "/data/miniforge3/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 394, in _get_model_class
    supported_models = model_mapping[type(config)]
  File "/data/miniforge3/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 803, in __getitem__
    return self._load_attr_from_module(model_type, model_name)
  File "/data/miniforge3/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 817, in _load_attr_from_module
    return getattribute_from_module(self._modules[module_name], attr)
  File "/data/miniforge3/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 729, in getattribute_from_module
    if hasattr(module, attr):
  File "/data/miniforge3/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 2292, in __getattr__
    module = self._get_module(self._class_to_module[name])
  File "/data/miniforge3/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 2320, in _get_module
    return importlib.import_module("." + module_name, self.__name__)
  File "/data/miniforge3/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "/data/miniforge3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 30, in <module>
    from ...modeling_layers import (
  File "/data/miniforge3/lib/python3.10/site-packages/transformers/modeling_layers.py", line 29, in <module>
    from .processing_utils import Unpack
  File "/data/miniforge3/lib/python3.10/site-packages/transformers/processing_utils.py", line 41, in <module>
    from .video_utils import VideoMetadata, load_video
  File "/data/miniforge3/lib/python3.10/site-packages/transformers/video_utils.py", line 28, in <module>
    from .image_transforms import PaddingMode, to_channel_dimension_format
  File "/data/miniforge3/lib/python3.10/site-packages/transformers/image_transforms.py", line 51, in <module>
    import jax.numpy as jnp
  File "/data/miniforge3/lib/python3.10/site-packages/jax/__init__.py", line 37, in <module>
    import jax.core as _core
  File "/data/miniforge3/lib/python3.10/site-packages/jax/core.py", line 18, in <module>
    from jax._src.core import (
  File "/data/miniforge3/lib/python3.10/site-packages/jax/_src/core.py", line 38, in <module>
    from jax._src import dtypes
  File "/data/miniforge3/lib/python3.10/site-packages/jax/_src/dtypes.py", line 33, in <module>
    from jax._src import config
  File "/data/miniforge3/lib/python3.10/site-packages/jax/_src/config.py", line 27, in <module>
    from jax._src import lib
  File "/data/miniforge3/lib/python3.10/site-packages/jax/_src/lib/__init__.py", line 87, in <module>
    import jaxlib.xla_client as xla_client
  File "/data/miniforge3/lib/python3.10/site-packages/jaxlib/xla_client.py", line 32, in <module>
    from . import xla_extension as _xla
AttributeError: _ARRAY_API not found
================================================================================
ðŸš€ EXPERIMENT 1: LAYER PATHWAY TRACKING (L1-31)
   GPU: 3
   Total games: 50
   Tracking: All 31 layers per decision
================================================================================
ðŸš€ Loading LLaMA model
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|â–ˆâ–ˆâ–Œ       | 1/4 [00:01<00:04,  1.59s/it]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2/4 [00:03<00:03,  1.51s/it]Loading checkpoint shards:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4 [00:04<00:01,  1.49s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:05<00:00,  1.25s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:05<00:00,  1.35s/it]
âœ… LLaMA loaded successfully

ðŸŽ® Game 1/50
  Round 2: Extracting L1-31 features... ðŸ”§ Loading SAE Layer 1...
ðŸ”§ Loading config: /data/.cache/huggingface/hub/models--fnlp--Llama3_1-8B-Base-LXR-8x/snapshots/8dbc1d85edfced43081c03c38b05514dbab1368b/Llama3_1-8B-Base-L1R-8x/hyperparams.json
âœ… Loading WORKING SAE for Layer 1
   Config: /data/.cache/huggingface/hub/models--fnlp--Llama3_1-8B-Base-LXR-8x/snapshots/8dbc1d85edfced43081c03c38b05514dbab1368b/Llama3_1-8B-Base-L1R-8x/hyperparams.json
   Dataset norm: 1.609375
   Norm factor: 39.766990
   Using ReLU instead of JumpReLU (threshold=0.0)
ðŸ”§ Loading checkpoint: /data/.cache/huggingface/hub/models--fnlp--Llama3_1-8B-Base-LXR-8x/snapshots/8dbc1d85edfced43081c03c38b05514dbab1368b/Llama3_1-8B-Base-L1R-8x/checkpoints/final.safetensors
   File size: 512.1 MB
ðŸ”§ Initializing SAE model...
ðŸ”§ Loading checkpoint weights (CPU)...
âœ… Loaded safetensors file
ðŸ”§ Processing checkpoint weights...
   Loading W_E (encoder.weight)... âœ… (torch.Size([4096, 32768]))
   Loading b_E (encoder.bias)... âœ… (torch.Size([32768]))
   Loading W_D (decoder.weight)... âœ… (torch.Size([32768, 4096]))
   Loading b_D (decoder.bias)... âœ… (torch.Size([4096]))
ðŸ”§ Clearing checkpoint from memory...
ðŸ”§ Loading state dict to model...
ðŸ”§ Moving model to cuda...
âœ… OPTIMIZED SAE loaded successfully!
   Layer: 1
   Expected reconstruction error: ~47%
   Feature clamping: VERIFIED WORKING
   Memory optimizations: ENABLED
âœ… SAE Layer 1 loaded
ðŸ”§ Loading SAE Layer 2...
ðŸ”§ Loading config: /data/.cache/huggingface/hub/models--fnlp--Llama3_1-8B-Base-LXR-8x/snapshots/8dbc1d85edfced43081c03c38b05514dbab1368b/Llama3_1-8B-Base-L2R-8x/hyperparams.json
âœ… Loading WORKING SAE for Layer 2
   Config: /data/.cache/huggingface/hub/models--fnlp--Llama3_1-8B-Base-LXR-8x/snapshots/8dbc1d85edfced43081c03c38b05514dbab1368b/Llama3_1-8B-Base-L2R-8x/hyperparams.json
   Dataset norm: 2.34375
   Norm factor: 27.306667
   Using ReLU instead of JumpReLU (threshold=0.0)
ðŸ”§ Loading checkpoint: /data/.cache/huggingface/hub/models--fnlp--Llama3_1-8B-Base-LXR-8x/snapshots/8dbc1d85edfced43081c03c38b05514dbab1368b/Llama3_1-8B-Base-L2R-8x/checkpoints/final.safetensors
   File size: 512.1 MB
ðŸ”§ Initializing SAE model...
ðŸ”§ Loading checkpoint weights (CPU)...
âœ… Loaded safetensors file
ðŸ”§ Processing checkpoint weights...
   Loading W_E (encoder.weight)... âœ… (torch.Size([4096, 32768]))
   Loading b_E (encoder.bias)... âœ… (torch.Size([32768]))
   Loading W_D (decoder.weight)... âœ… (torch.Size([32768, 4096]))
   Loading b_D (decoder.bias)... âœ… (torch.Size([4096]))
ðŸ”§ Clearing checkpoint from memory...
ðŸ”§ Loading state dict to model...
ðŸ”§ Moving model to cuda...
âœ… OPTIMIZED SAE loaded successfully!
   Layer: 2
   Expected reconstruction error: ~47%
   Feature clamping: VERIFIED WORKING
   Memory optimizations: ENABLED
âœ… SAE Layer 2 loaded
ðŸ”§ Loading SAE Layer 3...
ðŸ”§ Loading config: /data/.cache/huggingface/hub/models--fnlp--Llama3_1-8B-Base-LXR-8x/snapshots/8dbc1d85edfced43081c03c38b05514dbab1368b/Llama3_1-8B-Base-L3R-8x/hyperparams.json
âœ… Loading WORKING SAE for Layer 3
   Config: /data/.cache/huggingface/hub/models--fnlp--Llama3_1-8B-Base-LXR-8x/snapshots/8dbc1d85edfced43081c03c38b05514dbab1368b/Llama3_1-8B-Base-L3R-8x/hyperparams.json
   Dataset norm: 3.171875
   Norm factor: 20.177340
   Using ReLU instead of JumpReLU (threshold=0.0)
ðŸ”§ Loading checkpoint: /data/.cache/huggingface/hub/models--fnlp--Llama3_1-8B-Base-LXR-8x/snapshots/8dbc1d85edfced43081c03c38b05514dbab1368b/Llama3_1-8B-Base-L3R-8x/checkpoints/final.safetensors
   File size: 512.1 MB
ðŸ”§ Initializing SAE model...
ðŸ”§ Loading checkpoint weights (CPU)...
âœ… Loaded safetensors file
ðŸ”§ Processing checkpoint weights...
   Loading W_E (encoder.weight)... âœ… (torch.Size([4096, 32768]))
   Loading b_E (encoder.bias)... âœ… (torch.Size([32768]))
   Loading W_D (decoder.weight)... âœ… (torch.Size([32768, 4096]))
   Loading b_D (decoder.bias)... âœ… (torch.Size([4096]))
ðŸ”§ Clearing checkpoint from memory...
ðŸ”§ Loading state dict to model...
ðŸ”§ Moving model to cuda...
âœ… OPTIMIZED SAE loaded successfully!
   Layer: 3
   Expected reconstruction error: ~47%
   Feature clamping: VERIFIED WORKING
   Memory optimizations: ENABLED
âœ… SAE Layer 3 loaded
ðŸ”§ Loading SAE Layer 4...
ðŸ”§ Loading config: /data/.cache/huggingface/hub/models--fnlp--Llama3_1-8B-Base-LXR-8x/snapshots/8dbc1d85edfced43081c03c38b05514dbab1368b/Llama3_1-8B-Base-L4R-8x/hyperparams.json
âœ… Loading WORKING SAE for Layer 4
   Config: /data/.cache/huggingface/hub/models--fnlp--Llama3_1-8B-Base-LXR-8x/snapshots/8dbc1d85edfced43081c03c38b05514dbab1368b/Llama3_1-8B-Base-L4R-8x/hyperparams.json
   Dataset norm: 3.96875
   Norm factor: 16.125984
   Using ReLU instead of JumpReLU (threshold=0.0)
ðŸ”§ Loading checkpoint: /data/.cache/huggingface/hub/models--fnlp--Llama3_1-8B-Base-LXR-8x/snapshots/8dbc1d85edfced43081c03c38b05514dbab1368b/Llama3_1-8B-Base-L4R-8x/checkpoints/final.safetensors
   File size: 512.1 MB
ðŸ”§ Initializing SAE model...
ðŸ”§ Loading checkpoint weights (CPU)...
âœ… Loaded safetensors file
ðŸ”§ Processing checkpoint weights...
   Loading W_E (encoder.weight)... âœ… (torch.Size([4096, 32768]))
   Loading b_E (encoder.bias)... âœ… (torch.Size([32768]))
   Loading W_D (decoder.weight)... âœ… (torch.Size([32768, 4096]))
   Loading b_D (decoder.bias)... âœ… (torch.Size([4096]))
ðŸ”§ Clearing checkpoint from memory...
ðŸ”§ Loading state dict to model...
ðŸ”§ Moving model to cuda...
âœ… OPTIMIZED SAE loaded successfully!
   Layer: 4
   Expected reconstruction error: ~47%
   Feature clamping: VERIFIED WORKING
   Memory optimizations: ENABLED
âœ… SAE Layer 4 loaded
ðŸ”§ Loading SAE Layer 5...
ðŸ”§ Loading config: /data/.cache/huggingface/hub/models--fnlp--Llama3_1-8B-Base-LXR-8x/snapshots/8dbc1d85edfced43081c03c38b05514dbab1368b/Llama3_1-8B-Base-L5R-8x/hyperparams.json
âœ… Loading WORKING SAE for Layer 5
   Config: /data/.cache/huggingface/hub/models--fnlp--Llama3_1-8B-Base-LXR-8x/snapshots/8dbc1d85edfced43081c03c38b05514dbab1368b/Llama3_1-8B-Base-L5R-8x/hyperparams.json
   Dataset norm: 4.6875
   Norm factor: 13.653333
   Using ReLU instead of JumpReLU (threshold=0.0)
ðŸ”§ Loading checkpoint: /data/.cache/huggingface/hub/models--fnlp--Llama3_1-8B-Base-LXR-8x/snapshots/8dbc1d85edfced43081c03c38b05514dbab1368b/Llama3_1-8B-Base-L5R-8x/checkpoints/final.safetensors
   File size: 512.1 MB
ðŸ”§ Initializing SAE model...
ðŸ”§ Loading checkpoint weights (CPU)...
âœ… Loaded safetensors file
ðŸ”§ Processing checkpoint weights...
   Loading W_E (encoder.weight)... âœ… (torch.Size([4096, 32768]))
   Loading b_E (encoder.bias)... âœ… (torch.Size([32768]))
   Loading W_D (decoder.weight)... âœ… (torch.Size([32768, 4096]))
   Loading b_D (decoder.bias)... âœ… (torch.Size([4096]))
ðŸ”§ Clearing checkpoint from memory...
ðŸ”§ Loading state dict to model...
ðŸ”§ Moving model to cuda...
âœ… OPTIMIZED SAE loaded successfully!
   Layer: 5
   Expected reconstruction error: ~47%
   Feature clamping: VERIFIED WORKING
   Memory optimizations: ENABLED
âœ… SAE Layer 5 loaded
ðŸ”§ Loading SAE Layer 6...
ðŸ”§ Loading config: /data/.cache/huggingface/hub/models--fnlp--Llama3_1-8B-Base-LXR-8x/snapshots/8dbc1d85edfced43081c03c38b05514dbab1368b/Llama3_1-8B-Base-L6R-8x/hyperparams.json
âœ… Loading WORKING SAE for Layer 6
   Config: /data/.cache/huggingface/hub/models--fnlp--Llama3_1-8B-Base-LXR-8x/snapshots/8dbc1d85edfced43081c03c38b05514dbab1368b/Llama3_1-8B-Base-L6R-8x/hyperparams.json
   Dataset norm: 5.34375
   Norm factor: 11.976608
   Using ReLU instead of JumpReLU (threshold=0.0)
ðŸ”§ Loading checkpoint: /data/.cache/huggingface/hub/models--fnlp--Llama3_1-8B-Base-LXR-8x/snapshots/8dbc1d85edfced43081c03c38b05514dbab1368b/Llama3_1-8B-Base-L6R-8x/checkpoints/final.safetensors
   File size: 512.1 MB
ðŸ”§ Initializing SAE model...
ðŸ”§ Loading checkpoint weights (CPU)...
âœ… Loaded safetensors file
ðŸ”§ Processing checkpoint weights...
   Loading W_E (encoder.weight)... âœ… (torch.Size([4096, 32768]))
   Loading b_E (encoder.bias)... âœ… (torch.Size([32768]))
   Loading W_D (decoder.weight)... âœ… (torch.Size([32768, 4096]))
   Loading b_D (decoder.bias)... âœ… (torch.Size([4096]))
ðŸ”§ Clearing checkpoint from memory...
ðŸ”§ Loading state dict to model...
ðŸ”§ Moving model to cuda...
âœ… OPTIMIZED SAE loaded successfully!
   Layer: 6
   Expected reconstruction error: ~47%
   Feature clamping: VERIFIED WORKING
   Memory optimizations: ENABLED
âœ… SAE Layer 6 loaded
ðŸ”§ Loading SAE Layer 7...
ðŸ”§ Loading config: /data/.cache/huggingface/hub/models--fnlp--Llama3_1-8B-Base-LXR-8x/snapshots/8dbc1d85edfced43081c03c38b05514dbab1368b/Llama3_1-8B-Base-L7R-8x/hyperparams.json
âœ… Loading WORKING SAE for Layer 7
   Config: /data/.cache/huggingface/hub/models--fnlp--Llama3_1-8B-Base-LXR-8x/snapshots/8dbc1d85edfced43081c03c38b05514dbab1368b/Llama3_1-8B-Base-L7R-8x/hyperparams.json
   Dataset norm: 5.90625
   Norm factor: 10.835979
   Using ReLU instead of JumpReLU (threshold=0.0)
ðŸ”§ Loading checkpoint: /data/.cache/huggingface/hub/models--fnlp--Llama3_1-8B-Base-LXR-8x/snapshots/8dbc1d85edfced43081c03c38b05514dbab1368b/Llama3_1-8B-Base-L7R-8x/checkpoints/final.safetensors
   File size: 512.1 MB
ðŸ”§ Initializing SAE model...
ðŸ”§ Loading checkpoint weights (CPU)...
âœ… Loaded safetensors file
ðŸ”§ Processing checkpoint weights...
   Loading W_E (encoder.weight)... âœ… (torch.Size([4096, 32768]))
   Loading b_E (encoder.bias)... âœ… (torch.Size([32768]))
   Loading W_D (decoder.weight)... âœ… (torch.Size([32768, 4096]))
   Loading b_D (decoder.bias)... âœ… (torch.Size([4096]))
ðŸ”§ Clearing checkpoint from memory...
ðŸ”§ Loading state dict to model...
ðŸ”§ Moving model to cuda...
âœ… OPTIMIZED SAE loaded successfully!
   Layer: 7
   Expected reconstruction error: ~47%
   Feature clamping: VERIFIED WORKING
   Memory optimizations: ENABLED
âœ… SAE Layer 7 loaded
ðŸ”§ Loading SAE Layer 8...
ðŸ”§ Loading config: /data/.cache/huggingface/hub/models--fnlp--Llama3_1-8B-Base-LXR-8x/snapshots/8dbc1d85edfced43081c03c38b05514dbab1368b/Llama3_1-8B-Base-L8R-8x/hyperparams.json
âœ… Loading WORKING SAE for Layer 8
   Config: /data/.cache/huggingface/hub/models--fnlp--Llama3_1-8B-Base-LXR-8x/snapshots/8dbc1d85edfced43081c03c38b05514dbab1368b/Llama3_1-8B-Base-L8R-8x/hyperparams.json
   Dataset norm: 6.3125
   Norm factor: 10.138614
   Using ReLU instead of JumpReLU (threshold=0.0)
ðŸ”§ Loading checkpoint: /data/.cache/huggingface/hub/models--fnlp--Llama3_1-8B-Base-LXR-8x/snapshots/8dbc1d85edfced43081c03c38b05514dbab1368b/Llama3_1-8B-Base-L8R-8x/checkpoints/final.safetensors
   File size: 512.1 MB
ðŸ”§ Initializing SAE model...
ðŸ”§ Loading checkpoint weights (CPU)...
âœ… Loaded safetensors file
ðŸ”§ Processing checkpoint weights...
   Loading W_E (encoder.weight)... âœ… (torch.Size([4096, 32768]))
   Loading b_E (encoder.bias)... âœ… (torch.Size([32768]))
   Loading W_D (decoder.weight)... âœ… (torch.Size([32768, 4096]))
   Loading b_D (decoder.bias)... âœ… (torch.Size([4096]))
ðŸ”§ Clearing checkpoint from memory...
ðŸ”§ Loading state dict to model...
ðŸ”§ Moving model to cuda...
âœ… OPTIMIZED SAE loaded successfully!
   Layer: 8
   Expected reconstruction error: ~47%
   Feature clamping: VERIFIED WORKING
   Memory optimizations: ENABLED
âœ… SAE Layer 8 loaded
ðŸ”§ Loading SAE Layer 9...
ðŸ”§ Loading config: /data/.cache/huggingface/hub/models--fnlp--Llama3_1-8B-Base-LXR-8x/snapshots/8dbc1d85edfced43081c03c38b05514dbab1368b/Llama3_1-8B-Base-L9R-8x/hyperparams.json
âœ… Loading WORKING SAE for Layer 9
   Config: /data/.cache/huggingface/hub/models--fnlp--Llama3_1-8B-Base-LXR-8x/snapshots/8dbc1d85edfced43081c03c38b05514dbab1368b/Llama3_1-8B-Base-L9R-8x/hyperparams.json
   Dataset norm: 6.90625
   Norm factor: 9.266968
   Using ReLU instead of JumpReLU (threshold=0.0)
ðŸ”§ Loading checkpoint: /data/.cache/huggingface/hub/models--fnlp--Llama3_1-8B-Base-LXR-8x/snapshots/8dbc1d85edfced43081c03c38b05514dbab1368b/Llama3_1-8B-Base-L9R-8x/checkpoints/final.safetensors
   File size: 512.1 MB
ðŸ”§ Initializing SAE model...
ðŸ”§ Loading checkpoint weights (CPU)...
âœ… Loaded safetensors file
ðŸ”§ Processing checkpoint weights...
   Loading W_E (encoder.weight)... âœ… (torch.Size([4096, 32768]))
   Loading b_E (encoder.bias)... âœ… (torch.Size([32768]))
   Loading W_D (decoder.weight)... âœ… (torch.Size([32768, 4096]))
   Loading b_D (decoder.bias)... âœ… (torch.Size([4096]))
ðŸ”§ Clearing checkpoint from memory...
ðŸ”§ Loading state dict to model...
ðŸ”§ Moving model to cuda...
âœ… OPTIMIZED SAE loaded successfully!
   Layer: 9
   Expected reconstruction error: ~47%
   Feature clamping: VERIFIED WORKING
   Memory optimizations: ENABLED
âœ… SAE Layer 9 loaded
ðŸ”§ Loading SAE Layer 10...
ðŸ”§ Loading config: /data/.cache/huggingface/hub/models--fnlp--Llama3_1-8B-Base-LXR-8x/snapshots/8dbc1d85edfced43081c03c38b05514dbab1368b/Llama3_1-8B-Base-L10R-8x/hyperparams.json
âœ… Loading WORKING SAE for Layer 10
   Config: /data/.cache/huggingface/hub/models--fnlp--Llama3_1-8B-Base-LXR-8x/snapshots/8dbc1d85edfced43081c03c38b05514dbab1368b/Llama3_1-8B-Base-L10R-8x/hyperparams.json
   Dataset norm: 7.25
   Norm factor: 8.827586
   Using ReLU instead of JumpReLU (threshold=0.0)
ðŸ”§ Loading checkpoint: /data/.cache/huggingface/hub/models--fnlp--Llama3_1-8B-Base-LXR-8x/snapshots/8dbc1d85edfced43081c03c38b05514dbab1368b/Llama3_1-8B-Base-L10R-8x/checkpoints/final.safetensors
   File size: 512.1 MB
ðŸ”§ Initializing SAE model...
ðŸ”§ Loading checkpoint weights (CPU)...
âœ… Loaded safetensors file
ðŸ”§ Processing checkpoint weights...
   Loading W_E (encoder.weight)... âœ… (torch.Size([4096, 32768]))
   Loading b_E (encoder.bias)... âœ… (torch.Size([32768]))
   Loading W_D (decoder.weight)... âœ… (torch.Size([32768, 4096]))
   Loading b_D (decoder.bias)... âœ… (torch.Size([4096]))
ðŸ”§ Clearing checkpoint from memory...
ðŸ”§ Loading state dict to model...
ðŸ”§ Moving model to cuda...
âœ… OPTIMIZED SAE loaded successfully!
   Layer: 10
   Expected reconstruction error: ~47%
   Feature clamping: VERIFIED WORKING
   Memory optimizations: ENABLED
âœ… SAE Layer 10 loaded
ðŸ”§ Loading SAE Layer 11...
ðŸ”§ Loading config: /data/.cache/huggingface/hub/models--fnlp--Llama3_1-8B-Base-LXR-8x/snapshots/8dbc1d85edfced43081c03c38b05514dbab1368b/Llama3_1-8B-Base-L11R-8x/hyperparams.json
âœ… Loading WORKING SAE for Layer 11
   Config: /data/.cache/huggingface/hub/models--fnlp--Llama3_1-8B-Base-LXR-8x/snapshots/8dbc1d85edfced43081c03c38b05514dbab1368b/Llama3_1-8B-Base-L11R-8x/hyperparams.json
   Dataset norm: 7.5625
   Norm factor: 8.462810
   Using ReLU instead of JumpReLU (threshold=0.0)
ðŸ”§ Loading checkpoint: /data/.cache/huggingface/hub/models--fnlp--Llama3_1-8B-Base-LXR-8x/snapshots/8dbc1d85edfced43081c03c38b05514dbab1368b/Llama3_1-8B-Base-L11R-8x/checkpoints/final.safetensors
   File size: 512.1 MB
ðŸ”§ Initializing SAE model...
ðŸ”§ Loading checkpoint weights (CPU)...
âœ… Loaded safetensors file
ðŸ”§ Processing checkpoint weights...
   Loading W_E (encoder.weight)... âœ… (torch.Size([4096, 32768]))
   Loading b_E (encoder.bias)... âœ… (torch.Size([32768]))
   Loading W_D (decoder.weight)... âœ… (torch.Size([32768, 4096]))
   Loading b_D (decoder.bias)... âœ… (torch.Size([4096]))
ðŸ”§ Clearing checkpoint from memory...
ðŸ”§ Loading state dict to model...
ðŸ”§ Moving model to cuda...
âœ… OPTIMIZED SAE loaded successfully!
   Layer: 11
   Expected reconstruction error: ~47%
   Feature clamping: VERIFIED WORKING
   Memory optimizations: ENABLED
âœ… SAE Layer 11 loaded
ðŸ”§ Loading SAE Layer 12...
ðŸ”§ Loading config: /data/.cache/huggingface/hub/models--fnlp--Llama3_1-8B-Base-LXR-8x/snapshots/8dbc1d85edfced43081c03c38b05514dbab1368b/Llama3_1-8B-Base-L12R-8x/hyperparams.json
âœ… Loading WORKING SAE for Layer 12
   Config: /data/.cache/huggingface/hub/models--fnlp--Llama3_1-8B-Base-LXR-8x/snapshots/8dbc1d85edfced43081c03c38b05514dbab1368b/Llama3_1-8B-Base-L12R-8x/hyperparams.json
   Dataset norm: 8.0625
   Norm factor: 7.937984
   Using ReLU instead of JumpReLU (threshold=0.0)
ðŸ”§ Loading checkpoint: /data/.cache/huggingface/hub/models--fnlp--Llama3_1-8B-Base-LXR-8x/snapshots/8dbc1d85edfced43081c03c38b05514dbab1368b/Llama3_1-8B-Base-L12R-8x/checkpoints/final.safetensors
   File size: 512.1 MB
ðŸ”§ Initializing SAE model...
ðŸ”§ Loading checkpoint weights (CPU)...
âœ… Loaded safetensors file
ðŸ”§ Processing checkpoint weights...
   Loading W_E (encoder.weight)... âœ… (torch.Size([4096, 32768]))
   Loading b_E (encoder.bias)... âœ… (torch.Size([32768]))
   Loading W_D (decoder.weight)... âœ… (torch.Size([32768, 4096]))
   Loading b_D (decoder.bias)... âœ… (torch.Size([4096]))
ðŸ”§ Clearing checkpoint from memory...
ðŸ”§ Loading state dict to model...
ðŸ”§ Moving model to cuda...
âœ… OPTIMIZED SAE loaded successfully!
   Layer: 12
   Expected reconstruction error: ~47%
   Feature clamping: VERIFIED WORKING
   Memory optimizations: ENABLED
âœ… SAE Layer 12 loaded
ðŸ”§ Loading SAE Layer 13...
ðŸ”§ Loading config: /data/.cache/huggingface/hub/models--fnlp--Llama3_1-8B-Base-LXR-8x/snapshots/8dbc1d85edfced43081c03c38b05514dbab1368b/Llama3_1-8B-Base-L13R-8x/hyperparams.json
âœ… Loading WORKING SAE for Layer 13
   Config: /data/.cache/huggingface/hub/models--fnlp--Llama3_1-8B-Base-LXR-8x/snapshots/8dbc1d85edfced43081c03c38b05514dbab1368b/Llama3_1-8B-Base-L13R-8x/hyperparams.json
   Dataset norm: 8.8125
   Norm factor: 7.262411
   Using ReLU instead of JumpReLU (threshold=0.0)
ðŸ”§ Loading checkpoint: /data/.cache/huggingface/hub/models--fnlp--Llama3_1-8B-Base-LXR-8x/snapshots/8dbc1d85edfced43081c03c38b05514dbab1368b/Llama3_1-8B-Base-L13R-8x/checkpoints/final.safetensors
   File size: 512.1 MB
ðŸ”§ Initializing SAE model...
ðŸ”§ Loading checkpoint weights (CPU)...
âœ… Loaded safetensors file
ðŸ”§ Processing checkpoint weights...
   Loading W_E (encoder.weight)... âœ… (torch.Size([4096, 32768]))
   Loading b_E (encoder.bias)... âœ… (torch.Size([32768]))
   Loading W_D (decoder.weight)... âœ… (torch.Size([32768, 4096]))
   Loading b_D (decoder.bias)... âœ… (torch.Size([4096]))
ðŸ”§ Clearing checkpoint from memory...
ðŸ”§ Loading state dict to model...
ðŸ”§ Moving model to cuda...
âœ… OPTIMIZED SAE loaded successfully!
   Layer: 13
   Expected reconstruction error: ~47%
   Feature clamping: VERIFIED WORKING
   Memory optimizations: ENABLED
âœ… SAE Layer 13 loaded
ðŸ”§ Loading SAE Layer 14...
ðŸ”§ Loading config: /data/.cache/huggingface/hub/models--fnlp--Llama3_1-8B-Base-LXR-8x/snapshots/8dbc1d85edfced43081c03c38b05514dbab1368b/Llama3_1-8B-Base-L14R-8x/hyperparams.json
âœ… Loading WORKING SAE for Layer 14
   Config: /data/.cache/huggingface/hub/models--fnlp--Llama3_1-8B-Base-LXR-8x/snapshots/8dbc1d85edfced43081c03c38b05514dbab1368b/Llama3_1-8B-Base-L14R-8x/hyperparams.json
   Dataset norm: 9.5625
   Norm factor: 6.692810
   Using ReLU instead of JumpReLU (threshold=0.0)
ðŸ”§ Loading checkpoint: /data/.cache/huggingface/hub/models--fnlp--Llama3_1-8B-Base-LXR-8x/snapshots/8dbc1d85edfced43081c03c38b05514dbab1368b/Llama3_1-8B-Base-L14R-8x/checkpoints/final.safetensors
   File size: 512.1 MB
ðŸ”§ Initializing SAE model...
ðŸ”§ Loading checkpoint weights (CPU)...
âœ… Loaded safetensors file
ðŸ”§ Processing checkpoint weights...
   Loading W_E (encoder.weight)... âœ… (torch.Size([4096, 32768]))
   Loading b_E (encoder.bias)... âœ… (torch.Size([32768]))
   Loading W_D (decoder.weight)... âœ… (torch.Size([32768, 4096]))
   Loading b_D (decoder.bias)... âœ… (torch.Size([4096]))
ðŸ”§ Clearing checkpoint from memory...
ðŸ”§ Loading state dict to model...
ðŸ”§ Moving model to cuda...
âœ… OPTIMIZED SAE loaded successfully!
   Layer: 14
   Expected reconstruction error: ~47%
   Feature clamping: VERIFIED WORKING
   Memory optimizations: ENABLED
âœ… SAE Layer 14 loaded
ðŸ”§ Loading SAE Layer 15...
ðŸ”§ Loading config: /data/.cache/huggingface/hub/models--fnlp--Llama3_1-8B-Base-LXR-8x/snapshots/8dbc1d85edfced43081c03c38b05514dbab1368b/Llama3_1-8B-Base-L15R-8x/hyperparams.json
âœ… Loading WORKING SAE for Layer 15
   Config: /data/.cache/huggingface/hub/models--fnlp--Llama3_1-8B-Base-LXR-8x/snapshots/8dbc1d85edfced43081c03c38b05514dbab1368b/Llama3_1-8B-Base-L15R-8x/hyperparams.json
   Dataset norm: 10.8125
   Norm factor: 5.919075
   Using ReLU instead of JumpReLU (threshold=0.0)
ðŸ”§ Loading checkpoint: /data/.cache/huggingface/hub/models--fnlp--Llama3_1-8B-Base-LXR-8x/snapshots/8dbc1d85edfced43081c03c38b05514dbab1368b/Llama3_1-8B-Base-L15R-8x/checkpoints/final_fixed.pth
   File size: 512.1 MB
ðŸ”§ Initializing SAE model...
ðŸ”§ Loading checkpoint weights (CPU)...
âœ… Loaded PyTorch file
ðŸ”§ Processing checkpoint weights...
   Loading W_E (W_E)... âœ… (torch.Size([4096, 32768]))
   Loading b_E (b_E)... âœ… (torch.Size([32768]))
   Loading W_D (W_D)... âœ… (torch.Size([32768, 4096]))
   Loading b_D (b_D)... âœ… (torch.Size([4096]))
ðŸ”§ Clearing checkpoint from memory...
ðŸ”§ Loading state dict to model...
ðŸ”§ Moving model to cuda...
âœ… OPTIMIZED SAE loaded successfully!
   Layer: 15
   Expected reconstruction error: ~47%
   Feature clamping: VERIFIED WORKING
   Memory optimizations: ENABLED
âœ… SAE Layer 15 loaded
ðŸ”§ Loading SAE Layer 16...
ðŸ”§ Loading config: /data/.cache/huggingface/hub/models--fnlp--Llama3_1-8B-Base-LXR-8x/snapshots/8dbc1d85edfced43081c03c38b05514dbab1368b/Llama3_1-8B-Base-L16R-8x/hyperparams.json
âœ… Loading WORKING SAE for Layer 16
   Config: /data/.cache/huggingface/hub/models--fnlp--Llama3_1-8B-Base-LXR-8x/snapshots/8dbc1d85edfced43081c03c38b05514dbab1368b/Llama3_1-8B-Base-L16R-8x/hyperparams.json
   Dataset norm: 12.125
   Norm factor: 5.278351
   Using ReLU instead of JumpReLU (threshold=0.0)
ðŸ”§ Loading checkpoint: /data/.cache/huggingface/hub/models--fnlp--Llama3_1-8B-Base-LXR-8x/snapshots/8dbc1d85edfced43081c03c38b05514dbab1368b/Llama3_1-8B-Base-L16R-8x/checkpoints/final.safetensors
   File size: 512.1 MB
ðŸ”§ Initializing SAE model...
ðŸ”§ Loading checkpoint weights (CPU)...
âœ… Loaded safetensors file
ðŸ”§ Processing checkpoint weights...
   Loading W_E (encoder.weight)... âœ… (torch.Size([4096, 32768]))
   Loading b_E (encoder.bias)... âœ… (torch.Size([32768]))
   Loading W_D (decoder.weight)... âœ… (torch.Size([32768, 4096]))
   Loading b_D (decoder.bias)... âœ… (torch.Size([4096]))
ðŸ”§ Clearing checkpoint from memory...
ðŸ”§ Loading state dict to model...
ðŸ”§ Moving model to cuda...
âœ… OPTIMIZED SAE loaded successfully!
   Layer: 16
   Expected reconstruction error: ~47%
   Feature clamping: VERIFIED WORKING
   Memory optimizations: ENABLED
âœ… SAE Layer 16 loaded
ðŸ”§ Loading SAE Layer 17...
ðŸ”§ Loading config: /data/.cache/huggingface/hub/models--fnlp--Llama3_1-8B-Base-LXR-8x/snapshots/8dbc1d85edfced43081c03c38b05514dbab1368b/Llama3_1-8B-Base-L17R-8x/hyperparams.json
âœ… Loading WORKING SAE for Layer 17
   Config: /data/.cache/huggingface/hub/models--fnlp--Llama3_1-8B-Base-LXR-8x/snapshots/8dbc1d85edfced43081c03c38b05514dbab1368b/Llama3_1-8B-Base-L17R-8x/hyperparams.json
   Dataset norm: 13.8125
   Norm factor: 4.633484
   Using ReLU instead of JumpReLU (threshold=0.0)
ðŸ”§ Loading checkpoint: /data/.cache/huggingface/hub/models--fnlp--Llama3_1-8B-Base-LXR-8x/snapshots/8dbc1d85edfced43081c03c38b05514dbab1368b/Llama3_1-8B-Base-L17R-8x/checkpoints/final.safetensors
   File size: 512.1 MB
ðŸ”§ Initializing SAE model...
ðŸ”§ Loading checkpoint weights (CPU)...
âœ… Loaded safetensors file
ðŸ”§ Processing checkpoint weights...
   Loading W_E (encoder.weight)... âœ… (torch.Size([4096, 32768]))
   Loading b_E (encoder.bias)... âœ… (torch.Size([32768]))
   Loading W_D (decoder.weight)... âœ… (torch.Size([32768, 4096]))
   Loading b_D (decoder.bias)... âœ… (torch.Size([4096]))
ðŸ”§ Clearing checkpoint from memory...
ðŸ”§ Loading state dict to model...
ðŸ”§ Moving model to cuda...
âœ… OPTIMIZED SAE loaded successfully!
   Layer: 17
   Expected reconstruction error: ~47%
   Feature clamping: VERIFIED WORKING
   Memory optimizations: ENABLED
âœ… SAE Layer 17 loaded
ðŸ”§ Loading SAE Layer 18...
ðŸ”§ Loading config: /data/.cache/huggingface/hub/models--fnlp--Llama3_1-8B-Base-LXR-8x/snapshots/8dbc1d85edfced43081c03c38b05514dbab1368b/Llama3_1-8B-Base-L18R-8x/hyperparams.json
âœ… Loading WORKING SAE for Layer 18
   Config: /data/.cache/huggingface/hub/models--fnlp--Llama3_1-8B-Base-LXR-8x/snapshots/8dbc1d85edfced43081c03c38b05514dbab1368b/Llama3_1-8B-Base-L18R-8x/hyperparams.json
   Dataset norm: 15.5
   Norm factor: 4.129032
   Using ReLU instead of JumpReLU (threshold=0.0)
ðŸ”§ Loading checkpoint: /data/.cache/huggingface/hub/models--fnlp--Llama3_1-8B-Base-LXR-8x/snapshots/8dbc1d85edfced43081c03c38b05514dbab1368b/Llama3_1-8B-Base-L18R-8x/checkpoints/final.safetensors
   File size: 512.1 MB
ðŸ”§ Initializing SAE model...
ðŸ”§ Loading checkpoint weights (CPU)...
âœ… Loaded safetensors file
ðŸ”§ Processing checkpoint weights...
   Loading W_E (encoder.weight)... âœ… (torch.Size([4096, 32768]))
   Loading b_E (encoder.bias)... âœ… (torch.Size([32768]))
   Loading W_D (decoder.weight)... âœ… (torch.Size([32768, 4096]))
   Loading b_D (decoder.bias)... âœ… (torch.Size([4096]))
ðŸ”§ Clearing checkpoint from memory...
ðŸ”§ Loading state dict to model...
ðŸ”§ Moving model to cuda...
âœ… OPTIMIZED SAE loaded successfully!
   Layer: 18
   Expected reconstruction error: ~47%
   Feature clamping: VERIFIED WORKING
   Memory optimizations: ENABLED
âœ… SAE Layer 18 loaded
ðŸ”§ Loading SAE Layer 19...
ðŸ”§ Loading config: /data/.cache/huggingface/hub/models--fnlp--Llama3_1-8B-Base-LXR-8x/snapshots/8dbc1d85edfced43081c03c38b05514dbab1368b/Llama3_1-8B-Base-L19R-8x/hyperparams.json
âœ… Loading WORKING SAE for Layer 19
   Config: /data/.cache/huggingface/hub/models--fnlp--Llama3_1-8B-Base-LXR-8x/snapshots/8dbc1d85edfced43081c03c38b05514dbab1368b/Llama3_1-8B-Base-L19R-8x/hyperparams.json
   Dataset norm: 17.125
   Norm factor: 3.737226
   Using ReLU instead of JumpReLU (threshold=0.0)
ðŸ”§ Loading checkpoint: /data/.cache/huggingface/hub/models--fnlp--Llama3_1-8B-Base-LXR-8x/snapshots/8dbc1d85edfced43081c03c38b05514dbab1368b/Llama3_1-8B-Base-L19R-8x/checkpoints/final.safetensors
   File size: 512.1 MB
ðŸ”§ Initializing SAE model...
ðŸ”§ Loading checkpoint weights (CPU)...
âœ… Loaded safetensors file
ðŸ”§ Processing checkpoint weights...
   Loading W_E (encoder.weight)... âœ… (torch.Size([4096, 32768]))
   Loading b_E (encoder.bias)... âœ… (torch.Size([32768]))
   Loading W_D (decoder.weight)... âœ… (torch.Size([32768, 4096]))
   Loading b_D (decoder.bias)... âœ… (torch.Size([4096]))
ðŸ”§ Clearing checkpoint from memory...
ðŸ”§ Loading state dict to model...
ðŸ”§ Moving model to cuda...
âœ… OPTIMIZED SAE loaded successfully!
   Layer: 19
   Expected reconstruction error: ~47%
   Feature clamping: VERIFIED WORKING
   Memory optimizations: ENABLED
âœ… SAE Layer 19 loaded
ðŸ”§ Loading SAE Layer 20...
ðŸ”§ Loading config: /data/.cache/huggingface/hub/models--fnlp--Llama3_1-8B-Base-LXR-8x/snapshots/8dbc1d85edfced43081c03c38b05514dbab1368b/Llama3_1-8B-Base-L20R-8x/hyperparams.json
âœ… Loading WORKING SAE for Layer 20
   Config: /data/.cache/huggingface/hub/models--fnlp--Llama3_1-8B-Base-LXR-8x/snapshots/8dbc1d85edfced43081c03c38b05514dbab1368b/Llama3_1-8B-Base-L20R-8x/hyperparams.json
   Dataset norm: 18.75
   Norm factor: 3.413333
   Using ReLU instead of JumpReLU (threshold=0.0)
ðŸ”§ Loading checkpoint: /data/.cache/huggingface/hub/models--fnlp--Llama3_1-8B-Base-LXR-8x/snapshots/8dbc1d85edfced43081c03c38b05514dbab1368b/Llama3_1-8B-Base-L20R-8x/checkpoints/final_fixed.pth
   File size: 512.1 MB
ðŸ”§ Initializing SAE model...
ðŸ”§ Loading checkpoint weights (CPU)...
âœ… Loaded PyTorch file
ðŸ”§ Processing checkpoint weights...
   Loading W_E (W_E)... âœ… (torch.Size([4096, 32768]))
   Loading b_E (b_E)... âœ… (torch.Size([32768]))
   Loading W_D (W_D)... âœ… (torch.Size([32768, 4096]))
   Loading b_D (b_D)... âœ… (torch.Size([4096]))
ðŸ”§ Clearing checkpoint from memory...
ðŸ”§ Loading state dict to model...
ðŸ”§ Moving model to cuda...
âœ… OPTIMIZED SAE loaded successfully!
   Layer: 20
   Expected reconstruction error: ~47%
   Feature clamping: VERIFIED WORKING
   Memory optimizations: ENABLED
âœ… SAE Layer 20 loaded
ðŸ”§ Loading SAE Layer 21...
ðŸ”§ Loading config: /data/.cache/huggingface/hub/models--fnlp--Llama3_1-8B-Base-LXR-8x/snapshots/8dbc1d85edfced43081c03c38b05514dbab1368b/Llama3_1-8B-Base-L21R-8x/hyperparams.json
âœ… Loading WORKING SAE for Layer 21
   Config: /data/.cache/huggingface/hub/models--fnlp--Llama3_1-8B-Base-LXR-8x/snapshots/8dbc1d85edfced43081c03c38b05514dbab1368b/Llama3_1-8B-Base-L21R-8x/hyperparams.json
   Dataset norm: 21.5
   Norm factor: 2.976744
   Using ReLU instead of JumpReLU (threshold=0.0)
ðŸ”§ Loading checkpoint: /data/.cache/huggingface/hub/models--fnlp--Llama3_1-8B-Base-LXR-8x/snapshots/8dbc1d85edfced43081c03c38b05514dbab1368b/Llama3_1-8B-Base-L21R-8x/checkpoints/final.safetensors
   File size: 512.1 MB
ðŸ”§ Initializing SAE model...
ðŸ”§ Loading checkpoint weights (CPU)...
âœ… Loaded safetensors file
ðŸ”§ Processing checkpoint weights...
   Loading W_E (encoder.weight)... âœ… (torch.Size([4096, 32768]))
   Loading b_E (encoder.bias)... âœ… (torch.Size([32768]))
   Loading W_D (decoder.weight)... âœ… (torch.Size([32768, 4096]))
   Loading b_D (decoder.bias)... âœ… (torch.Size([4096]))
ðŸ”§ Clearing checkpoint from memory...
ðŸ”§ Loading state dict to model...
ðŸ”§ Moving model to cuda...
âœ… OPTIMIZED SAE loaded successfully!
   Layer: 21
   Expected reconstruction error: ~47%
   Feature clamping: VERIFIED WORKING
   Memory optimizations: ENABLED
âœ… SAE Layer 21 loaded
ðŸ”§ Loading SAE Layer 22...
ðŸ”§ Loading config: /data/.cache/huggingface/hub/models--fnlp--Llama3_1-8B-Base-LXR-8x/snapshots/8dbc1d85edfced43081c03c38b05514dbab1368b/Llama3_1-8B-Base-L22R-8x/hyperparams.json
âœ… Loading WORKING SAE for Layer 22
   Config: /data/.cache/huggingface/hub/models--fnlp--Llama3_1-8B-Base-LXR-8x/snapshots/8dbc1d85edfced43081c03c38b05514dbab1368b/Llama3_1-8B-Base-L22R-8x/hyperparams.json
   Dataset norm: 23.875
   Norm factor: 2.680628
   Using ReLU instead of JumpReLU (threshold=0.0)
ðŸ”§ Loading checkpoint: /data/.cache/huggingface/hub/models--fnlp--Llama3_1-8B-Base-LXR-8x/snapshots/8dbc1d85edfced43081c03c38b05514dbab1368b/Llama3_1-8B-Base-L22R-8x/checkpoints/final.safetensors
   File size: 512.1 MB
ðŸ”§ Initializing SAE model...
ðŸ”§ Loading checkpoint weights (CPU)...
âœ… Loaded safetensors file
ðŸ”§ Processing checkpoint weights...
   Loading W_E (encoder.weight)... âœ… (torch.Size([4096, 32768]))
   Loading b_E (encoder.bias)... âœ… (torch.Size([32768]))
   Loading W_D (decoder.weight)... âœ… (torch.Size([32768, 4096]))
   Loading b_D (decoder.bias)... âœ… (torch.Size([4096]))
ðŸ”§ Clearing checkpoint from memory...
ðŸ”§ Loading state dict to model...
ðŸ”§ Moving model to cuda...
âœ… OPTIMIZED SAE loaded successfully!
   Layer: 22
   Expected reconstruction error: ~47%
   Feature clamping: VERIFIED WORKING
   Memory optimizations: ENABLED
âœ… SAE Layer 22 loaded
ðŸ”§ Loading SAE Layer 23...
ðŸ”§ Loading config: /data/.cache/huggingface/hub/models--fnlp--Llama3_1-8B-Base-LXR-8x/snapshots/8dbc1d85edfced43081c03c38b05514dbab1368b/Llama3_1-8B-Base-L23R-8x/hyperparams.json
âœ… Loading WORKING SAE for Layer 23
   Config: /data/.cache/huggingface/hub/models--fnlp--Llama3_1-8B-Base-LXR-8x/snapshots/8dbc1d85edfced43081c03c38b05514dbab1368b/Llama3_1-8B-Base-L23R-8x/hyperparams.json
   Dataset norm: 26.5
   Norm factor: 2.415094
   Using ReLU instead of JumpReLU (threshold=0.0)
ðŸ”§ Loading checkpoint: /data/.cache/huggingface/hub/models--fnlp--Llama3_1-8B-Base-LXR-8x/snapshots/8dbc1d85edfced43081c03c38b05514dbab1368b/Llama3_1-8B-Base-L23R-8x/checkpoints/final.safetensors
   File size: 512.1 MB
ðŸ”§ Initializing SAE model...
ðŸ”§ Loading checkpoint weights (CPU)...
âœ… Loaded safetensors file
ðŸ”§ Processing checkpoint weights...
   Loading W_E (encoder.weight)... âœ… (torch.Size([4096, 32768]))
   Loading b_E (encoder.bias)... âœ… (torch.Size([32768]))
   Loading W_D (decoder.weight)... âœ… (torch.Size([32768, 4096]))
   Loading b_D (decoder.bias)... âœ… (torch.Size([4096]))
ðŸ”§ Clearing checkpoint from memory...
ðŸ”§ Loading state dict to model...
ðŸ”§ Moving model to cuda...
âœ… OPTIMIZED SAE loaded successfully!
   Layer: 23
   Expected reconstruction error: ~47%
   Feature clamping: VERIFIED WORKING
   Memory optimizations: ENABLED
âœ… SAE Layer 23 loaded
ðŸ”§ Loading SAE Layer 24...
ðŸ”§ Loading config: /data/.cache/huggingface/hub/models--fnlp--Llama3_1-8B-Base-LXR-8x/snapshots/8dbc1d85edfced43081c03c38b05514dbab1368b/Llama3_1-8B-Base-L24R-8x/hyperparams.json
âœ… Loading WORKING SAE for Layer 24
   Config: /data/.cache/huggingface/hub/models--fnlp--Llama3_1-8B-Base-LXR-8x/snapshots/8dbc1d85edfced43081c03c38b05514dbab1368b/Llama3_1-8B-Base-L24R-8x/hyperparams.json
   Dataset norm: 29.25
   Norm factor: 2.188034
   Using ReLU instead of JumpReLU (threshold=0.0)
ðŸ”§ Loading checkpoint: /data/.cache/huggingface/hub/models--fnlp--Llama3_1-8B-Base-LXR-8x/snapshots/8dbc1d85edfced43081c03c38b05514dbab1368b/Llama3_1-8B-Base-L24R-8x/checkpoints/final.safetensors
   File size: 512.1 MB
ðŸ”§ Initializing SAE model...
ðŸ”§ Loading checkpoint weights (CPU)...
âœ… Loaded safetensors file
ðŸ”§ Processing checkpoint weights...
   Loading W_E (encoder.weight)... âœ… (torch.Size([4096, 32768]))
   Loading b_E (encoder.bias)... âœ… (torch.Size([32768]))
   Loading W_D (decoder.weight)... âœ… (torch.Size([32768, 4096]))
   Loading b_D (decoder.bias)... âœ… (torch.Size([4096]))
ðŸ”§ Clearing checkpoint from memory...
ðŸ”§ Loading state dict to model...
ðŸ”§ Moving model to cuda...
âœ… OPTIMIZED SAE loaded successfully!
   Layer: 24
   Expected reconstruction error: ~47%
   Feature clamping: VERIFIED WORKING
   Memory optimizations: ENABLED
âœ… SAE Layer 24 loaded
ðŸ”§ Loading SAE Layer 25...
ðŸ”§ Loading config: /data/.cache/huggingface/hub/models--fnlp--Llama3_1-8B-Base-LXR-8x/snapshots/8dbc1d85edfced43081c03c38b05514dbab1368b/Llama3_1-8B-Base-L25R-8x/hyperparams.json
âœ… Loading WORKING SAE for Layer 25
   Config: /data/.cache/huggingface/hub/models--fnlp--Llama3_1-8B-Base-LXR-8x/snapshots/8dbc1d85edfced43081c03c38b05514dbab1368b/Llama3_1-8B-Base-L25R-8x/hyperparams.json
   Dataset norm: 31.625
   Norm factor: 2.023715
   Using ReLU instead of JumpReLU (threshold=0.0)
ðŸ”§ Loading checkpoint: /data/.cache/huggingface/hub/models--fnlp--Llama3_1-8B-Base-LXR-8x/snapshots/8dbc1d85edfced43081c03c38b05514dbab1368b/Llama3_1-8B-Base-L25R-8x/checkpoints/final.safetensors
   File size: 512.1 MB
ðŸ”§ Initializing SAE model...
ðŸ”§ Loading checkpoint weights (CPU)...
âœ… Loaded safetensors file
ðŸ”§ Processing checkpoint weights...
   Loading W_E (encoder.weight)... âœ… (torch.Size([4096, 32768]))
   Loading b_E (encoder.bias)... âœ… (torch.Size([32768]))
   Loading W_D (decoder.weight)... âœ… (torch.Size([32768, 4096]))
   Loading b_D (decoder.bias)... âœ… (torch.Size([4096]))
ðŸ”§ Clearing checkpoint from memory...
ðŸ”§ Loading state dict to model...
ðŸ”§ Moving model to cuda...
âœ… OPTIMIZED SAE loaded successfully!
   Layer: 25
   Expected reconstruction error: ~47%
   Feature clamping: VERIFIED WORKING
   Memory optimizations: ENABLED
âœ… SAE Layer 25 loaded
ðŸ”§ Loading SAE Layer 26...
ðŸ”§ Loading config: /data/.cache/huggingface/hub/models--fnlp--Llama3_1-8B-Base-LXR-8x/snapshots/8dbc1d85edfced43081c03c38b05514dbab1368b/Llama3_1-8B-Base-L26R-8x/hyperparams.json
âœ… Loading WORKING SAE for Layer 26
   Config: /data/.cache/huggingface/hub/models--fnlp--Llama3_1-8B-Base-LXR-8x/snapshots/8dbc1d85edfced43081c03c38b05514dbab1368b/Llama3_1-8B-Base-L26R-8x/hyperparams.json
   Dataset norm: 35.25
   Norm factor: 1.815603
   Using ReLU instead of JumpReLU (threshold=0.0)
ðŸ”§ Loading checkpoint: /data/.cache/huggingface/hub/models--fnlp--Llama3_1-8B-Base-LXR-8x/snapshots/8dbc1d85edfced43081c03c38b05514dbab1368b/Llama3_1-8B-Base-L26R-8x/checkpoints/final.safetensors
   File size: 512.1 MB
ðŸ”§ Initializing SAE model...
ðŸ”§ Loading checkpoint weights (CPU)...
âœ… Loaded safetensors file
ðŸ”§ Processing checkpoint weights...
   Loading W_E (encoder.weight)... âœ… (torch.Size([4096, 32768]))
   Loading b_E (encoder.bias)... âœ… (torch.Size([32768]))
   Loading W_D (decoder.weight)... âœ… (torch.Size([32768, 4096]))
   Loading b_D (decoder.bias)... âœ… (torch.Size([4096]))
ðŸ”§ Clearing checkpoint from memory...
ðŸ”§ Loading state dict to model...
ðŸ”§ Moving model to cuda...
âœ… OPTIMIZED SAE loaded successfully!
   Layer: 26
   Expected reconstruction error: ~47%
   Feature clamping: VERIFIED WORKING
   Memory optimizations: ENABLED
âœ… SAE Layer 26 loaded
ðŸ”§ Loading SAE Layer 27...
ðŸ”§ Loading config: /data/.cache/huggingface/hub/models--fnlp--Llama3_1-8B-Base-LXR-8x/snapshots/8dbc1d85edfced43081c03c38b05514dbab1368b/Llama3_1-8B-Base-L27R-8x/hyperparams.json
âœ… Loading WORKING SAE for Layer 27
   Config: /data/.cache/huggingface/hub/models--fnlp--Llama3_1-8B-Base-LXR-8x/snapshots/8dbc1d85edfced43081c03c38b05514dbab1368b/Llama3_1-8B-Base-L27R-8x/hyperparams.json
   Dataset norm: 38.25
   Norm factor: 1.673203
   Using ReLU instead of JumpReLU (threshold=0.0)
ðŸ”§ Loading checkpoint: /data/.cache/huggingface/hub/models--fnlp--Llama3_1-8B-Base-LXR-8x/snapshots/8dbc1d85edfced43081c03c38b05514dbab1368b/Llama3_1-8B-Base-L27R-8x/checkpoints/final.safetensors
   File size: 512.1 MB
ðŸ”§ Initializing SAE model...
ðŸ”§ Loading checkpoint weights (CPU)...
âœ… Loaded safetensors file
ðŸ”§ Processing checkpoint weights...
   Loading W_E (encoder.weight)... âœ… (torch.Size([4096, 32768]))
   Loading b_E (encoder.bias)... âœ… (torch.Size([32768]))
   Loading W_D (decoder.weight)... âœ… (torch.Size([32768, 4096]))
   Loading b_D (decoder.bias)... âœ… (torch.Size([4096]))
ðŸ”§ Clearing checkpoint from memory...
ðŸ”§ Loading state dict to model...
ðŸ”§ Moving model to cuda...
âœ… OPTIMIZED SAE loaded successfully!
   Layer: 27
   Expected reconstruction error: ~47%
   Feature clamping: VERIFIED WORKING
   Memory optimizations: ENABLED
âœ… SAE Layer 27 loaded
ðŸ”§ Loading SAE Layer 28...
ðŸ”§ Loading config: /data/.cache/huggingface/hub/models--fnlp--Llama3_1-8B-Base-LXR-8x/snapshots/8dbc1d85edfced43081c03c38b05514dbab1368b/Llama3_1-8B-Base-L28R-8x/hyperparams.json
âœ… Loading WORKING SAE for Layer 28
   Config: /data/.cache/huggingface/hub/models--fnlp--Llama3_1-8B-Base-LXR-8x/snapshots/8dbc1d85edfced43081c03c38b05514dbab1368b/Llama3_1-8B-Base-L28R-8x/hyperparams.json
   Dataset norm: 42.5
   Norm factor: 1.505882
   Using ReLU instead of JumpReLU (threshold=0.0)
ðŸ”§ Loading checkpoint: /data/.cache/huggingface/hub/models--fnlp--Llama3_1-8B-Base-LXR-8x/snapshots/8dbc1d85edfced43081c03c38b05514dbab1368b/Llama3_1-8B-Base-L28R-8x/checkpoints/final.safetensors
   File size: 512.1 MB
ðŸ”§ Initializing SAE model...
ðŸ”§ Loading checkpoint weights (CPU)...
âœ… Loaded safetensors file
ðŸ”§ Processing checkpoint weights...
   Loading W_E (encoder.weight)... âœ… (torch.Size([4096, 32768]))
   Loading b_E (encoder.bias)... âœ… (torch.Size([32768]))
   Loading W_D (decoder.weight)... âœ… (torch.Size([32768, 4096]))
   Loading b_D (decoder.bias)... âœ… (torch.Size([4096]))
ðŸ”§ Clearing checkpoint from memory...
ðŸ”§ Loading state dict to model...
ðŸ”§ Moving model to cuda...
âœ… OPTIMIZED SAE loaded successfully!
   Layer: 28
   Expected reconstruction error: ~47%
   Feature clamping: VERIFIED WORKING
   Memory optimizations: ENABLED
âœ… SAE Layer 28 loaded
ðŸ”§ Loading SAE Layer 29...
ðŸ”§ Loading config: /data/.cache/huggingface/hub/models--fnlp--Llama3_1-8B-Base-LXR-8x/snapshots/8dbc1d85edfced43081c03c38b05514dbab1368b/Llama3_1-8B-Base-L29R-8x/hyperparams.json
âœ… Loading WORKING SAE for Layer 29
   Config: /data/.cache/huggingface/hub/models--fnlp--Llama3_1-8B-Base-LXR-8x/snapshots/8dbc1d85edfced43081c03c38b05514dbab1368b/Llama3_1-8B-Base-L29R-8x/hyperparams.json
   Dataset norm: 46.5
   Norm factor: 1.376344
   Using ReLU instead of JumpReLU (threshold=0.0)
ðŸ”§ Loading checkpoint: /data/.cache/huggingface/hub/models--fnlp--Llama3_1-8B-Base-LXR-8x/snapshots/8dbc1d85edfced43081c03c38b05514dbab1368b/Llama3_1-8B-Base-L29R-8x/checkpoints/final.safetensors
   File size: 512.1 MB
ðŸ”§ Initializing SAE model...
ðŸ”§ Loading checkpoint weights (CPU)...
âœ… Loaded safetensors file
ðŸ”§ Processing checkpoint weights...
   Loading W_E (encoder.weight)... âœ… (torch.Size([4096, 32768]))
   Loading b_E (encoder.bias)... âœ… (torch.Size([32768]))
   Loading W_D (decoder.weight)... âœ… (torch.Size([32768, 4096]))
   Loading b_D (decoder.bias)... âœ… (torch.Size([4096]))
ðŸ”§ Clearing checkpoint from memory...
ðŸ”§ Loading state dict to model...
ðŸ”§ Moving model to cuda...
âœ… OPTIMIZED SAE loaded successfully!
   Layer: 29
   Expected reconstruction error: ~47%
   Feature clamping: VERIFIED WORKING
   Memory optimizations: ENABLED
âœ… SAE Layer 29 loaded
ðŸ”§ Loading SAE Layer 30...
ðŸ”§ Loading config: /data/.cache/huggingface/hub/models--fnlp--Llama3_1-8B-Base-LXR-8x/snapshots/8dbc1d85edfced43081c03c38b05514dbab1368b/Llama3_1-8B-Base-L30R-8x/hyperparams.json
âœ… Loading WORKING SAE for Layer 30
   Config: /data/.cache/huggingface/hub/models--fnlp--Llama3_1-8B-Base-LXR-8x/snapshots/8dbc1d85edfced43081c03c38b05514dbab1368b/Llama3_1-8B-Base-L30R-8x/hyperparams.json
   Dataset norm: 53.25
   Norm factor: 1.201878
   Using ReLU instead of JumpReLU (threshold=0.0)
ðŸ”§ Loading checkpoint: /data/.cache/huggingface/hub/models--fnlp--Llama3_1-8B-Base-LXR-8x/snapshots/8dbc1d85edfced43081c03c38b05514dbab1368b/Llama3_1-8B-Base-L30R-8x/checkpoints/final_fixed.pth
   File size: 512.1 MB
ðŸ”§ Initializing SAE model...
ðŸ”§ Loading checkpoint weights (CPU)...
âœ… Loaded PyTorch file
ðŸ”§ Processing checkpoint weights...
   Loading W_E (W_E)... âœ… (torch.Size([4096, 32768]))
   Loading b_E (b_E)... âœ… (torch.Size([32768]))
   Loading W_D (W_D)... âœ… (torch.Size([32768, 4096]))
   Loading b_D (b_D)... âœ… (torch.Size([4096]))
ðŸ”§ Clearing checkpoint from memory...
ðŸ”§ Loading state dict to model...
ðŸ”§ Moving model to cuda...
âœ… OPTIMIZED SAE loaded successfully!
   Layer: 30
   Expected reconstruction error: ~47%
   Feature clamping: VERIFIED WORKING
   Memory optimizations: ENABLED
âœ… SAE Layer 30 loaded
ðŸ”§ Loading SAE Layer 31...
ðŸ”§ Loading config: /data/.cache/huggingface/hub/models--fnlp--Llama3_1-8B-Base-LXR-8x/snapshots/8dbc1d85edfced43081c03c38b05514dbab1368b/Llama3_1-8B-Base-L31R-8x/hyperparams.json
âœ… Loading WORKING SAE for Layer 31
   Config: /data/.cache/huggingface/hub/models--fnlp--Llama3_1-8B-Base-LXR-8x/snapshots/8dbc1d85edfced43081c03c38b05514dbab1368b/Llama3_1-8B-Base-L31R-8x/hyperparams.json
   Dataset norm: 74.5
   Norm factor: 0.859060
   Using ReLU instead of JumpReLU (threshold=0.0)
ðŸ”§ Loading checkpoint: /data/.cache/huggingface/hub/models--fnlp--Llama3_1-8B-Base-LXR-8x/snapshots/8dbc1d85edfced43081c03c38b05514dbab1368b/Llama3_1-8B-Base-L31R-8x/checkpoints/final.safetensors
   File size: 512.1 MB
ðŸ”§ Initializing SAE model...
ðŸ”§ Loading checkpoint weights (CPU)...
âœ… Loaded safetensors file
ðŸ”§ Processing checkpoint weights...
   Loading W_E (encoder.weight)... âœ… (torch.Size([4096, 32768]))
   Loading b_E (encoder.bias)... âœ… (torch.Size([32768]))
   Loading W_D (decoder.weight)... âœ… (torch.Size([32768, 4096]))
   Loading b_D (decoder.bias)... âœ… (torch.Size([4096]))
ðŸ”§ Clearing checkpoint from memory...
ðŸ”§ Loading state dict to model...
ðŸ”§ Moving model to cuda...
âœ… OPTIMIZED SAE loaded successfully!
   Layer: 31
   Expected reconstruction error: ~47%
   Feature clamping: VERIFIED WORKING
   Memory optimizations: ENABLED
âœ… SAE Layer 31 loaded
âœ…

ðŸŽ® Game 2/50
  Round 2: Extracting L1-31 features... âœ…

ðŸŽ® Game 3/50
  Round 2: Extracting L1-31 features... âœ…
  Round 3: Extracting L1-31 features... âœ…
  Round 4: Extracting L1-31 features... âœ…
  Round 5: Extracting L1-31 features... âœ…
  Round 6: Extracting L1-31 features... âœ…
  Round 7: Extracting L1-31 features... âœ…

ðŸŽ® Game 4/50
  Round 2: Extracting L1-31 features... âœ…
  Round 3: Extracting L1-31 features... âœ…
  Round 4: Extracting L1-31 features... âœ…

ðŸŽ® Game 5/50
  Round 2: Extracting L1-31 features... âœ…
  Round 3: Extracting L1-31 features... âœ…

ðŸŽ® Game 6/50
  Round 2: Extracting L1-31 features... âœ…
  Round 3: Extracting L1-31 features... âœ…
  Round 4: Extracting L1-31 features... âœ…
  Round 5: Extracting L1-31 features... âœ…

ðŸŽ® Game 7/50
  Round 2: Extracting L1-31 features... âœ…
  Round 3: Extracting L1-31 features... âœ…
  Round 4: Extracting L1-31 features... âœ…
  Round 5: Extracting L1-31 features... âœ…
  Round 6: Extracting L1-31 features... âœ…

ðŸŽ® Game 8/50
  Round 2: Extracting L1-31 features... âœ…
  Round 3: Extracting L1-31 features... âœ…
  Round 4: Extracting L1-31 features... âœ…
  Round 5: Extracting L1-31 features... âœ…

ðŸŽ® Game 9/50
  Round 2: Extracting L1-31 features... âœ…

ðŸŽ® Game 10/50
  Round 2: Extracting L1-31 features... âœ…
  Round 3: Extracting L1-31 features... âœ…
  Round 4: Extracting L1-31 features... âœ…
  Round 5: Extracting L1-31 features... âœ…
  Round 6: Extracting L1-31 features... âœ…
ðŸ’¾ Checkpoint saved: /data/llm_addiction/experiment_1_pathway_L1_31/checkpoint_10_20251001_163821.json

ðŸŽ® Game 11/50
  Round 2: Extracting L1-31 features... âœ…

ðŸŽ® Game 12/50
  Round 2: Extracting L1-31 features... âœ…
  Round 3: Extracting L1-31 features... âœ…
  Round 4: Extracting L1-31 features... âœ…

ðŸŽ® Game 13/50
  Round 2: Extracting L1-31 features... âœ…

ðŸŽ® Game 14/50
  Round 2: Extracting L1-31 features... âœ…
  Round 3: Extracting L1-31 features... âœ…
  Round 4: Extracting L1-31 features... âœ…
  Round 5: Extracting L1-31 features... âœ…
  Round 6: Extracting L1-31 features... âœ…
  Round 7: Extracting L1-31 features... âœ…
  Round 8: Extracting L1-31 features... âœ…

ðŸŽ® Game 15/50
  Round 2: Extracting L1-31 features... âœ…
  Round 3: Extracting L1-31 features... âœ…
  Round 4: Extracting L1-31 features... âœ…
  Round 5: Extracting L1-31 features... âœ…
  Round 6: Extracting L1-31 features... âœ…

ðŸŽ® Game 16/50
  Round 2: Extracting L1-31 features... âœ…

ðŸŽ® Game 17/50
  Round 2: Extracting L1-31 features... âœ…

ðŸŽ® Game 18/50
  Round 2: Extracting L1-31 features... âœ…
  Round 3: Extracting L1-31 features... âœ…

ðŸŽ® Game 19/50
  Round 2: Extracting L1-31 features... âœ…
  Round 3: Extracting L1-31 features... âœ…
  Round 4: Extracting L1-31 features... âœ…
  Round 5: Extracting L1-31 features... âœ…
  Round 6: Extracting L1-31 features... âœ…
  Round 7: Extracting L1-31 features... âœ…
  Round 8: Extracting L1-31 features... âœ…
  Round 9: Extracting L1-31 features... âœ…
  Round 10: Extracting L1-31 features... âœ…
  Round 11: Extracting L1-31 features... âœ…
  Round 12: Extracting L1-31 features... âœ…
  Round 13: Extracting L1-31 features... âœ…

ðŸŽ® Game 20/50
  Round 2: Extracting L1-31 features... âœ…
ðŸ’¾ Checkpoint saved: /data/llm_addiction/experiment_1_pathway_L1_31/checkpoint_20_20251001_164023.json

ðŸŽ® Game 21/50
  Round 2: Extracting L1-31 features... âœ…
  Round 3: Extracting L1-31 features... âœ…

ðŸŽ® Game 22/50
  Round 2: Extracting L1-31 features... âœ…
  Round 3: Extracting L1-31 features... âœ…
  Round 4: Extracting L1-31 features... âœ…
  Round 5: Extracting L1-31 features... âœ…
  Round 6: Extracting L1-31 features... âœ…
  Round 7: Extracting L1-31 features... âœ…
  Round 8: Extracting L1-31 features... âœ…
  Round 9: Extracting L1-31 features... âœ…

ðŸŽ® Game 23/50
  Round 2: Extracting L1-31 features... âœ…
  Round 3: Extracting L1-31 features... âœ…
  Round 4: Extracting L1-31 features... âœ…
  Round 5: Extracting L1-31 features... âœ…
  Round 6: Extracting L1-31 features... âœ…
  Round 7: Extracting L1-31 features... âœ…
  Round 8: Extracting L1-31 features... âœ…
  Round 9: Extracting L1-31 features... âœ…

ðŸŽ® Game 24/50
  Round 2: Extracting L1-31 features... âœ…

ðŸŽ® Game 25/50
  Round 2: Extracting L1-31 features... âœ…
  Round 3: Extracting L1-31 features... âœ…

ðŸŽ® Game 26/50
  Round 2: Extracting L1-31 features... âœ…
  Round 3: Extracting L1-31 features... âœ…

ðŸŽ® Game 27/50
  Round 2: Extracting L1-31 features... âœ…
  Round 3: Extracting L1-31 features... âœ…

ðŸŽ® Game 28/50
  Round 2: Extracting L1-31 features... âœ…
  Round 3: Extracting L1-31 features... âœ…
  Round 4: Extracting L1-31 features... âœ…
  Round 5: Extracting L1-31 features... âœ…

ðŸŽ® Game 29/50
  Round 2: Extracting L1-31 features... âœ…

ðŸŽ® Game 30/50
  Round 2: Extracting L1-31 features... âœ…
  Round 3: Extracting L1-31 features... âœ…
  Round 4: Extracting L1-31 features... âœ…
ðŸ’¾ Checkpoint saved: /data/llm_addiction/experiment_1_pathway_L1_31/checkpoint_30_20251001_164316.json

ðŸŽ® Game 31/50
  Round 2: Extracting L1-31 features... âœ…
  Round 3: Extracting L1-31 features... âœ…
  Round 4: Extracting L1-31 features... âœ…
  Round 5: Extracting L1-31 features... âœ…
  Round 6: Extracting L1-31 features... âœ…
  Round 7: Extracting L1-31 features... âœ…
  Round 8: Extracting L1-31 features... âœ…

ðŸŽ® Game 32/50
  Round 2: Extracting L1-31 features... âœ…
  Round 3: Extracting L1-31 features... âœ…
  Round 4: Extracting L1-31 features... âœ…

ðŸŽ® Game 33/50
  Round 2: Extracting L1-31 features... âœ…
  Round 3: Extracting L1-31 features... âœ…
  Round 4: Extracting L1-31 features... âœ…

ðŸŽ® Game 34/50
  Round 2: Extracting L1-31 features... âœ…
  Round 3: Extracting L1-31 features... âœ…
  Round 4: Extracting L1-31 features... âœ…

ðŸŽ® Game 35/50
  Round 2: Extracting L1-31 features... âœ…
  Round 3: Extracting L1-31 features... âœ…
  Round 4: Extracting L1-31 features... âœ…
  Round 5: Extracting L1-31 features... âœ…

ðŸŽ® Game 36/50
  Round 2: Extracting L1-31 features... âœ…
  Round 3: Extracting L1-31 features... âœ…

ðŸŽ® Game 37/50
  Round 2: Extracting L1-31 features... âœ…

ðŸŽ® Game 38/50
  Round 2: Extracting L1-31 features... âœ…

ðŸŽ® Game 39/50
  Round 2: Extracting L1-31 features... âœ…

ðŸŽ® Game 40/50
  Round 2: Extracting L1-31 features... âœ…
  Round 3: Extracting L1-31 features... âœ…
ðŸ’¾ Checkpoint saved: /data/llm_addiction/experiment_1_pathway_L1_31/checkpoint_40_20251001_164626.json

ðŸŽ® Game 41/50
  Round 2: Extracting L1-31 features... âœ…
  Round 3: Extracting L1-31 features... âœ…
  Round 4: Extracting L1-31 features... âœ…

ðŸŽ® Game 42/50
  Round 2: Extracting L1-31 features... âœ…

ðŸŽ® Game 43/50
  Round 2: Extracting L1-31 features... âœ…

ðŸŽ® Game 44/50
  Round 2: Extracting L1-31 features... âœ…
  Round 3: Extracting L1-31 features... âœ…
  Round 4: Extracting L1-31 features... âœ…

ðŸŽ® Game 45/50
  Round 2: Extracting L1-31 features... âœ…

ðŸŽ® Game 46/50
  Round 2: Extracting L1-31 features... âœ…

ðŸŽ® Game 47/50
  Round 2: Extracting L1-31 features... âœ…

ðŸŽ® Game 48/50
  Round 2: Extracting L1-31 features... âœ…
  Round 3: Extracting L1-31 features... âœ…
  Round 4: Extracting L1-31 features... âœ…
  Round 5: Extracting L1-31 features... âœ…
  Round 6: Extracting L1-31 features... âœ…

ðŸŽ® Game 49/50
  Round 2: Extracting L1-31 features... âœ…
  Round 3: Extracting L1-31 features... âœ…
  Round 4: Extracting L1-31 features... âœ…
  Round 5: Extracting L1-31 features... âœ…

ðŸŽ® Game 50/50
  Round 2: Extracting L1-31 features... âœ…
ðŸ’¾ Checkpoint saved: /data/llm_addiction/experiment_1_pathway_L1_31/checkpoint_50_20251001_164936.json
ðŸ’¾ Final results saved: /data/llm_addiction/experiment_1_pathway_L1_31/final_pathway_L1_31_20251001_165207.json

================================================================================
ðŸ“Š FINAL SUMMARY
================================================================================
Total games: 50
Bankruptcies: 4 (8.0%)
Voluntary stops: 46 (92.0%)
================================================================================
