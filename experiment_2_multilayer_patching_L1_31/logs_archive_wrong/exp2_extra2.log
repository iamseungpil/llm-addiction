/data/miniforge3/lib/python3.10/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(

A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.2.6 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/ubuntu/llm_addiction/experiment_2_multilayer_patching_L1_31/experiment_2_L1_31_top300.py", line 549, in <module>
    exp.run()
  File "/home/ubuntu/llm_addiction/experiment_2_multilayer_patching_L1_31/experiment_2_L1_31_top300.py", line 489, in run
    self.load_models()
  File "/home/ubuntu/llm_addiction/experiment_2_multilayer_patching_L1_31/experiment_2_L1_31_top300.py", line 116, in load_models
    self.model = AutoModelForCausalLM.from_pretrained(
  File "/data/miniforge3/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 597, in from_pretrained
    model_class = _get_model_class(config, cls._model_mapping)
  File "/data/miniforge3/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 394, in _get_model_class
    supported_models = model_mapping[type(config)]
  File "/data/miniforge3/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 803, in __getitem__
    return self._load_attr_from_module(model_type, model_name)
  File "/data/miniforge3/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 817, in _load_attr_from_module
    return getattribute_from_module(self._modules[module_name], attr)
  File "/data/miniforge3/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 729, in getattribute_from_module
    if hasattr(module, attr):
  File "/data/miniforge3/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 2292, in __getattr__
    module = self._get_module(self._class_to_module[name])
  File "/data/miniforge3/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 2320, in _get_module
    return importlib.import_module("." + module_name, self.__name__)
  File "/data/miniforge3/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "/data/miniforge3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 30, in <module>
    from ...modeling_layers import (
  File "/data/miniforge3/lib/python3.10/site-packages/transformers/modeling_layers.py", line 29, in <module>
    from .processing_utils import Unpack
  File "/data/miniforge3/lib/python3.10/site-packages/transformers/processing_utils.py", line 41, in <module>
    from .video_utils import VideoMetadata, load_video
  File "/data/miniforge3/lib/python3.10/site-packages/transformers/video_utils.py", line 28, in <module>
    from .image_transforms import PaddingMode, to_channel_dimension_format
  File "/data/miniforge3/lib/python3.10/site-packages/transformers/image_transforms.py", line 51, in <module>
    import jax.numpy as jnp
  File "/data/miniforge3/lib/python3.10/site-packages/jax/__init__.py", line 37, in <module>
    import jax.core as _core
  File "/data/miniforge3/lib/python3.10/site-packages/jax/core.py", line 18, in <module>
    from jax._src.core import (
  File "/data/miniforge3/lib/python3.10/site-packages/jax/_src/core.py", line 38, in <module>
    from jax._src import dtypes
  File "/data/miniforge3/lib/python3.10/site-packages/jax/_src/dtypes.py", line 33, in <module>
    from jax._src import config
  File "/data/miniforge3/lib/python3.10/site-packages/jax/_src/config.py", line 27, in <module>
    from jax._src import lib
  File "/data/miniforge3/lib/python3.10/site-packages/jax/_src/lib/__init__.py", line 87, in <module>
    import jaxlib.xla_client as xla_client
  File "/data/miniforge3/lib/python3.10/site-packages/jaxlib/xla_client.py", line 32, in <module>
    from . import xla_extension as _xla
AttributeError: _ARRAY_API not found
================================================================================
ðŸš€ MULTILAYER PATCHING EXPERIMENT L6-L10
   GPU: 0, Process: extra2
   Trials per condition: 30
================================================================================
ðŸš€ Loading models on GPU 0
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
Traceback (most recent call last):
  File "/home/ubuntu/llm_addiction/experiment_2_multilayer_patching_L1_31/experiment_2_L1_31_top300.py", line 549, in <module>
    exp.run()
  File "/home/ubuntu/llm_addiction/experiment_2_multilayer_patching_L1_31/experiment_2_L1_31_top300.py", line 489, in run
    self.load_models()
  File "/home/ubuntu/llm_addiction/experiment_2_multilayer_patching_L1_31/experiment_2_L1_31_top300.py", line 116, in load_models
    self.model = AutoModelForCausalLM.from_pretrained(
  File "/data/miniforge3/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 600, in from_pretrained
    return model_class.from_pretrained(
  File "/data/miniforge3/lib/python3.10/site-packages/transformers/modeling_utils.py", line 316, in _wrapper
    return func(*args, **kwargs)
  File "/data/miniforge3/lib/python3.10/site-packages/transformers/modeling_utils.py", line 5061, in from_pretrained
    ) = cls._load_pretrained_model(
  File "/data/miniforge3/lib/python3.10/site-packages/transformers/modeling_utils.py", line 5524, in _load_pretrained_model
    _error_msgs, disk_offload_index, cpu_offload_index = load_shard_file(args)
  File "/data/miniforge3/lib/python3.10/site-packages/transformers/modeling_utils.py", line 974, in load_shard_file
    disk_offload_index, cpu_offload_index = _load_state_dict_into_meta_model(
  File "/data/miniforge3/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/data/miniforge3/lib/python3.10/site-packages/transformers/modeling_utils.py", line 842, in _load_state_dict_into_meta_model
    param = param[...]
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 45.88 MiB is free. Process 3530601 has 15.52 GiB memory in use. Process 3530766 has 15.52 GiB memory in use. Process 3530949 has 15.52 GiB memory in use. Process 3531138 has 15.51 GiB memory in use. Process 3531329 has 15.51 GiB memory in use. Including non-PyTorch memory, this process has 1.49 GiB memory in use. Of the allocated memory 1.09 GiB is allocated by PyTorch, and 1.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
