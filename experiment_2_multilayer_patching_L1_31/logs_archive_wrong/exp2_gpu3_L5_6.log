/data/miniforge3/lib/python3.10/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
================================================================================
ðŸš€ MULTILAYER PATCHING EXPERIMENT L5-L6
   GPU: 0, Process: gpu3_L5_6
   Trials per condition: 30
================================================================================
ðŸš€ Loading models on GPU 0
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|â–ˆâ–ˆâ–Œ       | 1/4 [00:01<00:04,  1.47s/it]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2/4 [00:02<00:02,  1.48s/it]Loading checkpoint shards:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4 [00:04<00:01,  1.50s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:04<00:00,  1.05s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:04<00:00,  1.21s/it]
âœ… LLaMA loaded successfully
ðŸ”§ SAEs will be loaded on-demand
ðŸ” Loading L1-31 features from /data/llm_addiction/experiment_1_L1_31_extraction/L1_31_features_FINAL_20250930_220003.json
Layer 5: 1752 significant -> selecting top 300
Layer 6: 1784 significant -> selecting top 300

âœ… Loaded 600 features total
ðŸ“Š Layer distribution: {5: 300, 6: 300}

ðŸ§ª Testing 600 features...
Testing features:   0%|          | 0/600 [00:00<?, ?it/s]