/data/miniforge3/lib/python3.10/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
================================================================================
ðŸš€ MULTILAYER PATCHING EXPERIMENT L29-L31
   GPU: 0, Process: gpu7_L29_31
   Trials per condition: 30
================================================================================
ðŸš€ Loading models on GPU 0
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|â–ˆâ–ˆâ–Œ       | 1/4 [00:01<00:04,  1.50s/it]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2/4 [00:02<00:02,  1.47s/it]Loading checkpoint shards:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4 [00:04<00:01,  1.47s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:04<00:00,  1.03s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:04<00:00,  1.20s/it]
âœ… LLaMA loaded successfully
ðŸ”§ SAEs will be loaded on-demand
ðŸ” Loading L1-31 features from /data/llm_addiction/experiment_1_L1_31_extraction/L1_31_features_FINAL_20250930_220003.json
Layer 29: 3450 significant -> selecting top 300
Layer 30: 3434 significant -> selecting top 300
Layer 31: 3328 significant -> selecting top 300

âœ… Loaded 900 features total
ðŸ“Š Layer distribution: {29: 300, 30: 300, 31: 300}

ðŸ§ª Testing 900 features...
Testing features:   0%|          | 0/900 [00:00<?, ?it/s]
ðŸ§ª Testing L29-2374
   Cohen's d: -1.532
   Safe mean: 0.366455, Risky mean: 0.194336
ðŸ”§ Loading SAE Layer 29...
ðŸ”§ Loading config: /data/.cache/huggingface/hub/models--fnlp--Llama3_1-8B-Base-LXR-8x/snapshots/8dbc1d85edfced43081c03c38b05514dbab1368b/Llama3_1-8B-Base-L29R-8x/hyperparams.json
âœ… Loading WORKING SAE for Layer 29
   Config: /data/.cache/huggingface/hub/models--fnlp--Llama3_1-8B-Base-LXR-8x/snapshots/8dbc1d85edfced43081c03c38b05514dbab1368b/Llama3_1-8B-Base-L29R-8x/hyperparams.json
   Dataset norm: 46.5
   Norm factor: 1.376344
   Using ReLU instead of JumpReLU (threshold=0.0)
ðŸ”§ Loading checkpoint: /data/.cache/huggingface/hub/models--fnlp--Llama3_1-8B-Base-LXR-8x/snapshots/8dbc1d85edfced43081c03c38b05514dbab1368b/Llama3_1-8B-Base-L29R-8x/checkpoints/final.safetensors
   File size: 512.1 MB
ðŸ”§ Initializing SAE model...
ðŸ”§ Loading checkpoint weights (CPU)...
âœ… Loaded safetensors file
ðŸ”§ Processing checkpoint weights...
   Loading W_E (encoder.weight)... converted... âœ… (torch.Size([4096, 32768]))
   Loading b_E (encoder.bias)... converted... âœ… (torch.Size([32768]))
   Loading W_D (decoder.weight)... converted... âœ… (torch.Size([32768, 4096]))
   Loading b_D (decoder.bias)... converted... Testing features:   0%|          | 1/900 [08:48<131:52:09, 528.06s/it]Testing features:   0%|          | 2/900 [18:42<141:31:07, 567.34s/it]Testing features:   0%|          | 3/900 [28:24<143:01:34, 574.02s/it]Testing features:   0%|          | 4/900 [37:10<138:09:43, 555.12s/it]