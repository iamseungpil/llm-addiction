/data/miniforge3/lib/python3.10/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
ðŸŽ¯ Layers 1-31
ðŸ’¾ Features: True, Attention: True
================================================================================
EXPERIMENT 6 OPTIMIZED: L1-L31
================================================================================
ðŸ”§ Loading LLaMA model...
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|â–ˆâ–ˆâ–Œ       | 1/4 [00:04<00:12,  4.14s/it]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2/4 [00:05<00:05,  2.57s/it]Loading checkpoint shards:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4 [00:07<00:02,  2.28s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:09<00:00,  1.96s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:09<00:00,  2.26s/it]
âœ… Model loaded
Scenarios:   0%|          | 0/10 [00:00<?, ?it/s]
ðŸ” Bankruptcy_90_all_in
  Tokens: 121

  Layers:   0%|          | 0/31 [00:00<?, ?it/s][A  Loading SAE Layer 1...
ðŸ”§ Loading config: /data/.cache/huggingface/hub/models--fnlp--Llama3_1-8B-Base-LXR-8x/snapshots/8dbc1d85edfced43081c03c38b05514dbab1368b/Llama3_1-8B-Base-L1R-8x/hyperparams.json
âœ… Loading WORKING SAE for Layer 1
   Config: /data/.cache/huggingface/hub/models--fnlp--Llama3_1-8B-Base-LXR-8x/snapshots/8dbc1d85edfced43081c03c38b05514dbab1368b/Llama3_1-8B-Base-L1R-8x/hyperparams.json
   Dataset norm: 1.609375
   Norm factor: 39.766990
   Using ReLU instead of JumpReLU (threshold=0.0)
ðŸ”§ Loading checkpoint: /data/.cache/huggingface/hub/models--fnlp--Llama3_1-8B-Base-LXR-8x/snapshots/8dbc1d85edfced43081c03c38b05514dbab1368b/Llama3_1-8B-Base-L1R-8x/checkpoints/final.safetensors
   File size: 512.1 MB
ðŸ”§ Initializing SAE model...
ðŸ”§ Loading checkpoint weights (CPU)...
âœ… Loaded safetensors file
ðŸ”§ Processing checkpoint weights...
   Loading W_E (encoder.weight)... converted... âœ… (torch.Size([4096, 32768]))
   Loading b_E (encoder.bias)... converted... âœ… (torch.Size([32768]))
   Loading W_D (decoder.weight)... converted... âœ… (torch.Size([32768, 4096]))
   Loading b_D (decoder.bias)... converted... 
  Layers:   3%|â–Ž         | 1/31 [00:03<01:34,  3.15s/it][Aâœ… (torch.Size([4096]))
ðŸ”§ Clearing checkpoint from memory...
ðŸ”§ Loading state dict to model...
ðŸ”§ Moving model to cuda:0...
âœ… OPTIMIZED SAE loaded successfully!
   Layer: 1
   Expected reconstruction error: ~47%
   Feature clamping: VERIFIED WORKING
   Memory optimizations: ENABLED
  Loading SAE Layer 2...
ðŸ”§ Loading config: /data/.cache/huggingface/hub/models--fnlp--Llama3_1-8B-Base-LXR-8x/snapshots/8dbc1d85edfced43081c03c38b05514dbab1368b/Llama3_1-8B-Base-L2R-8x/hyperparams.json
âœ… Loading WORKING SAE for Layer 2
   Config: /data/.cache/huggingface/hub/models--fnlp--Llama3_1-8B-Base-LXR-8x/snapshots/8dbc1d85edfced43081c03c38b05514dbab1368b/Llama3_1-8B-Base-L2R-8x/hyperparams.json
   Dataset norm: 2.34375
   Norm factor: 27.306667
   Using ReLU instead of JumpReLU (threshold=0.0)
ðŸ”§ Loading checkpoint: /data/.cache/huggingface/hub/models--fnlp--Llama3_1-8B-Base-LXR-8x/snapshots/8dbc1d85edfced43081c03c38b05514dbab1368b/Llama3_1-8B-Base-L2R-8x/checkpoints/final.safetensors
   File size: 512.1 MB
ðŸ”§ Initializing SAE model...
ðŸ”§ Loading checkpoint weights (CPU)...
âœ… Loaded safetensors file
ðŸ”§ Processing checkpoint weights...
   Loading W_E (encoder.weight)... converted... âœ… (torch.Size([4096, 32768]))
   Loading b_E (encoder.bias)... converted... âœ… (torch.Size([32768]))
   Loading W_D (decoder.weight)... converted... âœ… (torch.Size([32768, 4096]))
   Loading b_D (decoder.bias)... converted... 
  Layers:   6%|â–‹         | 2/31 [00:06<01:35,  3.30s/it][Aâœ… (torch.Size([4096]))
ðŸ”§ Clearing checkpoint from memory...
ðŸ”§ Loading state dict to model...
ðŸ”§ Moving model to cuda:0...
âœ… OPTIMIZED SAE loaded successfully!
   Layer: 2
   Expected reconstruction error: ~47%
   Feature clamping: VERIFIED WORKING
   Memory optimizations: ENABLED
  Loading SAE Layer 3...
ðŸ”§ Loading config: /data/.cache/huggingface/hub/models--fnlp--Llama3_1-8B-Base-LXR-8x/snapshots/8dbc1d85edfced43081c03c38b05514dbab1368b/Llama3_1-8B-Base-L3R-8x/hyperparams.json
âœ… Loading WORKING SAE for Layer 3
   Config: /data/.cache/huggingface/hub/models--fnlp--Llama3_1-8B-Base-LXR-8x/snapshots/8dbc1d85edfced43081c03c38b05514dbab1368b/Llama3_1-8B-Base-L3R-8x/hyperparams.json
   Dataset norm: 3.171875
   Norm factor: 20.177340
   Using ReLU instead of JumpReLU (threshold=0.0)
ðŸ”§ Loading checkpoint: /data/.cache/huggingface/hub/models--fnlp--Llama3_1-8B-Base-LXR-8x/snapshots/8dbc1d85edfced43081c03c38b05514dbab1368b/Llama3_1-8B-Base-L3R-8x/checkpoints/final.safetensors
   File size: 512.1 MB
ðŸ”§ Initializing SAE model...
ðŸ”§ Loading checkpoint weights (CPU)...
âœ… Loaded safetensors file
ðŸ”§ Processing checkpoint weights...
   Loading W_E (encoder.weight)... converted... âœ… (torch.Size([4096, 32768]))
   Loading b_E (encoder.bias)... converted... âœ… (torch.Size([32768]))
   Loading W_D (decoder.weight)... converted... âœ… (torch.Size([32768, 4096]))
   Loading b_D (decoder.bias)... converted... 
  Layers:  10%|â–‰         | 3/31 [00:09<01:27,  3.13s/it][Aâœ… (torch.Size([4096]))
ðŸ”§ Clearing checkpoint from memory...
ðŸ”§ Loading state dict to model...
ðŸ”§ Moving model to cuda:0...
âœ… OPTIMIZED SAE loaded successfully!
   Layer: 3
   Expected reconstruction error: ~47%
   Feature clamping: VERIFIED WORKING
   Memory optimizations: ENABLED
  Loading SAE Layer 4...
ðŸ”§ Loading config: /data/.cache/huggingface/hub/models--fnlp--Llama3_1-8B-Base-LXR-8x/snapshots/8dbc1d85edfced43081c03c38b05514dbab1368b/Llama3_1-8B-Base-L4R-8x/hyperparams.json
âœ… Loading WORKING SAE for Layer 4
   Config: /data/.cache/huggingface/hub/models--fnlp--Llama3_1-8B-Base-LXR-8x/snapshots/8dbc1d85edfced43081c03c38b05514dbab1368b/Llama3_1-8B-Base-L4R-8x/hyperparams.json
   Dataset norm: 3.96875
   Norm factor: 16.125984
   Using ReLU instead of JumpReLU (threshold=0.0)
ðŸ”§ Loading checkpoint: /data/.cache/huggingface/hub/models--fnlp--Llama3_1-8B-Base-LXR-8x/snapshots/8dbc1d85edfced43081c03c38b05514dbab1368b/Llama3_1-8B-Base-L4R-8x/checkpoints/final.safetensors
   File size: 512.1 MB
ðŸ”§ Initializing SAE model...
ðŸ”§ Loading checkpoint weights (CPU)...
âœ… Loaded safetensors file
ðŸ”§ Processing checkpoint weights...
   Loading W_E (encoder.weight)... converted... âœ… (torch.Size([4096, 32768]))
   Loading b_E (encoder.bias)... converted... âœ… (torch.Size([32768]))
   Loading W_D (decoder.weight)... converted... âœ… (torch.Size([32768, 4096]))
   Loading b_D (decoder.bias)... converted... 
  Layers:  13%|â–ˆâ–Ž        | 4/31 [00:12<01:22,  3.07s/it][Aâœ… (torch.Size([4096]))
ðŸ”§ Clearing checkpoint from memory...
ðŸ”§ Loading state dict to model...
ðŸ”§ Moving model to cuda:0...
âœ… OPTIMIZED SAE loaded successfully!
   Layer: 4
   Expected reconstruction error: ~47%
   Feature clamping: VERIFIED WORKING
   Memory optimizations: ENABLED
  Loading SAE Layer 5...
ðŸ”§ Loading config: /data/.cache/huggingface/hub/models--fnlp--Llama3_1-8B-Base-LXR-8x/snapshots/8dbc1d85edfced43081c03c38b05514dbab1368b/Llama3_1-8B-Base-L5R-8x/hyperparams.json
âœ… Loading WORKING SAE for Layer 5
   Config: /data/.cache/huggingface/hub/models--fnlp--Llama3_1-8B-Base-LXR-8x/snapshots/8dbc1d85edfced43081c03c38b05514dbab1368b/Llama3_1-8B-Base-L5R-8x/hyperparams.json
   Dataset norm: 4.6875
   Norm factor: 13.653333
   Using ReLU instead of JumpReLU (threshold=0.0)
ðŸ”§ Loading checkpoint: /data/.cache/huggingface/hub/models--fnlp--Llama3_1-8B-Base-LXR-8x/snapshots/8dbc1d85edfced43081c03c38b05514dbab1368b/Llama3_1-8B-Base-L5R-8x/checkpoints/final.safetensors
   File size: 512.1 MB
ðŸ”§ Initializing SAE model...
ðŸ”§ Loading checkpoint weights (CPU)...
âœ… Loaded safetensors file
ðŸ”§ Processing checkpoint weights...
   Loading W_E (encoder.weight)... converted... âœ… (torch.Size([4096, 32768]))
   Loading b_E (encoder.bias)... converted... âœ… (torch.Size([32768]))
   Loading W_D (decoder.weight)... converted... âœ… (torch.Size([32768, 4096]))
   Loading b_D (decoder.bias)... converted... 
  Layers:  16%|â–ˆâ–Œ        | 5/31 [00:15<01:17,  2.99s/it][Aâœ… (torch.Size([4096]))
ðŸ”§ Clearing checkpoint from memory...
ðŸ”§ Loading state dict to model...
ðŸ”§ Moving model to cuda:0...
âœ… OPTIMIZED SAE loaded successfully!
   Layer: 5
   Expected reconstruction error: ~47%
   Feature clamping: VERIFIED WORKING
   Memory optimizations: ENABLED
  Loading SAE Layer 6...
ðŸ”§ Loading config: /data/.cache/huggingface/hub/models--fnlp--Llama3_1-8B-Base-LXR-8x/snapshots/8dbc1d85edfced43081c03c38b05514dbab1368b/Llama3_1-8B-Base-L6R-8x/hyperparams.json
âœ… Loading WORKING SAE for Layer 6
   Config: /data/.cache/huggingface/hub/models--fnlp--Llama3_1-8B-Base-LXR-8x/snapshots/8dbc1d85edfced43081c03c38b05514dbab1368b/Llama3_1-8B-Base-L6R-8x/hyperparams.json
   Dataset norm: 5.34375
   Norm factor: 11.976608
   Using ReLU instead of JumpReLU (threshold=0.0)
ðŸ”§ Loading checkpoint: /data/.cache/huggingface/hub/models--fnlp--Llama3_1-8B-Base-LXR-8x/snapshots/8dbc1d85edfced43081c03c38b05514dbab1368b/Llama3_1-8B-Base-L6R-8x/checkpoints/final.safetensors
   File size: 512.1 MB
ðŸ”§ Initializing SAE model...
ðŸ”§ Loading checkpoint weights (CPU)...
âœ… Loaded safetensors file
ðŸ”§ Processing checkpoint weights...
   Loading W_E (encoder.weight)... converted... âœ… (torch.Size([4096, 32768]))
   Loading b_E (encoder.bias)... converted... âœ… (torch.Size([32768]))
   Loading W_D (decoder.weight)... converted... âœ… (torch.Size([32768, 4096]))
   Loading b_D (decoder.bias)... converted... 
  Layers:  19%|â–ˆâ–‰        | 6/31 [00:18<01:13,  2.95s/it][Aâœ… (torch.Size([4096]))
ðŸ”§ Clearing checkpoint from memory...
ðŸ”§ Loading state dict to model...
ðŸ”§ Moving model to cuda:0...
âœ… OPTIMIZED SAE loaded successfully!
   Layer: 6
   Expected reconstruction error: ~47%
   Feature clamping: VERIFIED WORKING
   Memory optimizations: ENABLED
  Loading SAE Layer 7...
ðŸ”§ Loading config: /data/.cache/huggingface/hub/models--fnlp--Llama3_1-8B-Base-LXR-8x/snapshots/8dbc1d85edfced43081c03c38b05514dbab1368b/Llama3_1-8B-Base-L7R-8x/hyperparams.json
âœ… Loading WORKING SAE for Layer 7
   Config: /data/.cache/huggingface/hub/models--fnlp--Llama3_1-8B-Base-LXR-8x/snapshots/8dbc1d85edfced43081c03c38b05514dbab1368b/Llama3_1-8B-Base-L7R-8x/hyperparams.json
   Dataset norm: 5.90625
   Norm factor: 10.835979
   Using ReLU instead of JumpReLU (threshold=0.0)
ðŸ”§ Loading checkpoint: /data/.cache/huggingface/hub/models--fnlp--Llama3_1-8B-Base-LXR-8x/snapshots/8dbc1d85edfced43081c03c38b05514dbab1368b/Llama3_1-8B-Base-L7R-8x/checkpoints/final.safetensors
   File size: 512.1 MB
ðŸ”§ Initializing SAE model...
ðŸ”§ Loading checkpoint weights (CPU)...
âœ… Loaded safetensors file
ðŸ”§ Processing checkpoint weights...
   Loading W_E (encoder.weight)... converted... âœ… (torch.Size([4096, 32768]))
   Loading b_E (encoder.bias)... converted... âœ… (torch.Size([32768]))
   Loading W_D (decoder.weight)... converted... âœ… (torch.Size([32768, 4096]))
   Loading b_D (decoder.bias)... converted... 
  Layers:  23%|â–ˆâ–ˆâ–Ž       | 7/31 [00:21<01:10,  2.94s/it][Aâœ… (torch.Size([4096]))
ðŸ”§ Clearing checkpoint from memory...
ðŸ”§ Loading state dict to model...
ðŸ”§ Moving model to cuda:0...
âœ… OPTIMIZED SAE loaded successfully!
   Layer: 7
   Expected reconstruction error: ~47%
   Feature clamping: VERIFIED WORKING
   Memory optimizations: ENABLED
  Loading SAE Layer 8...
ðŸ”§ Loading config: /data/.cache/huggingface/hub/models--fnlp--Llama3_1-8B-Base-LXR-8x/snapshots/8dbc1d85edfced43081c03c38b05514dbab1368b/Llama3_1-8B-Base-L8R-8x/hyperparams.json
âœ… Loading WORKING SAE for Layer 8
   Config: /data/.cache/huggingface/hub/models--fnlp--Llama3_1-8B-Base-LXR-8x/snapshots/8dbc1d85edfced43081c03c38b05514dbab1368b/Llama3_1-8B-Base-L8R-8x/hyperparams.json
   Dataset norm: 6.3125
   Norm factor: 10.138614
   Using ReLU instead of JumpReLU (threshold=0.0)
ðŸ”§ Loading checkpoint: /data/.cache/huggingface/hub/models--fnlp--Llama3_1-8B-Base-LXR-8x/snapshots/8dbc1d85edfced43081c03c38b05514dbab1368b/Llama3_1-8B-Base-L8R-8x/checkpoints/final.safetensors
   File size: 512.1 MB
ðŸ”§ Initializing SAE model...
ðŸ”§ Loading checkpoint weights (CPU)...
âœ… Loaded safetensors file
ðŸ”§ Processing checkpoint weights...
   Loading W_E (encoder.weight)... converted... âœ… (torch.Size([4096, 32768]))
   Loading b_E (encoder.bias)... converted... âœ… (torch.Size([32768]))
   Loading W_D (decoder.weight)... converted... âœ… (torch.Size([32768, 4096]))
   Loading b_D (decoder.bias)... converted... 
  Layers:  26%|â–ˆâ–ˆâ–Œ       | 8/31 [00:24<01:08,  2.98s/it][Aâœ… (torch.Size([4096]))
ðŸ”§ Clearing checkpoint from memory...
ðŸ”§ Loading state dict to model...
ðŸ”§ Moving model to cuda:0...
âœ… OPTIMIZED SAE loaded successfully!
   Layer: 8
   Expected reconstruction error: ~47%
   Feature clamping: VERIFIED WORKING
   Memory optimizations: ENABLED
  Loading SAE Layer 9...
ðŸ”§ Loading config: /data/.cache/huggingface/hub/models--fnlp--Llama3_1-8B-Base-LXR-8x/snapshots/8dbc1d85edfced43081c03c38b05514dbab1368b/Llama3_1-8B-Base-L9R-8x/hyperparams.json
âœ… Loading WORKING SAE for Layer 9
   Config: /data/.cache/huggingface/hub/models--fnlp--Llama3_1-8B-Base-LXR-8x/snapshots/8dbc1d85edfced43081c03c38b05514dbab1368b/Llama3_1-8B-Base-L9R-8x/hyperparams.json
   Dataset norm: 6.90625
   Norm factor: 9.266968
   Using ReLU instead of JumpReLU (threshold=0.0)
ðŸ”§ Loading checkpoint: /data/.cache/huggingface/hub/models--fnlp--Llama3_1-8B-Base-LXR-8x/snapshots/8dbc1d85edfced43081c03c38b05514dbab1368b/Llama3_1-8B-Base-L9R-8x/checkpoints/final.safetensors
   File size: 512.1 MB
ðŸ”§ Initializing SAE model...
ðŸ”§ Loading checkpoint weights (CPU)...
âœ… Loaded safetensors file
ðŸ”§ Processing checkpoint weights...
   Loading W_E (encoder.weight)... converted... âœ… (torch.Size([4096, 32768]))
   Loading b_E (encoder.bias)... converted... âœ… (torch.Size([32768]))
   Loading W_D (decoder.weight)... converted... âœ… (torch.Size([32768, 4096]))
   Loading b_D (decoder.bias)... converted... 
  Layers:  29%|â–ˆâ–ˆâ–‰       | 9/31 [00:27<01:05,  2.99s/it][Aâœ… (torch.Size([4096]))
ðŸ”§ Clearing checkpoint from memory...
ðŸ”§ Loading state dict to model...
ðŸ”§ Moving model to cuda:0...
âœ… OPTIMIZED SAE loaded successfully!
   Layer: 9
   Expected reconstruction error: ~47%
   Feature clamping: VERIFIED WORKING
   Memory optimizations: ENABLED
  Loading SAE Layer 10...
ðŸ”§ Loading config: /data/.cache/huggingface/hub/models--fnlp--Llama3_1-8B-Base-LXR-8x/snapshots/8dbc1d85edfced43081c03c38b05514dbab1368b/Llama3_1-8B-Base-L10R-8x/hyperparams.json
âœ… Loading WORKING SAE for Layer 10
   Config: /data/.cache/huggingface/hub/models--fnlp--Llama3_1-8B-Base-LXR-8x/snapshots/8dbc1d85edfced43081c03c38b05514dbab1368b/Llama3_1-8B-Base-L10R-8x/hyperparams.json
   Dataset norm: 7.25
   Norm factor: 8.827586
   Using ReLU instead of JumpReLU (threshold=0.0)
ðŸ”§ Loading checkpoint: /data/.cache/huggingface/hub/models--fnlp--Llama3_1-8B-Base-LXR-8x/snapshots/8dbc1d85edfced43081c03c38b05514dbab1368b/Llama3_1-8B-Base-L10R-8x/checkpoints/final.safetensors
   File size: 512.1 MB
ðŸ”§ Initializing SAE model...
ðŸ”§ Loading checkpoint weights (CPU)...
âœ… Loaded safetensors file
ðŸ”§ Processing checkpoint weights...
   Loading W_E (encoder.weight)... converted... âœ… (torch.Size([4096, 32768]))
   Loading b_E (encoder.bias)... converted... âœ… (torch.Size([32768]))
   Loading W_D (decoder.weight)... converted... âœ… (torch.Size([32768, 4096]))
   Loading b_D (decoder.bias)... converted... 
  Layers:  32%|â–ˆâ–ˆâ–ˆâ–      | 10/31 [00:30<01:02,  2.97s/it][Aâœ… (torch.Size([4096]))
ðŸ”§ Clearing checkpoint from memory...
ðŸ”§ Loading state dict to model...
ðŸ”§ Moving model to cuda:0...
âœ… OPTIMIZED SAE loaded successfully!
   Layer: 10
   Expected reconstruction error: ~47%
   Feature clamping: VERIFIED WORKING
   Memory optimizations: ENABLED
  Loading SAE Layer 11...
ðŸ”§ Loading config: /data/.cache/huggingface/hub/models--fnlp--Llama3_1-8B-Base-LXR-8x/snapshots/8dbc1d85edfced43081c03c38b05514dbab1368b/Llama3_1-8B-Base-L11R-8x/hyperparams.json
âœ… Loading WORKING SAE for Layer 11
   Config: /data/.cache/huggingface/hub/models--fnlp--Llama3_1-8B-Base-LXR-8x/snapshots/8dbc1d85edfced43081c03c38b05514dbab1368b/Llama3_1-8B-Base-L11R-8x/hyperparams.json
   Dataset norm: 7.5625
   Norm factor: 8.462810
   Using ReLU instead of JumpReLU (threshold=0.0)
ðŸ”§ Loading checkpoint: /data/.cache/huggingface/hub/models--fnlp--Llama3_1-8B-Base-LXR-8x/snapshots/8dbc1d85edfced43081c03c38b05514dbab1368b/Llama3_1-8B-Base-L11R-8x/checkpoints/final.safetensors
   File size: 512.1 MB
ðŸ”§ Initializing SAE model...
ðŸ”§ Loading checkpoint weights (CPU)...
âœ… Loaded safetensors file
ðŸ”§ Processing checkpoint weights...
   Loading W_E (encoder.weight)... converted... âœ… (torch.Size([4096, 32768]))
   Loading b_E (encoder.bias)... converted... âœ… (torch.Size([32768]))
   Loading W_D (decoder.weight)... converted... âœ… (torch.Size([32768, 4096]))
   Loading b_D (decoder.bias)... converted... 
  Layers:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 11/31 [00:33<00:59,  2.99s/it][Aâœ… (torch.Size([4096]))
ðŸ”§ Clearing checkpoint from memory...
ðŸ”§ Loading state dict to model...
ðŸ”§ Moving model to cuda:0...
âœ… OPTIMIZED SAE loaded successfully!
   Layer: 11
   Expected reconstruction error: ~47%
   Feature clamping: VERIFIED WORKING
   Memory optimizations: ENABLED
  Loading SAE Layer 12...
ðŸ”§ Loading config: /data/.cache/huggingface/hub/models--fnlp--Llama3_1-8B-Base-LXR-8x/snapshots/8dbc1d85edfced43081c03c38b05514dbab1368b/Llama3_1-8B-Base-L12R-8x/hyperparams.json
âœ… Loading WORKING SAE for Layer 12
   Config: /data/.cache/huggingface/hub/models--fnlp--Llama3_1-8B-Base-LXR-8x/snapshots/8dbc1d85edfced43081c03c38b05514dbab1368b/Llama3_1-8B-Base-L12R-8x/hyperparams.json
   Dataset norm: 8.0625
   Norm factor: 7.937984
   Using ReLU instead of JumpReLU (threshold=0.0)
ðŸ”§ Loading checkpoint: /data/.cache/huggingface/hub/models--fnlp--Llama3_1-8B-Base-LXR-8x/snapshots/8dbc1d85edfced43081c03c38b05514dbab1368b/Llama3_1-8B-Base-L12R-8x/checkpoints/final.safetensors
   File size: 512.1 MB
ðŸ”§ Initializing SAE model...
ðŸ”§ Loading checkpoint weights (CPU)...
âœ… Loaded safetensors file
ðŸ”§ Processing checkpoint weights...
   Loading W_E (encoder.weight)... converted... âœ… (torch.Size([4096, 32768]))
   Loading b_E (encoder.bias)... converted... âœ… (torch.Size([32768]))
   Loading W_D (decoder.weight)... converted... âœ… (torch.Size([32768, 4096]))
   Loading b_D (decoder.bias)... converted... 
  Layers:  39%|â–ˆâ–ˆâ–ˆâ–Š      | 12/31 [00:36<00:56,  2.99s/it][Aâœ… (torch.Size([4096]))
ðŸ”§ Clearing checkpoint from memory...
ðŸ”§ Loading state dict to model...
ðŸ”§ Moving model to cuda:0...
âœ… OPTIMIZED SAE loaded successfully!
   Layer: 12
   Expected reconstruction error: ~47%
   Feature clamping: VERIFIED WORKING
   Memory optimizations: ENABLED
  Loading SAE Layer 13...
ðŸ”§ Loading config: /data/.cache/huggingface/hub/models--fnlp--Llama3_1-8B-Base-LXR-8x/snapshots/8dbc1d85edfced43081c03c38b05514dbab1368b/Llama3_1-8B-Base-L13R-8x/hyperparams.json
âœ… Loading WORKING SAE for Layer 13
   Config: /data/.cache/huggingface/hub/models--fnlp--Llama3_1-8B-Base-LXR-8x/snapshots/8dbc1d85edfced43081c03c38b05514dbab1368b/Llama3_1-8B-Base-L13R-8x/hyperparams.json
   Dataset norm: 8.8125
   Norm factor: 7.262411
   Using ReLU instead of JumpReLU (threshold=0.0)
ðŸ”§ Loading checkpoint: /data/.cache/huggingface/hub/models--fnlp--Llama3_1-8B-Base-LXR-8x/snapshots/8dbc1d85edfced43081c03c38b05514dbab1368b/Llama3_1-8B-Base-L13R-8x/checkpoints/final.safetensors
   File size: 512.1 MB
ðŸ”§ Initializing SAE model...
ðŸ”§ Loading checkpoint weights (CPU)...
âœ… Loaded safetensors file
ðŸ”§ Processing checkpoint weights...
   Loading W_E (encoder.weight)... converted... âœ… (torch.Size([4096, 32768]))
   Loading b_E (encoder.bias)... converted... âœ… (torch.Size([32768]))
   Loading W_D (decoder.weight)... converted... âœ… (torch.Size([32768, 4096]))
   Loading b_D (decoder.bias)... converted... 
  Layers:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 13/31 [00:38<00:52,  2.93s/it][Aâœ… (torch.Size([4096]))
ðŸ”§ Clearing checkpoint from memory...
ðŸ”§ Loading state dict to model...
ðŸ”§ Moving model to cuda:0...
âœ… OPTIMIZED SAE loaded successfully!
   Layer: 13
   Expected reconstruction error: ~47%
   Feature clamping: VERIFIED WORKING
   Memory optimizations: ENABLED
  Loading SAE Layer 14...
ðŸ”§ Loading config: /data/.cache/huggingface/hub/models--fnlp--Llama3_1-8B-Base-LXR-8x/snapshots/8dbc1d85edfced43081c03c38b05514dbab1368b/Llama3_1-8B-Base-L14R-8x/hyperparams.json
âœ… Loading WORKING SAE for Layer 14
   Config: /data/.cache/huggingface/hub/models--fnlp--Llama3_1-8B-Base-LXR-8x/snapshots/8dbc1d85edfced43081c03c38b05514dbab1368b/Llama3_1-8B-Base-L14R-8x/hyperparams.json
   Dataset norm: 9.5625
   Norm factor: 6.692810
   Using ReLU instead of JumpReLU (threshold=0.0)
ðŸ”§ Loading checkpoint: /data/.cache/huggingface/hub/models--fnlp--Llama3_1-8B-Base-LXR-8x/snapshots/8dbc1d85edfced43081c03c38b05514dbab1368b/Llama3_1-8B-Base-L14R-8x/checkpoints/final.safetensors
   File size: 512.1 MB
ðŸ”§ Initializing SAE model...
ðŸ”§ Loading checkpoint weights (CPU)...
âœ… Loaded safetensors file
ðŸ”§ Processing checkpoint weights...
   Loading W_E (encoder.weight)... converted... âœ… (torch.Size([4096, 32768]))
   Loading b_E (encoder.bias)... converted... âœ… (torch.Size([32768]))
   Loading W_D (decoder.weight)... converted... âœ… (torch.Size([32768, 4096]))
   Loading b_D (decoder.bias)... converted... 
  Layers:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 14/31 [00:41<00:49,  2.93s/it][Aâœ… (torch.Size([4096]))
ðŸ”§ Clearing checkpoint from memory...
ðŸ”§ Loading state dict to model...
ðŸ”§ Moving model to cuda:0...
âœ… OPTIMIZED SAE loaded successfully!
   Layer: 14
   Expected reconstruction error: ~47%
   Feature clamping: VERIFIED WORKING
   Memory optimizations: ENABLED
  Loading SAE Layer 15...
ðŸ”§ Loading config: /data/.cache/huggingface/hub/models--fnlp--Llama3_1-8B-Base-LXR-8x/snapshots/8dbc1d85edfced43081c03c38b05514dbab1368b/Llama3_1-8B-Base-L15R-8x/hyperparams.json
âœ… Loading WORKING SAE for Layer 15
   Config: /data/.cache/huggingface/hub/models--fnlp--Llama3_1-8B-Base-LXR-8x/snapshots/8dbc1d85edfced43081c03c38b05514dbab1368b/Llama3_1-8B-Base-L15R-8x/hyperparams.json
   Dataset norm: 10.8125
   Norm factor: 5.919075
   Using ReLU instead of JumpReLU (threshold=0.0)
ðŸ”§ Loading checkpoint: /data/.cache/huggingface/hub/models--fnlp--Llama3_1-8B-Base-LXR-8x/snapshots/8dbc1d85edfced43081c03c38b05514dbab1368b/Llama3_1-8B-Base-L15R-8x/checkpoints/final_fixed.pth
   File size: 512.1 MB
ðŸ”§ Initializing SAE model...
ðŸ”§ Loading checkpoint weights (CPU)...
âœ… Loaded PyTorch file
ðŸ”§ Processing checkpoint weights...
   Loading W_E (W_E)... converted... âœ… (torch.Size([4096, 32768]))
   Loading b_E (b_E)... converted... âœ… (torch.Size([32768]))
   Loading W_D (W_D)... converted... âœ… (torch.Size([32768, 4096]))
   Loading b_D (b_D)... converted... 
  Layers:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 15/31 [00:46<00:53,  3.32s/it][Aâœ… (torch.Size([4096]))
ðŸ”§ Clearing checkpoint from memory...
ðŸ”§ Loading state dict to model...
ðŸ”§ Moving model to cuda:0...
âœ… OPTIMIZED SAE loaded successfully!
   Layer: 15
   Expected reconstruction error: ~47%
   Feature clamping: VERIFIED WORKING
   Memory optimizations: ENABLED
  Loading SAE Layer 16...
ðŸ”§ Loading config: /data/.cache/huggingface/hub/models--fnlp--Llama3_1-8B-Base-LXR-8x/snapshots/8dbc1d85edfced43081c03c38b05514dbab1368b/Llama3_1-8B-Base-L16R-8x/hyperparams.json
âœ… Loading WORKING SAE for Layer 16
   Config: /data/.cache/huggingface/hub/models--fnlp--Llama3_1-8B-Base-LXR-8x/snapshots/8dbc1d85edfced43081c03c38b05514dbab1368b/Llama3_1-8B-Base-L16R-8x/hyperparams.json
   Dataset norm: 12.125
   Norm factor: 5.278351
   Using ReLU instead of JumpReLU (threshold=0.0)
ðŸ”§ Loading checkpoint: /data/.cache/huggingface/hub/models--fnlp--Llama3_1-8B-Base-LXR-8x/snapshots/8dbc1d85edfced43081c03c38b05514dbab1368b/Llama3_1-8B-Base-L16R-8x/checkpoints/final.safetensors
   File size: 512.1 MB
ðŸ”§ Initializing SAE model...
ðŸ”§ Loading checkpoint weights (CPU)...
âœ… Loaded safetensors file
ðŸ”§ Processing checkpoint weights...
   Loading W_E (encoder.weight)... converted... âœ… (torch.Size([4096, 32768]))
   Loading b_E (encoder.bias)... converted... âœ… (torch.Size([32768]))
   Loading W_D (decoder.weight)... converted... âœ… (torch.Size([32768, 4096]))
   Loading b_D (decoder.bias)... converted... 
  Layers:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 16/31 [00:47<00:43,  2.89s/it][Aâœ… (torch.Size([4096]))
ðŸ”§ Clearing checkpoint from memory...
ðŸ”§ Loading state dict to model...
ðŸ”§ Moving model to cuda:0...
âœ… OPTIMIZED SAE loaded successfully!
   Layer: 16
   Expected reconstruction error: ~47%
   Feature clamping: VERIFIED WORKING
   Memory optimizations: ENABLED
  Loading SAE Layer 17...
ðŸ”§ Loading config: /data/.cache/huggingface/hub/models--fnlp--Llama3_1-8B-Base-LXR-8x/snapshots/8dbc1d85edfced43081c03c38b05514dbab1368b/Llama3_1-8B-Base-L17R-8x/hyperparams.json
âœ… Loading WORKING SAE for Layer 17
   Config: /data/.cache/huggingface/hub/models--fnlp--Llama3_1-8B-Base-LXR-8x/snapshots/8dbc1d85edfced43081c03c38b05514dbab1368b/Llama3_1-8B-Base-L17R-8x/hyperparams.json
   Dataset norm: 13.8125
   Norm factor: 4.633484
   Using ReLU instead of JumpReLU (threshold=0.0)
ðŸ”§ Loading checkpoint: /data/.cache/huggingface/hub/models--fnlp--Llama3_1-8B-Base-LXR-8x/snapshots/8dbc1d85edfced43081c03c38b05514dbab1368b/Llama3_1-8B-Base-L17R-8x/checkpoints/final.safetensors
   File size: 512.1 MB
ðŸ”§ Initializing SAE model...
ðŸ”§ Loading checkpoint weights (CPU)...
âœ… Loaded safetensors file
ðŸ”§ Processing checkpoint weights...
   Loading W_E (encoder.weight)... converted... âœ… (torch.Size([4096, 32768]))
   Loading b_E (encoder.bias)... converted... âœ… (torch.Size([32768]))
   Loading W_D (decoder.weight)... converted... âœ… (torch.Size([32768, 4096]))
   Loading b_D (decoder.bias)... converted... 
  Layers:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 17/31 [00:50<00:40,  2.88s/it][Aâœ… (torch.Size([4096]))
ðŸ”§ Clearing checkpoint from memory...
ðŸ”§ Loading state dict to model...
ðŸ”§ Moving model to cuda:0...
âœ… OPTIMIZED SAE loaded successfully!
   Layer: 17
   Expected reconstruction error: ~47%
   Feature clamping: VERIFIED WORKING
   Memory optimizations: ENABLED
  Loading SAE Layer 18...
ðŸ”§ Loading config: /data/.cache/huggingface/hub/models--fnlp--Llama3_1-8B-Base-LXR-8x/snapshots/8dbc1d85edfced43081c03c38b05514dbab1368b/Llama3_1-8B-Base-L18R-8x/hyperparams.json
âœ… Loading WORKING SAE for Layer 18
   Config: /data/.cache/huggingface/hub/models--fnlp--Llama3_1-8B-Base-LXR-8x/snapshots/8dbc1d85edfced43081c03c38b05514dbab1368b/Llama3_1-8B-Base-L18R-8x/hyperparams.json
   Dataset norm: 15.5
   Norm factor: 4.129032
   Using ReLU instead of JumpReLU (threshold=0.0)
ðŸ”§ Loading checkpoint: /data/.cache/huggingface/hub/models--fnlp--Llama3_1-8B-Base-LXR-8x/snapshots/8dbc1d85edfced43081c03c38b05514dbab1368b/Llama3_1-8B-Base-L18R-8x/checkpoints/final.safetensors
   File size: 512.1 MB
ðŸ”§ Initializing SAE model...
ðŸ”§ Loading checkpoint weights (CPU)...
âœ… Loaded safetensors file
ðŸ”§ Processing checkpoint weights...
   Loading W_E (encoder.weight)... converted... âœ… (torch.Size([4096, 32768]))
   Loading b_E (encoder.bias)... converted... âœ… (torch.Size([32768]))
   Loading W_D (decoder.weight)... converted... âœ… (torch.Size([32768, 4096]))
   Loading b_D (decoder.bias)... converted... 
  Layers:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 18/31 [00:53<00:38,  2.95s/it][Aâœ… (torch.Size([4096]))
ðŸ”§ Clearing checkpoint from memory...
ðŸ”§ Loading state dict to model...
ðŸ”§ Moving model to cuda:0...
âœ… OPTIMIZED SAE loaded successfully!
   Layer: 18
   Expected reconstruction error: ~47%
   Feature clamping: VERIFIED WORKING
   Memory optimizations: ENABLED
  Loading SAE Layer 19...
ðŸ”§ Loading config: /data/.cache/huggingface/hub/models--fnlp--Llama3_1-8B-Base-LXR-8x/snapshots/8dbc1d85edfced43081c03c38b05514dbab1368b/Llama3_1-8B-Base-L19R-8x/hyperparams.json
âœ… Loading WORKING SAE for Layer 19
   Config: /data/.cache/huggingface/hub/models--fnlp--Llama3_1-8B-Base-LXR-8x/snapshots/8dbc1d85edfced43081c03c38b05514dbab1368b/Llama3_1-8B-Base-L19R-8x/hyperparams.json
   Dataset norm: 17.125
   Norm factor: 3.737226
   Using ReLU instead of JumpReLU (threshold=0.0)
ðŸ”§ Loading checkpoint: /data/.cache/huggingface/hub/models--fnlp--Llama3_1-8B-Base-LXR-8x/snapshots/8dbc1d85edfced43081c03c38b05514dbab1368b/Llama3_1-8B-Base-L19R-8x/checkpoints/final.safetensors
   File size: 512.1 MB
ðŸ”§ Initializing SAE model...
ðŸ”§ Loading checkpoint weights (CPU)...
âœ… Loaded safetensors file
ðŸ”§ Processing checkpoint weights...
   Loading W_E (encoder.weight)... converted... âœ… (torch.Size([4096, 32768]))
   Loading b_E (encoder.bias)... converted... âœ… (torch.Size([32768]))
   Loading W_D (decoder.weight)... converted... âœ… (torch.Size([32768, 4096]))
   Loading b_D (decoder.bias)... converted... 
  Layers:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 19/31 [00:56<00:35,  2.94s/it][Aâœ… (torch.Size([4096]))
ðŸ”§ Clearing checkpoint from memory...
ðŸ”§ Loading state dict to model...
ðŸ”§ Moving model to cuda:0...
âœ… OPTIMIZED SAE loaded successfully!
   Layer: 19
   Expected reconstruction error: ~47%
   Feature clamping: VERIFIED WORKING
   Memory optimizations: ENABLED
  Loading SAE Layer 20...
ðŸ”§ Loading config: /data/.cache/huggingface/hub/models--fnlp--Llama3_1-8B-Base-LXR-8x/snapshots/8dbc1d85edfced43081c03c38b05514dbab1368b/Llama3_1-8B-Base-L20R-8x/hyperparams.json
âœ… Loading WORKING SAE for Layer 20
   Config: /data/.cache/huggingface/hub/models--fnlp--Llama3_1-8B-Base-LXR-8x/snapshots/8dbc1d85edfced43081c03c38b05514dbab1368b/Llama3_1-8B-Base-L20R-8x/hyperparams.json
   Dataset norm: 18.75
   Norm factor: 3.413333
   Using ReLU instead of JumpReLU (threshold=0.0)
ðŸ”§ Loading checkpoint: /data/.cache/huggingface/hub/models--fnlp--Llama3_1-8B-Base-LXR-8x/snapshots/8dbc1d85edfced43081c03c38b05514dbab1368b/Llama3_1-8B-Base-L20R-8x/checkpoints/final_fixed.pth
   File size: 512.1 MB
ðŸ”§ Initializing SAE model...
ðŸ”§ Loading checkpoint weights (CPU)...
âœ… Loaded PyTorch file
ðŸ”§ Processing checkpoint weights...
   Loading W_E (W_E)... converted... âœ… (torch.Size([4096, 32768]))
   Loading b_E (b_E)... converted... âœ… (torch.Size([32768]))
   Loading W_D (W_D)... converted... âœ… (torch.Size([32768, 4096]))
   Loading b_D (b_D)... converted... 
  Layers:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 20/31 [00:59<00:32,  2.96s/it][Aâœ… (torch.Size([4096]))
ðŸ”§ Clearing checkpoint from memory...
ðŸ”§ Loading state dict to model...
ðŸ”§ Moving model to cuda:0...
âœ… OPTIMIZED SAE loaded successfully!
   Layer: 20
   Expected reconstruction error: ~47%
   Feature clamping: VERIFIED WORKING
   Memory optimizations: ENABLED
  Loading SAE Layer 21...
ðŸ”§ Loading config: /data/.cache/huggingface/hub/models--fnlp--Llama3_1-8B-Base-LXR-8x/snapshots/8dbc1d85edfced43081c03c38b05514dbab1368b/Llama3_1-8B-Base-L21R-8x/hyperparams.json
âœ… Loading WORKING SAE for Layer 21
   Config: /data/.cache/huggingface/hub/models--fnlp--Llama3_1-8B-Base-LXR-8x/snapshots/8dbc1d85edfced43081c03c38b05514dbab1368b/Llama3_1-8B-Base-L21R-8x/hyperparams.json
   Dataset norm: 21.5
   Norm factor: 2.976744
   Using ReLU instead of JumpReLU (threshold=0.0)
ðŸ”§ Loading checkpoint: /data/.cache/huggingface/hub/models--fnlp--Llama3_1-8B-Base-LXR-8x/snapshots/8dbc1d85edfced43081c03c38b05514dbab1368b/Llama3_1-8B-Base-L21R-8x/checkpoints/final.safetensors
   File size: 512.1 MB
ðŸ”§ Initializing SAE model...
ðŸ”§ Loading checkpoint weights (CPU)...
âœ… Loaded safetensors file
ðŸ”§ Processing checkpoint weights...
   Loading W_E (encoder.weight)... converted... âœ… (torch.Size([4096, 32768]))
   Loading b_E (encoder.bias)... converted... âœ… (torch.Size([32768]))
   Loading W_D (decoder.weight)... converted... âœ… (torch.Size([32768, 4096]))
   Loading b_D (decoder.bias)... converted... 
  Layers:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 21/31 [01:02<00:29,  2.98s/it][Aâœ… (torch.Size([4096]))
ðŸ”§ Clearing checkpoint from memory...
ðŸ”§ Loading state dict to model...
ðŸ”§ Moving model to cuda:0...
âœ… OPTIMIZED SAE loaded successfully!
   Layer: 21
   Expected reconstruction error: ~47%
   Feature clamping: VERIFIED WORKING
   Memory optimizations: ENABLED
  Loading SAE Layer 22...
ðŸ”§ Loading config: /data/.cache/huggingface/hub/models--fnlp--Llama3_1-8B-Base-LXR-8x/snapshots/8dbc1d85edfced43081c03c38b05514dbab1368b/Llama3_1-8B-Base-L22R-8x/hyperparams.json
âœ… Loading WORKING SAE for Layer 22
   Config: /data/.cache/huggingface/hub/models--fnlp--Llama3_1-8B-Base-LXR-8x/snapshots/8dbc1d85edfced43081c03c38b05514dbab1368b/Llama3_1-8B-Base-L22R-8x/hyperparams.json
   Dataset norm: 23.875
   Norm factor: 2.680628
   Using ReLU instead of JumpReLU (threshold=0.0)
ðŸ”§ Loading checkpoint: /data/.cache/huggingface/hub/models--fnlp--Llama3_1-8B-Base-LXR-8x/snapshots/8dbc1d85edfced43081c03c38b05514dbab1368b/Llama3_1-8B-Base-L22R-8x/checkpoints/final.safetensors
   File size: 512.1 MB
ðŸ”§ Initializing SAE model...
ðŸ”§ Loading checkpoint weights (CPU)...
âœ… Loaded safetensors file
ðŸ”§ Processing checkpoint weights...
   Loading W_E (encoder.weight)... converted... âœ… (torch.Size([4096, 32768]))
   Loading b_E (encoder.bias)... converted... âœ… (torch.Size([32768]))
   Loading W_D (decoder.weight)... converted... âœ… (torch.Size([32768, 4096]))
   Loading b_D (decoder.bias)... converted... 
  Layers:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 22/31 [01:05<00:26,  2.91s/it][Aâœ… (torch.Size([4096]))
ðŸ”§ Clearing checkpoint from memory...
ðŸ”§ Loading state dict to model...
ðŸ”§ Moving model to cuda:0...
âœ… OPTIMIZED SAE loaded successfully!
   Layer: 22
   Expected reconstruction error: ~47%
   Feature clamping: VERIFIED WORKING
   Memory optimizations: ENABLED
  Loading SAE Layer 23...
ðŸ”§ Loading config: /data/.cache/huggingface/hub/models--fnlp--Llama3_1-8B-Base-LXR-8x/snapshots/8dbc1d85edfced43081c03c38b05514dbab1368b/Llama3_1-8B-Base-L23R-8x/hyperparams.json
âœ… Loading WORKING SAE for Layer 23
   Config: /data/.cache/huggingface/hub/models--fnlp--Llama3_1-8B-Base-LXR-8x/snapshots/8dbc1d85edfced43081c03c38b05514dbab1368b/Llama3_1-8B-Base-L23R-8x/hyperparams.json
   Dataset norm: 26.5
   Norm factor: 2.415094
   Using ReLU instead of JumpReLU (threshold=0.0)
ðŸ”§ Loading checkpoint: /data/.cache/huggingface/hub/models--fnlp--Llama3_1-8B-Base-LXR-8x/snapshots/8dbc1d85edfced43081c03c38b05514dbab1368b/Llama3_1-8B-Base-L23R-8x/checkpoints/final.safetensors
   File size: 512.1 MB
ðŸ”§ Initializing SAE model...
ðŸ”§ Loading checkpoint weights (CPU)...
âœ… Loaded safetensors file
ðŸ”§ Processing checkpoint weights...
   Loading W_E (encoder.weight)... converted... âœ… (torch.Size([4096, 32768]))
   Loading b_E (encoder.bias)... converted... âœ… (torch.Size([32768]))
   Loading W_D (decoder.weight)... converted... âœ… (torch.Size([32768, 4096]))
   Loading b_D (decoder.bias)... converted... 
  Layers:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 23/31 [01:08<00:23,  2.97s/it][Aâœ… (torch.Size([4096]))
ðŸ”§ Clearing checkpoint from memory...
ðŸ”§ Loading state dict to model...
ðŸ”§ Moving model to cuda:0...
âœ… OPTIMIZED SAE loaded successfully!
   Layer: 23
   Expected reconstruction error: ~47%
   Feature clamping: VERIFIED WORKING
   Memory optimizations: ENABLED
  Loading SAE Layer 24...
ðŸ”§ Loading config: /data/.cache/huggingface/hub/models--fnlp--Llama3_1-8B-Base-LXR-8x/snapshots/8dbc1d85edfced43081c03c38b05514dbab1368b/Llama3_1-8B-Base-L24R-8x/hyperparams.json
âœ… Loading WORKING SAE for Layer 24
   Config: /data/.cache/huggingface/hub/models--fnlp--Llama3_1-8B-Base-LXR-8x/snapshots/8dbc1d85edfced43081c03c38b05514dbab1368b/Llama3_1-8B-Base-L24R-8x/hyperparams.json
   Dataset norm: 29.25
   Norm factor: 2.188034
   Using ReLU instead of JumpReLU (threshold=0.0)
ðŸ”§ Loading checkpoint: /data/.cache/huggingface/hub/models--fnlp--Llama3_1-8B-Base-LXR-8x/snapshots/8dbc1d85edfced43081c03c38b05514dbab1368b/Llama3_1-8B-Base-L24R-8x/checkpoints/final.safetensors
   File size: 512.1 MB
ðŸ”§ Initializing SAE model...
ðŸ”§ Loading checkpoint weights (CPU)...
âœ… Loaded safetensors file
ðŸ”§ Processing checkpoint weights...
   Loading W_E (encoder.weight)... converted... âœ… (torch.Size([4096, 32768]))
   Loading b_E (encoder.bias)... converted... âœ… (torch.Size([32768]))
   Loading W_D (decoder.weight)... converted... âœ… (torch.Size([32768, 4096]))
   Loading b_D (decoder.bias)... converted... 
  Layers:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 24/31 [01:11<00:20,  2.92s/it][Aâœ… (torch.Size([4096]))
ðŸ”§ Clearing checkpoint from memory...
ðŸ”§ Loading state dict to model...
ðŸ”§ Moving model to cuda:0...
âœ… OPTIMIZED SAE loaded successfully!
   Layer: 24
   Expected reconstruction error: ~47%
   Feature clamping: VERIFIED WORKING
   Memory optimizations: ENABLED
  Loading SAE Layer 25...
ðŸ”§ Loading config: /data/.cache/huggingface/hub/models--fnlp--Llama3_1-8B-Base-LXR-8x/snapshots/8dbc1d85edfced43081c03c38b05514dbab1368b/Llama3_1-8B-Base-L25R-8x/hyperparams.json
âœ… Loading WORKING SAE for Layer 25
   Config: /data/.cache/huggingface/hub/models--fnlp--Llama3_1-8B-Base-LXR-8x/snapshots/8dbc1d85edfced43081c03c38b05514dbab1368b/Llama3_1-8B-Base-L25R-8x/hyperparams.json
   Dataset norm: 31.625
   Norm factor: 2.023715
   Using ReLU instead of JumpReLU (threshold=0.0)
ðŸ”§ Loading checkpoint: /data/.cache/huggingface/hub/models--fnlp--Llama3_1-8B-Base-LXR-8x/snapshots/8dbc1d85edfced43081c03c38b05514dbab1368b/Llama3_1-8B-Base-L25R-8x/checkpoints/final.safetensors
   File size: 512.1 MB
ðŸ”§ Initializing SAE model...
ðŸ”§ Loading checkpoint weights (CPU)...
âœ… Loaded safetensors file
ðŸ”§ Processing checkpoint weights...
   Loading W_E (encoder.weight)... converted... âœ… (torch.Size([4096, 32768]))
   Loading b_E (encoder.bias)... converted... âœ… (torch.Size([32768]))
   Loading W_D (decoder.weight)... converted... âœ… (torch.Size([32768, 4096]))
   Loading b_D (decoder.bias)... converted... 
  Layers:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 25/31 [01:14<00:17,  2.90s/it][Aâœ… (torch.Size([4096]))
ðŸ”§ Clearing checkpoint from memory...
ðŸ”§ Loading state dict to model...
ðŸ”§ Moving model to cuda:0...
âœ… OPTIMIZED SAE loaded successfully!
   Layer: 25
   Expected reconstruction error: ~47%
   Feature clamping: VERIFIED WORKING
   Memory optimizations: ENABLED
  Loading SAE Layer 26...
ðŸ”§ Loading config: /data/.cache/huggingface/hub/models--fnlp--Llama3_1-8B-Base-LXR-8x/snapshots/8dbc1d85edfced43081c03c38b05514dbab1368b/Llama3_1-8B-Base-L26R-8x/hyperparams.json
âœ… Loading WORKING SAE for Layer 26
   Config: /data/.cache/huggingface/hub/models--fnlp--Llama3_1-8B-Base-LXR-8x/snapshots/8dbc1d85edfced43081c03c38b05514dbab1368b/Llama3_1-8B-Base-L26R-8x/hyperparams.json
   Dataset norm: 35.25
   Norm factor: 1.815603
   Using ReLU instead of JumpReLU (threshold=0.0)
ðŸ”§ Loading checkpoint: /data/.cache/huggingface/hub/models--fnlp--Llama3_1-8B-Base-LXR-8x/snapshots/8dbc1d85edfced43081c03c38b05514dbab1368b/Llama3_1-8B-Base-L26R-8x/checkpoints/final.safetensors
   File size: 512.1 MB
ðŸ”§ Initializing SAE model...
ðŸ”§ Loading checkpoint weights (CPU)...
âœ… Loaded safetensors file
ðŸ”§ Processing checkpoint weights...
   Loading W_E (encoder.weight)... converted... âœ… (torch.Size([4096, 32768]))
   Loading b_E (encoder.bias)... converted... âœ… (torch.Size([32768]))
   Loading W_D (decoder.weight)... converted... âœ… (torch.Size([32768, 4096]))
   Loading b_D (decoder.bias)... converted... 
  Layers:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 26/31 [01:17<00:14,  2.96s/it][Aâœ… (torch.Size([4096]))
ðŸ”§ Clearing checkpoint from memory...
ðŸ”§ Loading state dict to model...
ðŸ”§ Moving model to cuda:0...
âœ… OPTIMIZED SAE loaded successfully!
   Layer: 26
   Expected reconstruction error: ~47%
   Feature clamping: VERIFIED WORKING
   Memory optimizations: ENABLED
  Loading SAE Layer 27...
ðŸ”§ Loading config: /data/.cache/huggingface/hub/models--fnlp--Llama3_1-8B-Base-LXR-8x/snapshots/8dbc1d85edfced43081c03c38b05514dbab1368b/Llama3_1-8B-Base-L27R-8x/hyperparams.json
âœ… Loading WORKING SAE for Layer 27
   Config: /data/.cache/huggingface/hub/models--fnlp--Llama3_1-8B-Base-LXR-8x/snapshots/8dbc1d85edfced43081c03c38b05514dbab1368b/Llama3_1-8B-Base-L27R-8x/hyperparams.json
   Dataset norm: 38.25
   Norm factor: 1.673203
   Using ReLU instead of JumpReLU (threshold=0.0)
ðŸ”§ Loading checkpoint: /data/.cache/huggingface/hub/models--fnlp--Llama3_1-8B-Base-LXR-8x/snapshots/8dbc1d85edfced43081c03c38b05514dbab1368b/Llama3_1-8B-Base-L27R-8x/checkpoints/final.safetensors
   File size: 512.1 MB
ðŸ”§ Initializing SAE model...
ðŸ”§ Loading checkpoint weights (CPU)...
âœ… Loaded safetensors file
ðŸ”§ Processing checkpoint weights...
   Loading W_E (encoder.weight)... converted... âœ… (torch.Size([4096, 32768]))
   Loading b_E (encoder.bias)... converted... âœ… (torch.Size([32768]))
   Loading W_D (decoder.weight)... converted... âœ… (torch.Size([32768, 4096]))
   Loading b_D (decoder.bias)... converted... 
  Layers:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 27/31 [01:20<00:11,  2.97s/it][Aâœ… (torch.Size([4096]))
ðŸ”§ Clearing checkpoint from memory...
ðŸ”§ Loading state dict to model...
ðŸ”§ Moving model to cuda:0...
âœ… OPTIMIZED SAE loaded successfully!
   Layer: 27
   Expected reconstruction error: ~47%
   Feature clamping: VERIFIED WORKING
   Memory optimizations: ENABLED
  Loading SAE Layer 28...
ðŸ”§ Loading config: /data/.cache/huggingface/hub/models--fnlp--Llama3_1-8B-Base-LXR-8x/snapshots/8dbc1d85edfced43081c03c38b05514dbab1368b/Llama3_1-8B-Base-L28R-8x/hyperparams.json
âœ… Loading WORKING SAE for Layer 28
   Config: /data/.cache/huggingface/hub/models--fnlp--Llama3_1-8B-Base-LXR-8x/snapshots/8dbc1d85edfced43081c03c38b05514dbab1368b/Llama3_1-8B-Base-L28R-8x/hyperparams.json
   Dataset norm: 42.5
   Norm factor: 1.505882
   Using ReLU instead of JumpReLU (threshold=0.0)
ðŸ”§ Loading checkpoint: /data/.cache/huggingface/hub/models--fnlp--Llama3_1-8B-Base-LXR-8x/snapshots/8dbc1d85edfced43081c03c38b05514dbab1368b/Llama3_1-8B-Base-L28R-8x/checkpoints/final.safetensors
   File size: 512.1 MB
ðŸ”§ Initializing SAE model...
ðŸ”§ Loading checkpoint weights (CPU)...
âœ… Loaded safetensors file
ðŸ”§ Processing checkpoint weights...
   Loading W_E (encoder.weight)... converted... âœ… (torch.Size([4096, 32768]))
   Loading b_E (encoder.bias)... converted... âœ… (torch.Size([32768]))
   Loading W_D (decoder.weight)... converted... âœ… (torch.Size([32768, 4096]))
   Loading b_D (decoder.bias)... converted... 
  Layers:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 28/31 [01:23<00:08,  2.95s/it][Aâœ… (torch.Size([4096]))
ðŸ”§ Clearing checkpoint from memory...
ðŸ”§ Loading state dict to model...
ðŸ”§ Moving model to cuda:0...
âœ… OPTIMIZED SAE loaded successfully!
   Layer: 28
   Expected reconstruction error: ~47%
   Feature clamping: VERIFIED WORKING
   Memory optimizations: ENABLED
  Loading SAE Layer 29...
ðŸ”§ Loading config: /data/.cache/huggingface/hub/models--fnlp--Llama3_1-8B-Base-LXR-8x/snapshots/8dbc1d85edfced43081c03c38b05514dbab1368b/Llama3_1-8B-Base-L29R-8x/hyperparams.json
âœ… Loading WORKING SAE for Layer 29
   Config: /data/.cache/huggingface/hub/models--fnlp--Llama3_1-8B-Base-LXR-8x/snapshots/8dbc1d85edfced43081c03c38b05514dbab1368b/Llama3_1-8B-Base-L29R-8x/hyperparams.json
   Dataset norm: 46.5
   Norm factor: 1.376344
   Using ReLU instead of JumpReLU (threshold=0.0)
ðŸ”§ Loading checkpoint: /data/.cache/huggingface/hub/models--fnlp--Llama3_1-8B-Base-LXR-8x/snapshots/8dbc1d85edfced43081c03c38b05514dbab1368b/Llama3_1-8B-Base-L29R-8x/checkpoints/final.safetensors
   File size: 512.1 MB
ðŸ”§ Initializing SAE model...
ðŸ”§ Loading checkpoint weights (CPU)...
âœ… Loaded safetensors file
ðŸ”§ Processing checkpoint weights...
   Loading W_E (encoder.weight)... converted... âœ… (torch.Size([4096, 32768]))
   Loading b_E (encoder.bias)... converted... âœ… (torch.Size([32768]))
   Loading W_D (decoder.weight)... converted... âœ… (torch.Size([32768, 4096]))
   Loading b_D (decoder.bias)... converted... 
  Layers:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 29/31 [01:26<00:05,  2.92s/it][Aâœ… (torch.Size([4096]))
ðŸ”§ Clearing checkpoint from memory...
ðŸ”§ Loading state dict to model...
ðŸ”§ Moving model to cuda:0...
âœ… OPTIMIZED SAE loaded successfully!
   Layer: 29
   Expected reconstruction error: ~47%
   Feature clamping: VERIFIED WORKING
   Memory optimizations: ENABLED
  Loading SAE Layer 30...
ðŸ”§ Loading config: /data/.cache/huggingface/hub/models--fnlp--Llama3_1-8B-Base-LXR-8x/snapshots/8dbc1d85edfced43081c03c38b05514dbab1368b/Llama3_1-8B-Base-L30R-8x/hyperparams.json
âœ… Loading WORKING SAE for Layer 30
   Config: /data/.cache/huggingface/hub/models--fnlp--Llama3_1-8B-Base-LXR-8x/snapshots/8dbc1d85edfced43081c03c38b05514dbab1368b/Llama3_1-8B-Base-L30R-8x/hyperparams.json
   Dataset norm: 53.25
   Norm factor: 1.201878
   Using ReLU instead of JumpReLU (threshold=0.0)
ðŸ”§ Loading checkpoint: /data/.cache/huggingface/hub/models--fnlp--Llama3_1-8B-Base-LXR-8x/snapshots/8dbc1d85edfced43081c03c38b05514dbab1368b/Llama3_1-8B-Base-L30R-8x/checkpoints/final_fixed.pth
   File size: 512.1 MB
ðŸ”§ Initializing SAE model...
ðŸ”§ Loading checkpoint weights (CPU)...
âœ… Loaded PyTorch file
ðŸ”§ Processing checkpoint weights...
   Loading W_E (W_E)... converted... âœ… (torch.Size([4096, 32768]))
   Loading b_E (b_E)... converted... âœ… (torch.Size([32768]))
   Loading W_D (W_D)... converted... âœ… (torch.Size([32768, 4096]))
   Loading b_D (b_D)... converted... 
  Layers:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 30/31 [01:29<00:02,  2.96s/it][Aâœ… (torch.Size([4096]))
ðŸ”§ Clearing checkpoint from memory...
ðŸ”§ Loading state dict to model...
ðŸ”§ Moving model to cuda:0...
âœ… OPTIMIZED SAE loaded successfully!
   Layer: 30
   Expected reconstruction error: ~47%
   Feature clamping: VERIFIED WORKING
   Memory optimizations: ENABLED
  Loading SAE Layer 31...
ðŸ”§ Loading config: /data/.cache/huggingface/hub/models--fnlp--Llama3_1-8B-Base-LXR-8x/snapshots/8dbc1d85edfced43081c03c38b05514dbab1368b/Llama3_1-8B-Base-L31R-8x/hyperparams.json
âœ… Loading WORKING SAE for Layer 31
   Config: /data/.cache/huggingface/hub/models--fnlp--Llama3_1-8B-Base-LXR-8x/snapshots/8dbc1d85edfced43081c03c38b05514dbab1368b/Llama3_1-8B-Base-L31R-8x/hyperparams.json
   Dataset norm: 74.5
   Norm factor: 0.859060
   Using ReLU instead of JumpReLU (threshold=0.0)
ðŸ”§ Loading checkpoint: /data/.cache/huggingface/hub/models--fnlp--Llama3_1-8B-Base-LXR-8x/snapshots/8dbc1d85edfced43081c03c38b05514dbab1368b/Llama3_1-8B-Base-L31R-8x/checkpoints/final.safetensors
   File size: 512.1 MB
ðŸ”§ Initializing SAE model...
ðŸ”§ Loading checkpoint weights (CPU)...
âœ… Loaded safetensors file
ðŸ”§ Processing checkpoint weights...
   Loading W_E (encoder.weight)... converted... âœ… (torch.Size([4096, 32768]))
   Loading b_E (encoder.bias)... converted... âœ… (torch.Size([32768]))
   Loading W_D (decoder.weight)... converted... âœ… (torch.Size([32768, 4096]))
   Loading b_D (decoder.bias)... converted... 
  Layers: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 31/31 [01:32<00:00,  2.99s/it][A
                                                         [AScenarios:  10%|â–ˆ         | 1/10 [01:40<15:07, 100.84s/it]âœ… (torch.Size([4096]))
ðŸ”§ Clearing checkpoint from memory...
ðŸ”§ Loading state dict to model...
ðŸ”§ Moving model to cuda:0...
âœ… OPTIMIZED SAE loaded successfully!
   Layer: 31
   Expected reconstruction error: ~47%
   Feature clamping: VERIFIED WORKING
   Memory optimizations: ENABLED

ðŸ” Desperate_10
  Tokens: 135

  Layers:   0%|          | 0/31 [00:00<?, ?it/s][A
  Layers:   3%|â–Ž         | 1/31 [00:02<01:03,  2.13s/it][A
  Layers:   6%|â–‹         | 2/31 [00:02<00:31,  1.08s/it][A
  Layers:  10%|â–‰         | 3/31 [00:02<00:20,  1.35it/s][A
  Layers:  13%|â–ˆâ–Ž        | 4/31 [00:03<00:15,  1.72it/s][A
  Layers:  16%|â–ˆâ–Œ        | 5/31 [00:03<00:12,  2.00it/s][A
  Layers:  19%|â–ˆâ–‰        | 6/31 [00:03<00:11,  2.20it/s][A
  Layers:  23%|â–ˆâ–ˆâ–Ž       | 7/31 [00:04<00:10,  2.38it/s][A
  Layers:  26%|â–ˆâ–ˆâ–Œ       | 8/31 [00:04<00:09,  2.51it/s][A
  Layers:  29%|â–ˆâ–ˆâ–‰       | 9/31 [00:05<00:12,  1.75it/s][A
  Layers:  32%|â–ˆâ–ˆâ–ˆâ–      | 10/31 [00:05<00:10,  2.00it/s][A
  Layers:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 11/31 [00:06<00:09,  2.15it/s][A
  Layers:  39%|â–ˆâ–ˆâ–ˆâ–Š      | 12/31 [00:06<00:08,  2.37it/s][A
  Layers:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 13/31 [00:06<00:07,  2.44it/s][A
  Layers:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 14/31 [00:07<00:06,  2.45it/s][A
  Layers:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 15/31 [00:07<00:06,  2.57it/s][A
  Layers:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 16/31 [00:08<00:05,  2.65it/s][A
  Layers:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 17/31 [00:08<00:05,  2.77it/s][A
  Layers:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 18/31 [00:08<00:04,  2.82it/s][A
  Layers:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 19/31 [00:09<00:04,  2.90it/s][A
  Layers:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 20/31 [00:09<00:03,  2.93it/s][A
  Layers:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 21/31 [00:09<00:03,  2.94it/s][A
  Layers:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 22/31 [00:10<00:03,  2.99it/s][A
  Layers:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 23/31 [00:10<00:02,  2.95it/s][A
  Layers:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 24/31 [00:10<00:02,  2.93it/s][A
  Layers:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 25/31 [00:11<00:02,  2.87it/s][A
  Layers:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 26/31 [00:11<00:01,  2.82it/s][A
  Layers:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 27/31 [00:11<00:01,  2.84it/s][A
  Layers:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 28/31 [00:12<00:01,  2.86it/s][A
  Layers:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 29/31 [00:12<00:00,  2.88it/s][A
  Layers:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 30/31 [00:12<00:00,  2.79it/s][A
  Layers: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 31/31 [00:13<00:00,  2.35it/s][A
                                                         [AScenarios:  20%|â–ˆâ–ˆ        | 2/10 [01:54<06:35, 49.48s/it] 
ðŸ” Safe_130_one_win
  Tokens: 96

  Layers:   0%|          | 0/31 [00:00<?, ?it/s][A
  Layers:   3%|â–Ž         | 1/31 [00:00<00:07,  3.83it/s][A
  Layers:   6%|â–‹         | 2/31 [00:00<00:07,  4.02it/s][A
  Layers:  10%|â–‰         | 3/31 [00:00<00:06,  4.18it/s][A
  Layers:  13%|â–ˆâ–Ž        | 4/31 [00:00<00:06,  4.27it/s][A
  Layers:  16%|â–ˆâ–Œ        | 5/31 [00:01<00:06,  4.25it/s][A
  Layers:  19%|â–ˆâ–‰        | 6/31 [00:01<00:05,  4.35it/s][A
  Layers:  23%|â–ˆâ–ˆâ–Ž       | 7/31 [00:01<00:05,  4.27it/s][A
  Layers:  26%|â–ˆâ–ˆâ–Œ       | 8/31 [00:01<00:05,  4.37it/s][A
  Layers:  29%|â–ˆâ–ˆâ–‰       | 9/31 [00:02<00:05,  4.38it/s][A
  Layers:  32%|â–ˆâ–ˆâ–ˆâ–      | 10/31 [00:02<00:04,  4.31it/s][A
  Layers:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 11/31 [00:02<00:04,  4.16it/s][A
  Layers:  39%|â–ˆâ–ˆâ–ˆâ–Š      | 12/31 [00:02<00:04,  4.32it/s][A
  Layers:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 13/31 [00:03<00:04,  4.07it/s][A
  Layers:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 14/31 [00:03<00:04,  4.11it/s][A
  Layers:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 15/31 [00:03<00:03,  4.14it/s][A
  Layers:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 16/31 [00:03<00:03,  4.13it/s][A
  Layers:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 17/31 [00:04<00:03,  4.12it/s][A
  Layers:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 18/31 [00:04<00:03,  4.17it/s][A
  Layers:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 19/31 [00:04<00:02,  4.11it/s][A
  Layers:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 20/31 [00:04<00:02,  4.23it/s][A
  Layers:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 21/31 [00:04<00:02,  4.29it/s][A
  Layers:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 22/31 [00:05<00:02,  4.34it/s][A
  Layers:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 23/31 [00:05<00:01,  4.35it/s][A
  Layers:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 24/31 [00:05<00:01,  4.31it/s][A
  Layers:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 25/31 [00:05<00:01,  4.45it/s][A
  Layers:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 26/31 [00:06<00:01,  4.42it/s][A
  Layers:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 27/31 [00:06<00:00,  4.41it/s][A
  Layers:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 28/31 [00:06<00:00,  4.33it/s][A
  Layers:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 29/31 [00:06<00:00,  4.28it/s][A
  Layers:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 30/31 [00:07<00:00,  4.08it/s][A
  Layers: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 31/31 [00:07<00:00,  3.42it/s][A
                                                         [AScenarios:  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [02:02<03:34, 30.71s/it]
ðŸ” Risky_40
  Tokens: 124

  Layers:   0%|          | 0/31 [00:00<?, ?it/s][A
  Layers:   3%|â–Ž         | 1/31 [00:00<00:24,  1.22it/s][A
  Layers:   6%|â–‹         | 2/31 [00:01<00:14,  1.97it/s][A
  Layers:  10%|â–‰         | 3/31 [00:01<00:11,  2.34it/s][A
  Layers:  13%|â–ˆâ–Ž        | 4/31 [00:01<00:10,  2.62it/s][A
  Layers:  16%|â–ˆâ–Œ        | 5/31 [00:02<00:09,  2.76it/s][A
  Layers:  19%|â–ˆâ–‰        | 6/31 [00:02<00:08,  2.86it/s][A
  Layers:  23%|â–ˆâ–ˆâ–Ž       | 7/31 [00:02<00:08,  3.00it/s][A
  Layers:  26%|â–ˆâ–ˆâ–Œ       | 8/31 [00:03<00:07,  3.02it/s][A
  Layers:  29%|â–ˆâ–ˆâ–‰       | 9/31 [00:03<00:07,  3.02it/s][A
  Layers:  32%|â–ˆâ–ˆâ–ˆâ–      | 10/31 [00:03<00:06,  3.02it/s][A
  Layers:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 11/31 [00:04<00:06,  2.98it/s][A
  Layers:  39%|â–ˆâ–ˆâ–ˆâ–Š      | 12/31 [00:04<00:06,  3.00it/s][A
  Layers:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 13/31 [00:04<00:06,  2.94it/s][A
  Layers:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 14/31 [00:05<00:05,  2.91it/s][A
  Layers:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 15/31 [00:05<00:05,  3.04it/s][A
  Layers:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 16/31 [00:05<00:04,  3.05it/s][A
  Layers:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 17/31 [00:06<00:04,  3.08it/s][A
  Layers:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 18/31 [00:06<00:04,  3.07it/s][A
  Layers:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 19/31 [00:06<00:03,  3.10it/s][A
  Layers:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 20/31 [00:06<00:03,  3.05it/s][A
  Layers:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 21/31 [00:07<00:03,  3.07it/s][A
  Layers:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 22/31 [00:07<00:02,  3.17it/s][A
  Layers:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 23/31 [00:07<00:02,  3.15it/s][A
  Layers:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 24/31 [00:08<00:02,  3.09it/s][A
  Layers:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 25/31 [00:08<00:01,  3.19it/s][A
  Layers:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 26/31 [00:08<00:01,  3.25it/s][A
  Layers:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 27/31 [00:09<00:01,  3.25it/s][A
  Layers:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 28/31 [00:09<00:00,  3.29it/s][A
  Layers:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 29/31 [00:09<00:00,  3.30it/s][A
  Layers:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 30/31 [00:10<00:00,  3.12it/s][A
  Layers: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 31/31 [00:10<00:00,  2.61it/s][A
                                                         [AScenarios:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [02:15<02:21, 23.62s/it]
ðŸ” Safe_140_near_goal
  Tokens: 110

  Layers:   0%|          | 0/31 [00:00<?, ?it/s][A
  Layers:   3%|â–Ž         | 1/31 [00:02<01:04,  2.14s/it][A
  Layers:   6%|â–‹         | 2/31 [00:02<00:30,  1.04s/it][A
  Layers:  10%|â–‰         | 3/31 [00:02<00:19,  1.45it/s][A
  Layers:  13%|â–ˆâ–Ž        | 4/31 [00:02<00:14,  1.89it/s][A
  Layers:  16%|â–ˆâ–Œ        | 5/31 [00:03<00:11,  2.30it/s][A
  Layers:  19%|â–ˆâ–‰        | 6/31 [00:03<00:09,  2.51it/s][A
  Layers:  23%|â–ˆâ–ˆâ–Ž       | 7/31 [00:03<00:08,  2.73it/s][A
  Layers:  26%|â–ˆâ–ˆâ–Œ       | 8/31 [00:04<00:08,  2.87it/s][A
  Layers:  29%|â–ˆâ–ˆâ–‰       | 9/31 [00:04<00:07,  3.01it/s][A
  Layers:  32%|â–ˆâ–ˆâ–ˆâ–      | 10/31 [00:04<00:06,  3.14it/s][A
  Layers:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 11/31 [00:05<00:06,  3.16it/s][A
  Layers:  39%|â–ˆâ–ˆâ–ˆâ–Š      | 12/31 [00:05<00:05,  3.34it/s][A
  Layers:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 13/31 [00:05<00:05,  3.27it/s][A
  Layers:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 14/31 [00:05<00:05,  3.33it/s][A
  Layers:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 15/31 [00:06<00:04,  3.46it/s][A
  Layers:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 16/31 [00:06<00:04,  3.48it/s][A
  Layers:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 17/31 [00:06<00:03,  3.59it/s][A
  Layers:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 18/31 [00:07<00:03,  3.55it/s][A
  Layers:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 19/31 [00:07<00:03,  3.64it/s][A
  Layers:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 20/31 [00:07<00:03,  3.59it/s][A
  Layers:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 21/31 [00:07<00:02,  3.69it/s][A
  Layers:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 22/31 [00:08<00:02,  3.64it/s][A
  Layers:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 23/31 [00:08<00:02,  3.60it/s][A
  Layers:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 24/31 [00:08<00:01,  3.55it/s][A
  Layers:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 25/31 [00:08<00:01,  3.56it/s][A
  Layers:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 26/31 [00:09<00:01,  3.64it/s][A
  Layers:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 27/31 [00:09<00:01,  3.63it/s][A
  Layers:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 28/31 [00:09<00:00,  3.61it/s][A
  Layers:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 29/31 [00:10<00:00,  3.58it/s][A
  Layers:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 30/31 [00:10<00:00,  3.44it/s][A
  Layers: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 31/31 [00:10<00:00,  2.85it/s][A
                                                         [AScenarios:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [02:26<01:35, 19.05s/it]
ðŸ” Initial_100_first_round
  Tokens: 79

  Layers:   0%|          | 0/31 [00:00<?, ?it/s][A
  Layers:   3%|â–Ž         | 1/31 [00:00<00:23,  1.25it/s][A
  Layers:   6%|â–‹         | 2/31 [00:00<00:12,  2.29it/s][A
  Layers:  10%|â–‰         | 3/31 [00:01<00:09,  3.10it/s][A
  Layers:  13%|â–ˆâ–Ž        | 4/31 [00:01<00:07,  3.65it/s][A
  Layers:  16%|â–ˆâ–Œ        | 5/31 [00:01<00:06,  4.15it/s][A
  Layers:  19%|â–ˆâ–‰        | 6/31 [00:01<00:05,  4.39it/s][A
  Layers:  23%|â–ˆâ–ˆâ–Ž       | 7/31 [00:01<00:05,  4.77it/s][A
  Layers:  26%|â–ˆâ–ˆâ–Œ       | 8/31 [00:02<00:04,  4.82it/s][A
  Layers:  29%|â–ˆâ–ˆâ–‰       | 9/31 [00:02<00:04,  4.80it/s][A
  Layers:  32%|â–ˆâ–ˆâ–ˆâ–      | 10/31 [00:02<00:04,  4.83it/s][A
  Layers:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 11/31 [00:02<00:04,  4.92it/s][A
  Layers:  39%|â–ˆâ–ˆâ–ˆâ–Š      | 12/31 [00:02<00:03,  5.12it/s][A
  Layers:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 13/31 [00:03<00:03,  4.92it/s][A
  Layers:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 14/31 [00:03<00:03,  5.01it/s][A
  Layers:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 15/31 [00:03<00:03,  5.14it/s][A
  Layers:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 16/31 [00:03<00:02,  5.05it/s][A
  Layers:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 17/31 [00:03<00:02,  5.22it/s][A
  Layers:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 18/31 [00:04<00:02,  5.13it/s][A
  Layers:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 19/31 [00:04<00:02,  5.13it/s][A
  Layers:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 20/31 [00:04<00:02,  5.08it/s][A
  Layers:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 21/31 [00:04<00:01,  5.17it/s][A
  Layers:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 22/31 [00:04<00:01,  5.31it/s][A
  Layers:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 23/31 [00:05<00:01,  5.18it/s][A
  Layers:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 24/31 [00:05<00:01,  5.10it/s][A
  Layers:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 25/31 [00:05<00:01,  5.11it/s][A
  Layers:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 26/31 [00:05<00:00,  5.25it/s][A
  Layers:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 27/31 [00:05<00:00,  5.39it/s][A
  Layers:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 28/31 [00:06<00:00,  5.31it/s][A
  Layers:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 29/31 [00:06<00:00,  5.39it/s][A
  Layers:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 30/31 [00:06<00:00,  4.87it/s][A
  Layers: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 31/31 [00:06<00:00,  3.70it/s][A
                                                         [AScenarios:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [02:33<00:59, 14.96s/it]
ðŸ” Medium_60
  Tokens: 124

  Layers:   0%|          | 0/31 [00:00<?, ?it/s][A
  Layers:   3%|â–Ž         | 1/31 [00:00<00:23,  1.25it/s][A
  Layers:   6%|â–‹         | 2/31 [00:01<00:15,  1.90it/s][A
  Layers:  10%|â–‰         | 3/31 [00:01<00:12,  2.32it/s][A
  Layers:  13%|â–ˆâ–Ž        | 4/31 [00:01<00:10,  2.56it/s][A
  Layers:  16%|â–ˆâ–Œ        | 5/31 [00:02<00:09,  2.72it/s][A
  Layers:  19%|â–ˆâ–‰        | 6/31 [00:02<00:08,  2.85it/s][A
  Layers:  23%|â–ˆâ–ˆâ–Ž       | 7/31 [00:02<00:08,  2.95it/s][A
  Layers:  26%|â–ˆâ–ˆâ–Œ       | 8/31 [00:03<00:07,  3.00it/s][A
  Layers:  29%|â–ˆâ–ˆâ–‰       | 9/31 [00:03<00:07,  2.99it/s][A
  Layers:  32%|â–ˆâ–ˆâ–ˆâ–      | 10/31 [00:03<00:06,  3.06it/s][A
  Layers:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 11/31 [00:04<00:06,  2.98it/s][A
  Layers:  39%|â–ˆâ–ˆâ–ˆâ–Š      | 12/31 [00:04<00:06,  3.01it/s][A
  Layers:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 13/31 [00:04<00:06,  2.96it/s][A
  Layers:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 14/31 [00:05<00:05,  3.08it/s][A
  Layers:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 15/31 [00:05<00:05,  3.11it/s][A
  Layers:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 16/31 [00:05<00:04,  3.11it/s][A
  Layers:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 17/31 [00:05<00:04,  3.14it/s][A
  Layers:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 18/31 [00:06<00:04,  3.06it/s][A
  Layers:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 19/31 [00:06<00:03,  3.16it/s][A
  Layers:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 20/31 [00:06<00:03,  3.13it/s][A
  Layers:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 21/31 [00:07<00:03,  3.15it/s][A
  Layers:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 22/31 [00:07<00:02,  3.14it/s][A
  Layers:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 23/31 [00:07<00:02,  3.21it/s][A
  Layers:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 24/31 [00:08<00:02,  3.12it/s][A
  Layers:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 25/31 [00:08<00:01,  3.14it/s][A
  Layers:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 26/31 [00:08<00:01,  3.11it/s][A
  Layers:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 27/31 [00:09<00:01,  3.14it/s][A
  Layers:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 28/31 [00:09<00:00,  3.14it/s][A
  Layers:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 29/31 [00:09<00:00,  3.10it/s][A
  Layers:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 30/31 [00:10<00:00,  2.99it/s][A
  Layers: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 31/31 [00:10<00:00,  2.52it/s][A
                                                         [AScenarios:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [02:44<00:41, 13.72s/it]
ðŸ” Goal_achieved_200
  Tokens: 110

  Layers:   0%|          | 0/31 [00:00<?, ?it/s][A
  Layers:   3%|â–Ž         | 1/31 [00:01<00:59,  1.98s/it][A
  Layers:   6%|â–‹         | 2/31 [00:02<00:28,  1.02it/s][A
  Layers:  10%|â–‰         | 3/31 [00:02<00:18,  1.54it/s][A
  Layers:  13%|â–ˆâ–Ž        | 4/31 [00:02<00:13,  1.99it/s][A
  Layers:  16%|â–ˆâ–Œ        | 5/31 [00:03<00:11,  2.34it/s][A
  Layers:  19%|â–ˆâ–‰        | 6/31 [00:03<00:09,  2.69it/s][A
  Layers:  23%|â–ˆâ–ˆâ–Ž       | 7/31 [00:03<00:08,  2.91it/s][A
  Layers:  26%|â–ˆâ–ˆâ–Œ       | 8/31 [00:03<00:07,  3.07it/s][A
  Layers:  29%|â–ˆâ–ˆâ–‰       | 9/31 [00:04<00:07,  3.13it/s][A
  Layers:  32%|â–ˆâ–ˆâ–ˆâ–      | 10/31 [00:04<00:06,  3.21it/s][A
  Layers:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 11/31 [00:04<00:06,  3.19it/s][A
  Layers:  39%|â–ˆâ–ˆâ–ˆâ–Š      | 12/31 [00:05<00:05,  3.35it/s][A
  Layers:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 13/31 [00:05<00:05,  3.24it/s][A
  Layers:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 14/31 [00:05<00:05,  3.34it/s][A
  Layers:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 15/31 [00:05<00:04,  3.41it/s][A
  Layers:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 16/31 [00:06<00:04,  3.35it/s][A
  Layers:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 17/31 [00:06<00:04,  3.44it/s][A
  Layers:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 18/31 [00:06<00:03,  3.49it/s][A
  Layers:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 19/31 [00:07<00:03,  3.52it/s][A
  Layers:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 20/31 [00:07<00:03,  3.60it/s][A
  Layers:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 21/31 [00:07<00:02,  3.70it/s][A
  Layers:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 22/31 [00:07<00:02,  3.61it/s][A
  Layers:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 23/31 [00:08<00:02,  3.58it/s][A
  Layers:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 24/31 [00:08<00:01,  3.65it/s][A
  Layers:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 25/31 [00:08<00:01,  3.72it/s][A
  Layers:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 26/31 [00:09<00:01,  3.76it/s][A
  Layers:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 27/31 [00:09<00:01,  3.72it/s][A
  Layers:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 28/31 [00:09<00:00,  3.69it/s][A
  Layers:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 29/31 [00:09<00:00,  3.74it/s][A
  Layers:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 30/31 [00:10<00:00,  3.54it/s][A
  Layers: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 31/31 [00:10<00:00,  2.91it/s][A
                                                         [AScenarios:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:55<00:25, 12.75s/it]
ðŸ” Very_risky_25
  Tokens: 121

  Layers:   0%|          | 0/31 [00:00<?, ?it/s][A
  Layers:   3%|â–Ž         | 1/31 [00:00<00:26,  1.15it/s][A
  Layers:   6%|â–‹         | 2/31 [00:01<00:15,  1.85it/s][A
  Layers:  10%|â–‰         | 3/31 [00:01<00:12,  2.32it/s][A
  Layers:  13%|â–ˆâ–Ž        | 4/31 [00:01<00:10,  2.64it/s][A
  Layers:  16%|â–ˆâ–Œ        | 5/31 [00:02<00:09,  2.81it/s][A
  Layers:  19%|â–ˆâ–‰        | 6/31 [00:02<00:08,  2.95it/s][A
  Layers:  23%|â–ˆâ–ˆâ–Ž       | 7/31 [00:02<00:07,  3.11it/s][A
  Layers:  26%|â–ˆâ–ˆâ–Œ       | 8/31 [00:03<00:07,  3.11it/s][A
  Layers:  29%|â–ˆâ–ˆâ–‰       | 9/31 [00:03<00:07,  3.10it/s][A
  Layers:  32%|â–ˆâ–ˆâ–ˆâ–      | 10/31 [00:03<00:06,  3.12it/s][A
  Layers:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 11/31 [00:03<00:06,  3.11it/s][A
  Layers:  39%|â–ˆâ–ˆâ–ˆâ–Š      | 12/31 [00:04<00:05,  3.22it/s][A
  Layers:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 13/31 [00:04<00:05,  3.18it/s][A
  Layers:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 14/31 [00:04<00:05,  3.20it/s][A
  Layers:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 15/31 [00:05<00:04,  3.21it/s][A
  Layers:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 16/31 [00:05<00:04,  3.21it/s][A
  Layers:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 17/31 [00:05<00:04,  3.25it/s][A
  Layers:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 18/31 [00:06<00:03,  3.29it/s][A
  Layers:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 19/31 [00:06<00:03,  3.28it/s][A
  Layers:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 20/31 [00:06<00:03,  3.22it/s][A
  Layers:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 21/31 [00:07<00:03,  3.30it/s][A
  Layers:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 22/31 [00:07<00:02,  3.35it/s][A
  Layers:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 23/31 [00:07<00:02,  3.28it/s][A
  Layers:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 24/31 [00:07<00:02,  3.33it/s][A
  Layers:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 25/31 [00:08<00:01,  3.38it/s][A
  Layers:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 26/31 [00:08<00:01,  3.33it/s][A
  Layers:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 27/31 [00:08<00:01,  3.39it/s][A
  Layers:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 28/31 [00:09<00:00,  3.33it/s][A
  Layers:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 29/31 [00:09<00:00,  3.31it/s][A
  Layers:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 30/31 [00:09<00:00,  3.15it/s][A
  Layers: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 31/31 [00:10<00:00,  2.58it/s][A
                                                         [AScenarios:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [03:06<00:12, 12.33s/it]
ðŸ” Big_success_280
  Tokens: 110

  Layers:   0%|          | 0/31 [00:00<?, ?it/s][A
  Layers:   3%|â–Ž         | 1/31 [00:02<01:16,  2.54s/it][A
  Layers:   6%|â–‹         | 2/31 [00:02<00:35,  1.21s/it][A
  Layers:  10%|â–‰         | 3/31 [00:03<00:21,  1.27it/s][A
  Layers:  13%|â–ˆâ–Ž        | 4/31 [00:03<00:15,  1.71it/s][A
  Layers:  16%|â–ˆâ–Œ        | 5/31 [00:03<00:12,  2.12it/s][A
  Layers:  19%|â–ˆâ–‰        | 6/31 [00:04<00:13,  1.79it/s][A
  Layers:  23%|â–ˆâ–ˆâ–Ž       | 7/31 [00:04<00:11,  2.14it/s][A
  Layers:  26%|â–ˆâ–ˆâ–Œ       | 8/31 [00:04<00:09,  2.47it/s][A
  Layers:  29%|â–ˆâ–ˆâ–‰       | 9/31 [00:05<00:08,  2.71it/s][A
  Layers:  32%|â–ˆâ–ˆâ–ˆâ–      | 10/31 [00:05<00:07,  2.92it/s][A
  Layers:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 11/31 [00:05<00:06,  3.06it/s][A
  Layers:  39%|â–ˆâ–ˆâ–ˆâ–Š      | 12/31 [00:06<00:05,  3.26it/s][A
  Layers:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 13/31 [00:06<00:05,  3.24it/s][A
  Layers:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 14/31 [00:06<00:04,  3.40it/s][A
  Layers:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 15/31 [00:06<00:04,  3.45it/s][A
  Layers:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 16/31 [00:07<00:04,  3.45it/s][A
  Layers:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 17/31 [00:07<00:03,  3.51it/s][A
  Layers:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 18/31 [00:07<00:03,  3.51it/s][A
  Layers:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 19/31 [00:08<00:03,  3.63it/s][A
  Layers:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 20/31 [00:08<00:03,  3.60it/s][A
  Layers:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 21/31 [00:08<00:02,  3.61it/s][A
  Layers:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 22/31 [00:08<00:02,  3.60it/s][A
  Layers:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 23/31 [00:09<00:02,  3.67it/s][A
  Layers:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 24/31 [00:09<00:01,  3.71it/s][A
  Layers:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 25/31 [00:09<00:01,  3.70it/s][A
  Layers:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 26/31 [00:09<00:01,  3.75it/s][A
  Layers:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 27/31 [00:10<00:01,  3.69it/s][A
  Layers:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 28/31 [00:10<00:00,  3.75it/s][A
  Layers:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 29/31 [00:10<00:00,  3.79it/s][A
  Layers:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 30/31 [00:11<00:00,  2.51it/s][A
  Layers: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 31/31 [00:11<00:00,  2.27it/s][A
                                                         [AScenarios: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [03:18<00:00, 12.23s/it]Scenarios: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [03:18<00:00, 19.87s/it]

âœ… Complete!
   Metadata: metadata_L1_31_20251020_002355.json
   Data: /data/llm_addiction/experiment_6_L1_31_token_tracking/[scenario_name]/layer_*.npz
   Total size: 272.6 MB
