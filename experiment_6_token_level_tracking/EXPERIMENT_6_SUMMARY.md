# Experiment 6: Token-Level Feature Tracking - Summary

## ğŸ¯ Objective
Understand how specific tokens (especially balance amounts) activate SAE features at different positions in the prompt, tracing the pathway from input tokens to features to output.

## ğŸ“Š Experimental Design

### Data Collection
- **10 diverse scenarios** covering balance range $10-$280
- **3 layers analyzed**: L8 (early), L15 (middle), L31 (late)
- **Token-level feature extraction**: 32,768 features per position per layer
- **Attention patterns**: Captured for all layers
- **Total data size**: 2.3 GB

### Scenarios Tested
1. **Desperate_10** ($10) - Near bankruptcy
2. **VeryRisky_25** ($25) - Very low balance
3. **Risky_40** ($40) - Low balance
4. **Medium_60** ($60) - Medium-low balance
5. **Bankruptcy_90** ($90) - Critical balance (common all-in scenario)
6. **Initial_100** ($100) - Starting balance
7. **Safe_130** ($130) - Safe balance (most common voluntary stop)
8. **Safe_140** ($140) - Safe balance
9. **Goal_200** ($200) - Goal achieved
10. **BigSuccess_280** ($280) - Big success

## ğŸ”¬ Key Findings

### Finding 1: L8-2083 is the Dominant Balance Token Feature

**Discovery**: L8-2083 appears in **TOP 1** position for **ALL 10 scenarios** (100%)

**Activation Pattern**:
```
Balance â†’ L8-2083 Activation
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
$ 90 â†’ 19.21 â­ HIGHEST
$130 â†’ 18.49 â­ 2nd HIGHEST
$100 â†’ 15.83
$ 40 â†’ 15.36
$ 10 â†’ 15.15
$140 â†’ 13.82
$200 â†’ 13.16
$ 25 â†’ 12.76
$ 60 â†’ 12.02
$280 â†’ 12.07
```

**Statistical Analysis**:
- **Pearson r = -0.224** (p = 0.534) - Weak negative correlation
- **Spearman Ï = -0.103** (p = 0.777) - No monotonic relationship

**Interpretation**: L8-2083 does **NOT** encode simple numerical magnitude. Instead:
- Highest activation at **$90** (critical bankruptcy scenario)
- 2nd highest at **$130** (most common safe stop)
- These are **task-salient** balances, not just large numbers
- Suggests L8-2083 encodes **"numerical salience in gambling context"**

### Finding 2: Universal Feature Trio

Three features appear in **Top 10 for ALL scenarios**:

1. **L8-2083** (100% of scenarios) - Balance token feature
2. **L8-17211** (100% of scenarios) - Supporting numerical feature
3. **L8-16593** (100% of scenarios) - Supporting numerical feature

**Implication**: These three features form a stable "numerical representation complex" for balance amounts.

### Finding 3: Feature Sparsity Pattern

**Average active features per scenario**: 350-470 out of 32,768 (1.1-1.4%)
- **Bankruptcy_90**: 474 active features (HIGHEST sparsity)
- **BigSuccess_280**: 388 active features (LOWEST sparsity)

**Total activation varies**:
- Range: ~220-265 total activation units
- No clear correlation with balance amount

### Finding 4: Different Features for Different Processing Stages

| Stage | Layer | Feature Examples | Function |
|-------|-------|------------------|----------|
| **Token Encoding** | L8 | 2083, 17211, 16593 | Encode numerical tokens |
| **Decision-Making** | L8, L31 | 2059, 12478, 10692 | Discriminate bankruptcy vs safe |

**Critical Insight**:
- **L8-2083** (token-level) â‰  **L8-2059** (decision-level)
- Different features operate at different processing stages!
- Token features encode INPUT information
- Decision features encode OUTPUT behavior

## âš ï¸ Limitations Discovered

### 1. Prompt vs Generation
**Issue**: This experiment only captured **prompt encoding**, not **decision generation**

**Impact**:
- L31 features at "output" position are from prompt end, not actual model output
- Attention patterns don't show decision-making flow
- L31-10692 (known risky feature) = 0.000 for all scenarios

**What's Missing**: Need to capture features **during token generation** to see true decision pathways

### 2. Attention Patterns Not Informative
**Issue**: Attention from balance tokens to output position â‰ˆ 0.000

**Reason**: The "output" is just the last prompt token ("Choice: "), not a generated decision

**What Would Work**: Capture attention during generation of "Bet" or "Stop" token

### 3. L31 Output Features Empty
**Issue**: L31-10692 (risky feature from Exp 1) shows 0.000 activation

**Reason**: This feature activates during decision-making, not prompt encoding

**Solution**: Run generation and capture features from the generated decision token

## âœ… What This Experiment Successfully Demonstrated

1. âœ… **Token-level feature extraction works** - Successfully captured 32,768 features per token position
2. âœ… **Discovered L8-2083** - Universal balance token encoding feature
3. âœ… **Confirmed feature specificity** - Different tokens activate different feature patterns
4. âœ… **Identified feature diversity** - Thousands of features activate, maintaining sparsity
5. âœ… **Demonstrated layer specialization** - L8 encodes tokens, L31 (would) encode decisions

## ğŸ’¡ Key Insights

### Insight 1: Hierarchical Feature Organization
```
Input Layer (L8)
    â†“
 L8-2083, L8-17211, L8-16593
    (Encode numerical tokens)
    â†“
Middle Layers (L15)
    (Transform representations)
    â†“
Output Layer (L31)
    â†“
 L31-10692, L31-???
    (Encode decision features)
    â†“
Generated Token ("Bet" / "Stop")
```

### Insight 2: Task-Relevant Salience
L8-2083 activation pattern suggests the model learns **task-relevant importance**:
- $90 (19.21) - Critical decision point (all-in or stop?)
- $130 (18.49) - Common safe stopping point
- Not just "bigger number = more activation"

### Insight 3: Feature Stability
The universal trio (2083, 17211, 16593) shows **robust numerical encoding**:
- Works across all balance amounts
- Stable across different prompt contexts
- Suggests these are fundamental number-processing features

## ğŸ”„ Comparison with Experiment 1

| Aspect | Experiment 1 | Experiment 6 |
|--------|-------------|--------------|
| **Focus** | Decision outcomes | Token processing |
| **Features** | L8-2059, L8-12478 | L8-2083, L8-17211 |
| **Position** | Output token (last) | Input tokens (all) |
| **Function** | Discriminate decisions | Encode numbers |
| **When Active** | During generation | During encoding |

**Conclusion**: Both experiments reveal **complementary mechanisms**:
- Exp 1: Features that predict OUTCOMES
- Exp 6: Features that encode INPUTS

## ğŸ“ Recommended Next Steps

### 1. Experiment 6B: Generation-Time Feature Tracking
**Goal**: Capture features **during token generation**

**Design**:
```python
# Generate decision token
outputs = model.generate(
    inputs,
    max_new_tokens=1,
    output_hidden_states=True,
    return_dict_in_generate=True
)

# Extract features from GENERATED token
generated_token_features = sae.encode(
    outputs.hidden_states[0][-1][-1]  # Last layer, last token
)
```

**Expected**: See L31-10692 and other decision features activate

### 2. Experiment 6C: Full Pathway Analysis
**Goal**: Trace complete pathway with generation

**Capture**:
1. L8 features at balance token (input)
2. L8 â†’ L15 â†’ L31 feature evolution
3. Attention from balance to generated decision
4. L31 features at generated decision token (output)

**Analysis**: Correlation between input features and output features

### 3. Cross-Reference with Experiment 2
**Goal**: Validate causal features work at both encoding and generation

**Method**:
- Patch L8-2083 at balance token position
- Measure effect on generated decision
- Compare with patching L8-2059 at output position

## ğŸ“ˆ Visualizations Generated

1. **final_comprehensive.png** - 6-panel comprehensive analysis
   - Balance vs L8-2083 activation (main finding)
   - Top 3 features comparison
   - Feature sparsity analysis
   - Total activation by balance
   - Scenario balance distribution
   - Feature activation heatmap

2. **corrected_analysis.png** - Pathway analysis (limited by prompt-only data)

3. **balance_to_feature.png** - Original analysis (showed zeros due to wrong features)

## ğŸ¯ Final Assessment

**Success Metrics**:
- âœ… Token-level extraction: SUCCESS
- âœ… Feature discovery: SUCCESS (L8-2083, L8-17211, L8-16593)
- âš ï¸ Pathway tracing: PARTIAL (limited by prompt-only data)
- âŒ Decision features: FAILED (need generation-time capture)

**Overall**: **Experiment 6 successfully demonstrates token-level feature extraction and discovers numerical encoding features, but reveals the need for generation-time capture to analyze decision pathways.**

---

**Files Generated**:
- `/data/llm_addiction/experiment_6_token_level/token_level_20251010_042447.json` (2.3 GB)
- `corrected_analysis.png`
- `final_comprehensive.png`
- `corrected_analysis.log`
- `final_report.log`

**Date**: 2025-10-10
**Status**: âœ… COMPLETE
