# Experiment 6: Token-Level Feature Tracking

## ğŸ¯ ëª©í‘œ

Experiment 1ì˜ í•œê³„ë¥¼ ê·¹ë³µí•˜ì—¬ **token-level attribution** ê°€ëŠ¥í•˜ê²Œ ë§Œë“¤ê¸°

## âŒ Experiment 1ì˜ í•œê³„

```python
# Experiment 1 ë°ì´í„° êµ¬ì¡°
{
  "L8": [32768 floats]  # Last tokenë§Œ, shape: (32768,)
}
```

**ë¶ˆê°€ëŠ¥í–ˆë˜ ê²ƒ**:
1. âŒ Token-level attribution: "$100" í† í°ì˜ ì˜í–¥ ë¶ˆê°€
2. âŒ Causal validation: Correlationë§Œ ê°€ëŠ¥
3. âŒ Attention flow: Attention patterns ì—†ìŒ
4. âŒ Position-specific: Last tokenë§Œ ì¶”ì¶œ

## âœ… Experiment 6ì˜ í•´ê²°ì±…

```python
# Experiment 6 ë°ì´í„° êµ¬ì¡°
{
  "tokens": ["Current", "balance", ":", "$", "100", ...],
  "layers": {
    "L8": {
      "features": [[f1], [f2], ..., [fn]],  # shape: (seq_len, 32768) âœ…
      "attention": [[[...]]]                 # shape: (n_heads, seq_len, seq_len) âœ…
    }
  }
}
```

**ê°€ëŠ¥í•œ ê²ƒ**:
1. âœ… Token-level attribution: ê° í† í°ì˜ contribution ì¸¡ì •
2. âœ… Attention flow: ì–´ë–¤ í† í°ì´ outputì— attendí•˜ëŠ”ì§€
3. âœ… Position-specific analysis: "$100" ìœ„ì¹˜ì˜ features ì¶”ì¶œ
4. âœ… Token â†’ Feature â†’ Output pathway ì¶”ì 

---

## ğŸ“Š ì‹¤í—˜ ì„¤ê³„

### Data Collection

**Critical Layers**: L8, L15, L31 (Phase 1ì—ì„œ ì¤‘ìš”í•˜ë‹¤ê³  ë°œê²¬)

**ê° ê²Œì„ë§ˆë‹¤ ìˆ˜ì§‘**:
1. **Tokens**: ëª¨ë“  token positions
2. **Features**: `(seq_len, 32768)` per layer
3. **Attention**: `(n_heads, seq_len, seq_len)` per layer

### Sample Size

**Prototype**: 10 games (ë¹ ë¥¸ ê²€ì¦)
- ì˜ˆìƒ ì†Œìš”: ~30ë¶„
- íŒŒì¼ í¬ê¸°: ~100-200MB

**Full**: 50 games (Experiment 1ê³¼ ë™ì¼)
- ì˜ˆìƒ ì†Œìš”: ~2-3ì‹œê°„
- íŒŒì¼ í¬ê¸°: ~1-2GB

---

## ğŸš€ ì‹¤í–‰ ë°©ë²•

### 1. Prototype ì‹¤í–‰ (10 games)

```bash
cd /home/ubuntu/llm_addiction/experiment_6_token_level_tracking
conda activate llama_sae_env
python experiment_6_token_tracking.py
```

### 2. ë¶„ì„

```bash
# Token attribution ë¶„ì„
python analyze_token_attribution.py /data/llm_addiction/experiment_6_token_level/token_level_tracking_*.json
```

---

## ğŸ“ˆ ì˜ˆìƒ ê²°ê³¼

### Token Attribution

```
L8 Token Attribution:
Top 10 tokens contributing to output:
Rank   Position   Token                Importance    Attention     ||Features||
1      45         $100                 0.234567      0.156         1.234
2      12         balance              0.123456      0.089         0.987
3      67         Bet                  0.098765      0.234         0.456
...
```

### Attention Flow

ê° ë ˆì´ì–´ì—ì„œ:
- "$100" í† í°ì´ outputì— ì–¼ë§ˆë‚˜ attendí•˜ëŠ”ì§€
- "balance" í† í°ì˜ ì˜í–¥
- "Bet" vs "Stop" ì„ íƒì§€ì˜ attention

### Feature Heatmap

Position Ã— Feature íˆíŠ¸ë§µ:
- ì–´ëŠ positionì—ì„œ ì–´ë–¤ featureê°€ í™œì„±í™”ë˜ëŠ”ì§€
- "$100" ìœ„ì¹˜ì—ì„œ L8-2059 ê°™ì€ risky feature í™œì„±í™”?

---

## ğŸ”¬ Anthropic 2025 ë°©ë²•ë¡  ì ìš©

### í˜„ì¬ ê°€ëŠ¥ (Experiment 6 ì™„ë£Œ í›„)

1. **Attribution Patching**
   ```python
   # "$100" â†’ "$10" ë¡œ ë°”ê¿”ì„œ feature ë³€í™” ì¸¡ì •
   clean_prompt = "Current balance: $100"
   corrupted_prompt = "Current balance: $10"

   # Position 45 ($100) íŒ¨ì¹­
   patch_activation(layer=8, position=45, from_prompt=corrupted, to_prompt=clean)
   measure_output_change()
   ```

2. **Attention-weighted Feature Attribution**
   ```python
   # "$100" í† í°ì˜ ê¸°ì—¬ë„
   contribution = (
       attention_to_output[pos_$100] *
       feature_magnitude[pos_$100] *
       feature_importance[L8-2059]
   )
   ```

3. **Backward Tracing**
   ```python
   # Output "Bet $10" â† L31-10692 â† L8-2059 â† Position "$100"
   trace_pathway(
       from_output="Bet",
       through_features=[("L31", 10692), ("L8", 2059)],
       to_input_position=45  # "$100"
   )
   ```

### ì•„ì§ ë¶ˆê°€ëŠ¥ (CLTs í•„ìš”)

1. **Cross-Layer Transcoders (CLTs)**
   - Anthropicê°€ ë”°ë¡œ í•™ìŠµí•œ ëª¨ë¸
   - Layer ê°„ feature ì—°ê²°ì„ decompose
   - ìš°ë¦¬ëŠ” correlationìœ¼ë¡œ ê·¼ì‚¬

2. **Causal Graphs**
   - Anthropic: ì™„ì „í•œ computational graph
   - ìš°ë¦¬: Attention + correlation ê¸°ë°˜ pathway

---

## ğŸ“ íŒŒì¼ êµ¬ì¡°

```
experiment_6_token_level_tracking/
â”œâ”€â”€ experiment_6_token_tracking.py    # ë©”ì¸ ì‹¤í—˜ ì½”ë“œ
â”œâ”€â”€ analyze_token_attribution.py     # ë¶„ì„ ì½”ë“œ
â”œâ”€â”€ README.md                         # ì´ íŒŒì¼
â””â”€â”€ /data/llm_addiction/experiment_6_token_level/
    â””â”€â”€ token_level_tracking_*.json  # ê²°ê³¼ íŒŒì¼
```

---

## ğŸ¯ ë‹µë³€: "ë¶ˆê°€ëŠ¥í•œ ê²ƒì„ ë‹¤ì‹œ ì‹¤í—˜ìœ¼ë¡œ í™•ì¸ ê°€ëŠ¥í•œê°€?"

### âœ… YES! Experiment 6ìœ¼ë¡œ ê°€ëŠ¥

| í•­ëª© | Experiment 1 | Experiment 6 |
|------|--------------|--------------|
| Token-level attribution | âŒ | âœ… Position-specific features |
| Causal validation | âŒ | âœ… Attribution patching ê°€ëŠ¥ |
| Attention flow | âŒ | âœ… Full attention patterns |
| Position-specific | âŒ | âœ… All positions extracted |

### ì‹¤í–‰ ê°€ëŠ¥ ë¶„ì„

1. **"$100" í† í°ì´ "bet" ê²°ì •ì— ì–¼ë§ˆë‚˜ ê¸°ì—¬í•˜ëŠ”ê°€?**
   - âœ… Attention weight Ã— Feature magnitudeë¡œ ì¸¡ì •

2. **"balance" í† í°ì˜ ì˜í–¥ì€?**
   - âœ… Position-specific feature activation ë¶„ì„

3. **L8ì˜ ì–´ë–¤ positionì—ì„œ risky featuresê°€ í™œì„±í™”ë˜ëŠ”ê°€?**
   - âœ… Feature heatmapìœ¼ë¡œ ì‹œê°í™”

4. **Token â†’ Feature â†’ Output pathway?**
   - âœ… Attention flow + feature correlationìœ¼ë¡œ ì¶”ì 

---

## â±ï¸ ì˜ˆìƒ ì†Œìš” ì‹œê°„

**Prototype (10 games)**:
- ë°ì´í„° ìˆ˜ì§‘: ~20-30ë¶„
- ë¶„ì„: ~10ë¶„
- ì´: ~40ë¶„

**Full (50 games)**:
- ë°ì´í„° ìˆ˜ì§‘: ~2-3ì‹œê°„
- ë¶„ì„: ~30ë¶„
- ì´: ~3-4ì‹œê°„

---

## ğŸš€ ì¦‰ì‹œ ì‹œì‘ ê°€ëŠ¥

**GPU 2 ì‚¬ìš© ê°€ëŠ¥** (0 MiB / 81920 MiB)

```bash
# GPU 2ì—ì„œ prototype ì‹¤í–‰
tmux new -s exp6_token_level
conda activate llama_sae_env
cd /home/ubuntu/llm_addiction/experiment_6_token_level_tracking
python experiment_6_token_tracking.py
```

**ì™„ë£Œ í›„**:
- Token attribution ë¶„ì„
- Attention flow ì‹œê°í™”
- Experiment 1ê³¼ ë¹„êµ
- ë…¼ë¬¸ì— ì¶”ê°€ ê°€ëŠ¥í•œ ê²°ê³¼

---

**Date**: 2025-10-10
**Status**: Ready to run
**GPU**: 2 (available)
