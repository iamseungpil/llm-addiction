/data/miniforge3/envs/llama_sae_env/lib/python3.11/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
====================================================================================================
EXPERIMENT 6: TOKEN-LEVEL FEATURE TRACKING
====================================================================================================

ðŸ”§ Loading LLaMA model...
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|â–ˆâ–ˆâ–Œ       | 1/4 [00:01<00:04,  1.49s/it]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2/4 [00:02<00:02,  1.48s/it]Loading checkpoint shards:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4 [00:04<00:01,  1.48s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:04<00:00,  1.03s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:04<00:00,  1.20s/it]
âœ… Model loaded

ðŸ”§ Loading SAEs...
  Loading Layer 8...
ðŸ”§ Loading config: /data/.cache/huggingface/hub/models--fnlp--Llama3_1-8B-Base-LXR-8x/snapshots/8dbc1d85edfced43081c03c38b05514dbab1368b/Llama3_1-8B-Base-L8R-8x/hyperparams.json
âœ… Loading WORKING SAE for Layer 8
   Config: /data/.cache/huggingface/hub/models--fnlp--Llama3_1-8B-Base-LXR-8x/snapshots/8dbc1d85edfced43081c03c38b05514dbab1368b/Llama3_1-8B-Base-L8R-8x/hyperparams.json
   Dataset norm: 6.3125
   Norm factor: 10.138614
   Using ReLU instead of JumpReLU (threshold=0.0)
ðŸ”§ Loading checkpoint: /data/.cache/huggingface/hub/models--fnlp--Llama3_1-8B-Base-LXR-8x/snapshots/8dbc1d85edfced43081c03c38b05514dbab1368b/Llama3_1-8B-Base-L8R-8x/checkpoints/final.safetensors
   File size: 512.1 MB
ðŸ”§ Initializing SAE model...
ðŸ”§ Loading checkpoint weights (CPU)...
âœ… Loaded safetensors file
ðŸ”§ Processing checkpoint weights...
   Loading W_E (encoder.weight)... converted... âœ… (torch.Size([4096, 32768]))
   Loading b_E (encoder.bias)... converted... âœ… (torch.Size([32768]))
   Loading W_D (decoder.weight)... converted... âœ… (torch.Size([32768, 4096]))
   Loading b_D (decoder.bias)... converted... âœ… (torch.Size([4096]))
ðŸ”§ Clearing checkpoint from memory...
ðŸ”§ Loading state dict to model...
ðŸ”§ Moving model to cuda:0...
âœ… OPTIMIZED SAE loaded successfully!
   Layer: 8
   Expected reconstruction error: ~47%
   Feature clamping: VERIFIED WORKING
   Memory optimizations: ENABLED
  âœ… Layer 8 ready
  Loading Layer 15...
ðŸ”§ Loading config: /data/.cache/huggingface/hub/models--fnlp--Llama3_1-8B-Base-LXR-8x/snapshots/8dbc1d85edfced43081c03c38b05514dbab1368b/Llama3_1-8B-Base-L15R-8x/hyperparams.json
âœ… Loading WORKING SAE for Layer 15
   Config: /data/.cache/huggingface/hub/models--fnlp--Llama3_1-8B-Base-LXR-8x/snapshots/8dbc1d85edfced43081c03c38b05514dbab1368b/Llama3_1-8B-Base-L15R-8x/hyperparams.json
   Dataset norm: 10.8125
   Norm factor: 5.919075
   Using ReLU instead of JumpReLU (threshold=0.0)
ðŸ”§ Loading checkpoint: /data/.cache/huggingface/hub/models--fnlp--Llama3_1-8B-Base-LXR-8x/snapshots/8dbc1d85edfced43081c03c38b05514dbab1368b/Llama3_1-8B-Base-L15R-8x/checkpoints/final_fixed.pth
   File size: 512.1 MB
ðŸ”§ Initializing SAE model...
ðŸ”§ Loading checkpoint weights (CPU)...
âœ… Loaded PyTorch file
ðŸ”§ Processing checkpoint weights...
   Loading W_E (W_E)... converted... âœ… (torch.Size([4096, 32768]))
   Loading b_E (b_E)... converted... âœ… (torch.Size([32768]))
   Loading W_D (W_D)... converted... âœ… (torch.Size([32768, 4096]))
   Loading b_D (b_D)... converted... âœ… (torch.Size([4096]))
ðŸ”§ Clearing checkpoint from memory...
ðŸ”§ Loading state dict to model...
ðŸ”§ Moving model to cuda:0...
âœ… OPTIMIZED SAE loaded successfully!
   Layer: 15
   Expected reconstruction error: ~47%
   Feature clamping: VERIFIED WORKING
   Memory optimizations: ENABLED
  âœ… Layer 15 ready
  Loading Layer 31...
ðŸ”§ Loading config: /data/.cache/huggingface/hub/models--fnlp--Llama3_1-8B-Base-LXR-8x/snapshots/8dbc1d85edfced43081c03c38b05514dbab1368b/Llama3_1-8B-Base-L31R-8x/hyperparams.json
âœ… Loading WORKING SAE for Layer 31
   Config: /data/.cache/huggingface/hub/models--fnlp--Llama3_1-8B-Base-LXR-8x/snapshots/8dbc1d85edfced43081c03c38b05514dbab1368b/Llama3_1-8B-Base-L31R-8x/hyperparams.json
   Dataset norm: 74.5
   Norm factor: 0.859060
   Using ReLU instead of JumpReLU (threshold=0.0)
ðŸ”§ Loading checkpoint: /data/.cache/huggingface/hub/models--fnlp--Llama3_1-8B-Base-LXR-8x/snapshots/8dbc1d85edfced43081c03c38b05514dbab1368b/Llama3_1-8B-Base-L31R-8x/checkpoints/final.safetensors
   File size: 512.1 MB
ðŸ”§ Initializing SAE model...
ðŸ”§ Loading checkpoint weights (CPU)...
âœ… Loaded safetensors file
ðŸ”§ Processing checkpoint weights...
   Loading W_E (encoder.weight)... converted... âœ… (torch.Size([4096, 32768]))
   Loading b_E (encoder.bias)... converted... âœ… (torch.Size([32768]))
   Loading W_D (decoder.weight)... converted... âœ… (torch.Size([32768, 4096]))
   Loading b_D (decoder.bias)... converted... âœ… (torch.Size([4096]))
ðŸ”§ Clearing checkpoint from memory...
ðŸ”§ Loading state dict to model...
ðŸ”§ Moving model to cuda:0...
âœ… OPTIMIZED SAE loaded successfully!
   Layer: 31
   Expected reconstruction error: ~47%
   Feature clamping: VERIFIED WORKING
   Memory optimizations: ENABLED
  âœ… Layer 31 ready
Processing:   0%|          | 0/10 [00:00<?, ?it/s]Processing:  10%|â–ˆ         | 1/10 [00:02<00:18,  2.08s/it]Processing:  20%|â–ˆâ–ˆ        | 2/10 [00:03<00:13,  1.72s/it]Processing:  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:04<00:09,  1.29s/it]Processing:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [00:05<00:06,  1.14s/it]Processing:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [00:06<00:05,  1.04s/it]Processing:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [00:06<00:03,  1.11it/s]Processing:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [00:07<00:02,  1.09it/s]Processing:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [00:08<00:01,  1.12it/s]Processing:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [00:09<00:00,  1.10it/s]Processing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:10<00:00,  1.11it/s]Processing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:10<00:00,  1.03s/it]

ðŸ” Bankruptcy_90
  Tokens: 121
  Layer 8...
  Layer 15...
  Layer 31...

ðŸ” Desperate_10
  Tokens: 121
  Layer 8...
  Layer 15...
  Layer 31...

ðŸ” Safe_130
  Tokens: 96
  Layer 8...
  Layer 15...
  Layer 31...

ðŸ” Risky_40
  Tokens: 110
  Layer 8...
  Layer 15...
  Layer 31...

ðŸ” Safe_140
  Tokens: 110
  Layer 8...
  Layer 15...
  Layer 31...

ðŸ” Initial_100
  Tokens: 79
  Layer 8...
  Layer 15...
  Layer 31...

ðŸ” Medium_60
  Tokens: 110
  Layer 8...
  Layer 15...
  Layer 31...

ðŸ” Goal_200
  Tokens: 110
  Layer 8...
  Layer 15...
  Layer 31...

ðŸ” VeryRisky_25
  Tokens: 110
  Layer 8...
  Layer 15...
  Layer 31...

ðŸ” BigSuccess_280
  Tokens: 110
  Layer 8...
  Layer 15...
  Layer 31...

âœ… Saved: /data/llm_addiction/experiment_6_token_level/token_level_20251010_042447.json
ðŸ’¾ Size: 2337.35 MB
