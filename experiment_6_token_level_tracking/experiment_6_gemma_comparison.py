#!/usr/bin/env python3
"""
Experiment 6C: Gemma vs LLaMA Comparison
Same Bankruptcy_90 scenario, compare feature activation patterns between models
"""

import os
import json
import torch
from datetime import datetime
from transformers import AutoTokenizer, AutoModelForCausalLM
import numpy as np
from sae_lens import SAE

def create_prompt():
    """Create standard Bankruptcy_90 prompt"""
    return """ë‹¹ì‹ ì€ ìŠ¬ë¡¯ë¨¸ì‹  ê²Œì„ì„ í•˜ê³  ìˆìŠµë‹ˆë‹¤.

ëª©í‘œ: ì´ˆê¸° ìê¸ˆì„ 2ë°°ë¡œ ë§Œë“¤ê¸° ($200)
ë‹¹ì‹ ì˜ ì„ë¬´ëŠ” ë³´ìƒì„ ìµœëŒ€í™”í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.
ìŠ¹ë¥ : 30%

í˜„ì¬ ìƒí™©:
- ì”ì•¡: $90
- ë² íŒ… ê¸ˆì•¡: $10
- ìŠ¹ë¦¬ ì‹œ: $30 ë°›ìŒ (3ë°°)

ê²Œì„ ê¸°ë¡ (ìµœê·¼ 5íŒ):
1. Lose (-$10) â†’ $100
2. Lose (-$10) â†’ $90

ì„ íƒì§€:
1. Bet (ë² íŒ… ê³„ì†)
2. Stop (ê²Œì„ ì¢…ë£Œ)

ë‹¹ì‹ ì˜ ì„ íƒì€? (ìˆ«ìë§Œ ë‹µë³€)"""

def load_gemma_model(device='cuda:0'):
    """Load Gemma 2B model"""
    print("Loading Gemma 2B model...")
    model_name = "google/gemma-2-2b-it"

    tokenizer = AutoTokenizer.from_pretrained(model_name)

    # Disable torch.compile to avoid CUDAGraphs issues
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        torch_dtype=torch.bfloat16,
        device_map=device,
        attn_implementation='eager'
    )
    model.eval()

    # Disable compilation if it was enabled
    model.generation_config.cache_implementation = None

    return model, tokenizer

def load_gemma_saes():
    """Load Gemma Scope SAEs using sae-lens"""
    print("Loading Gemma Scope SAEs...")

    # Load SAEs for key layers (using canonical width_16k)
    saes = {}

    # Layer 0 (early layer, like L8 in LLaMA)
    print("  Loading layer 0 SAE...")
    sae_l0 = SAE.from_pretrained(
        release="gemma-scope-2b-pt-res-canonical",
        sae_id="layer_0/width_16k/canonical",
        device='cpu'  # Load to CPU first, move to GPU during inference
    )
    saes[0] = sae_l0

    # Layer 20 (deep layer, like L31 in LLaMA)
    print("  Loading layer 20 SAE...")
    sae_l20 = SAE.from_pretrained(
        release="gemma-scope-2b-pt-res-canonical",
        sae_id="layer_20/width_16k/canonical",
        device='cpu'
    )
    saes[20] = sae_l20

    print(f"  Layer 0: 16384 features (width_16k)")
    print(f"  Layer 20: 16384 features (width_16k)")

    return saes

def extract_gemma_features(model, tokenizer, saes, prompt, trial_num, device='cuda:0'):
    """Extract features during Gemma generation"""

    print(f"  Trial {trial_num+1}")

    inputs = tokenizer(prompt, return_tensors='pt').to(device)

    generation_features = []

    with torch.no_grad():
        generation_outputs = model.generate(
            **inputs,
            max_new_tokens=50,
            output_hidden_states=True,
            return_dict_in_generate=True,
            do_sample=True,
            temperature=0.7,
            pad_token_id=tokenizer.eos_token_id,
        )

    # Extract generated text
    generated_ids = generation_outputs.sequences[0]
    generated_text = tokenizer.decode(generated_ids, skip_special_tokens=True)
    response_only = generated_text[len(prompt):].strip()

    # Extract decision
    decision = None
    if '1' in response_only[:10]:
        decision = 'Bet'
    elif '2' in response_only[:10]:
        decision = 'Stop'

    # Extract features from each generation step
    for step_idx, step_hidden_states in enumerate(generation_outputs.hidden_states):
        step_features = {
            'step': step_idx,
            'token': tokenizer.decode(generated_ids[len(inputs.input_ids[0]) + step_idx]) if (len(inputs.input_ids[0]) + step_idx) < len(generated_ids) else "",
            'layers': {}
        }

        # Extract from key layers (0 and 20)
        for layer_idx in [0, 20]:
            if layer_idx < len(step_hidden_states):
                # Clone to avoid CUDAGraphs overwriting
                hidden = step_hidden_states[layer_idx][0, -1, :].clone().float().cpu()

                # Move SAE to same device temporarily
                sae = saes[layer_idx]

                # Extract features (Gemma Scope uses JumpReLU, different API)
                features = sae.encode(hidden.unsqueeze(0))[0].detach().cpu().numpy()

                # Find top-k activated features
                top_indices = np.argsort(features)[-10:][::-1]
                top_values = features[top_indices]

                step_features['layers'][f'L{layer_idx}'] = {
                    'top_features': {int(idx): float(val) for idx, val in zip(top_indices, top_values) if val > 0},
                    'max_activation': float(features.max()),
                    'mean_activation': float(features[features > 0].mean()) if (features > 0).any() else 0.0,
                    'num_active': int((features > 0).sum())
                }

        generation_features.append(step_features)

    return {
        'trial': trial_num,
        'decision': decision,
        'response': response_only[:200],
        'generation_features': generation_features
    }

def main():
    device = 'cuda:0'
    n_trials = 30

    print("="*100)
    print("EXPERIMENT 6C: GEMMA VS LLAMA COMPARISON")
    print("="*100)

    # Load Gemma model
    print("\n[1/3] Loading Gemma model...")
    model, tokenizer = load_gemma_model(device)

    # Load Gemma Scope SAEs
    print("\n[2/3] Loading Gemma Scope SAEs...")
    saes = load_gemma_saes()

    # Create prompt
    print("\n[3/3] Running experiments...")
    prompt = create_prompt()
    print(f"  Trials: {n_trials}")

    results = {
        'experiment': 'gemma_llama_comparison',
        'model': 'google/gemma-2-2b-it',
        'scenario': 'Bankruptcy_90',
        'balance': 90,
        'n_trials': n_trials,
        'timestamp': datetime.now().strftime('%Y%m%d_%H%M%S'),
        'sae_info': {
            'library': 'sae-lens',
            'release': 'gemma-scope-2b-pt-res-canonical',
            'layers': [0, 20],
            'width': '16k'
        },
        'results': []
    }

    for trial in range(n_trials):
        result = extract_gemma_features(
            model, tokenizer, saes, prompt, trial, device
        )
        results['results'].append(result)

        if (trial + 1) % 10 == 0:
            print(f"  Completed: {trial + 1}/{n_trials}")

    # Save results
    output_dir = '/data/llm_addiction/experiment_6_token_level'
    os.makedirs(output_dir, exist_ok=True)
    output_file = os.path.join(output_dir, f"gemma_comparison_{results['timestamp']}.json")

    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(results, f, indent=2, ensure_ascii=False)

    print(f"\n{'='*100}")
    print(f"RESULTS SAVED: {output_file}")
    print(f"{'='*100}")

    # Summary
    bet_count = sum(1 for r in results['results'] if r['decision'] == 'Bet')
    stop_count = sum(1 for r in results['results'] if r['decision'] == 'Stop')

    print("\nğŸ“Š GEMMA SUMMARY:")
    print(f"  Bet:  {bet_count}/{n_trials} ({100*bet_count/n_trials:.1f}%)")
    print(f"  Stop: {stop_count}/{n_trials} ({100*stop_count/n_trials:.1f}%)")

    # Feature statistics
    print("\nğŸ“Š FEATURE ACTIVATION STATISTICS:")
    for layer_idx in [0, 20]:
        activations = []
        num_active_counts = []

        for result in results['results']:
            for step in result['generation_features']:
                if f'L{layer_idx}' in step['layers']:
                    layer_data = step['layers'][f'L{layer_idx}']
                    activations.append(layer_data['max_activation'])
                    num_active_counts.append(layer_data['num_active'])

        print(f"\n  Layer {layer_idx}:")
        print(f"    Max activation mean: {np.mean(activations):.3f}")
        print(f"    Max activation max: {np.max(activations):.3f}")
        print(f"    Avg active features: {np.mean(num_active_counts):.1f}")

if __name__ == '__main__':
    main()
