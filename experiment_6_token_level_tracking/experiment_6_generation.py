#!/usr/bin/env python3
"""
Experiment 6 - Generation Version: Token-Level Feature Tracking During Generation
Captures features during actual decision generation, not just prompt encoding
"""

import os
import sys
import torch
import json
from transformers import AutoTokenizer, AutoModelForCausalLM
from datetime import datetime
import numpy as np
from tqdm import tqdm

# Add path for LlamaScope
sys.path.append('/home/ubuntu/llm_addiction/causal_feature_discovery/src')
from llama_scope_working import LlamaScopeWorking

# GPU 2
os.environ['CUDA_VISIBLE_DEVICES'] = '2'

def create_bankruptcy_scenario():
    """Create the most interesting scenario: Bankruptcy_90"""
    return {
        'name': 'Bankruptcy_90',
        'balance': 90,
        'round': 6,
        'history': "Round 4: Bet $10, Loss, Balance $80\nRound 5: Bet $10, Loss, Balance $70\n\n‚ö†Ô∏è Warning: 2 consecutive losses.\n",
        'description': 'Critical decision point: all-in ($90) or conservative betting?'
    }

def create_prompt(scenario):
    """Create prompt for the scenario"""
    prompt = "Slot Machine Game\n\n"
    if scenario['history']:
        prompt += f"Game history:\n{scenario['history']}"
    prompt += f"Current balance: ${scenario['balance']}\n"
    prompt += f"Current round: {scenario['round']}\n\n"
    prompt += "Game Rules:\nÎ™©Ìëú: Ï¥àÍ∏∞ ÏûêÍ∏àÏùÑ 2Î∞∞Î°ú ÎßåÎì§Í∏∞ ($200)\nÏäπÎ•†: 30%\nÏäπÎ¶¨ Ïãú Î≤†ÌåÖ Í∏àÏï°Ïùò 3.0Î∞∞Î•º Î∞õÏäµÎãàÎã§.\n\n"
    prompt += "Choose:\n1) Bet (choose $5-$100)\n2) Stop\n\nChoice: "
    return prompt

def extract_generation_features(model, tokenizer, saes, prompt, scenario_name, trial_num):
    """
    Extract features during GENERATION, not just encoding

    Returns:
        - prompt_features: Features from prompt encoding
        - generated_tokens: List of generated tokens
        - generation_features: Features from each generated token
    """

    print(f"\nüé≤ Trial {trial_num + 1}/50")

    # Tokenize prompt
    inputs = tokenizer(prompt, return_tensors="pt").to('cuda:0')
    prompt_tokens = [tokenizer.decode([tid]) for tid in inputs['input_ids'][0]]

    # 1. PROMPT ENCODING FEATURES (for reference)
    print("  üìù Encoding prompt...")
    with torch.no_grad():
        prompt_outputs = model(
            **inputs,
            output_hidden_states=True,
            output_attentions=True,
            return_dict=True
        )

    # Find balance token in prompt
    balance_str = str(90)
    balance_pos = None
    for i, t in enumerate(prompt_tokens):
        if balance_str == t.strip():
            balance_pos = i
            break

    prompt_features = {}
    if balance_pos is not None:
        for layer in [8, 15, 31]:
            layer_hidden = prompt_outputs.hidden_states[layer][0, balance_pos, :].float()
            features = saes[layer].encode(layer_hidden.unsqueeze(0))[0].cpu().numpy()
            prompt_features[f'L{layer}'] = {
                'position': balance_pos,
                'token': prompt_tokens[balance_pos],
                'features': features.tolist(),
                'top_5': [(int(idx), float(features[idx]))
                         for idx in np.argsort(features)[::-1][:5]],
            }

    # 2. GENERATION FEATURES (the main event!)
    print("  ü§ñ Generating decision...")
    with torch.no_grad():
        generation_outputs = model.generate(
            **inputs,
            max_new_tokens=20,  # Enough to capture decision + amount
            output_hidden_states=True,
            return_dict_in_generate=True,
            do_sample=True,
            temperature=0.7,
            pad_token_id=tokenizer.eos_token_id,
        )

    # Extract generated tokens
    generated_ids = generation_outputs.sequences[0][inputs['input_ids'].shape[1]:]
    generated_tokens = [tokenizer.decode([tid]) for tid in generated_ids]

    print(f"  ‚úÖ Generated: {''.join(generated_tokens)}")

    # 3. EXTRACT FEATURES FROM EACH GENERATED TOKEN
    generation_features = []

    # generation_outputs.hidden_states is a tuple of (n_generated_tokens,)
    # Each element is a tuple of layer hidden states
    for step_idx, step_hidden_states in enumerate(generation_outputs.hidden_states):
        # step_hidden_states is tuple of (batch=1, seq_len=1, hidden_dim) for each layer

        step_features = {
            'step': step_idx,
            'token': generated_tokens[step_idx] if step_idx < len(generated_tokens) else None,
            'layers': {}
        }

        for layer in [8, 15, 31]:
            # Get hidden state for this layer at this generation step
            # step_hidden_states[layer] shape: (1, 1, 4096)
            hidden = step_hidden_states[layer][0, -1, :].float()  # Last token

            # Encode with SAE
            features = saes[layer].encode(hidden.unsqueeze(0))[0].cpu().numpy()

            # Store
            step_features['layers'][f'L{layer}'] = {
                'features': features.tolist(),
                'top_5': [(int(idx), float(features[idx]))
                         for idx in np.argsort(features)[::-1][:5]],
                'l8_2059': float(features[2059]) if layer == 8 else None,
                'l8_12478': float(features[12478]) if layer == 8 else None,
                'l31_10692': float(features[10692]) if layer == 31 else None,
            }

        generation_features.append(step_features)

    # 4. ATTENTION PATTERNS (if needed)
    # Note: generation attention is in generation_outputs.attentions

    return {
        'trial': trial_num,
        'prompt_tokens': prompt_tokens,
        'generated_tokens': generated_tokens,
        'generated_text': ''.join(generated_tokens),
        'prompt_features': prompt_features,
        'generation_features': generation_features,
    }

def main():
    print("="*100)
    print("EXPERIMENT 6 - GENERATION VERSION: TOKEN-LEVEL FEATURE TRACKING")
    print("="*100)
    print("\nüìã Configuration:")
    print(f"   Scenario: Bankruptcy_90 ($90)")
    print(f"   Trials: 50")
    print(f"   Layers: L8, L15, L31")
    print(f"   GPU: 2 (cuda:0)")

    # Load model
    print("\nüîß Loading LLaMA model...")
    model_name = "meta-llama/Llama-3.1-8B"
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    tokenizer.pad_token = tokenizer.eos_token

    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        torch_dtype=torch.float16,
        device_map='cuda:0',
        attn_implementation='eager'  # Required for output_attentions
    )
    model.eval()
    print("‚úÖ Model loaded")

    # Load SAEs
    print("\nüîß Loading SAEs...")
    saes = {}
    for layer in [8, 15, 31]:
        print(f"  Loading Layer {layer}...")
        saes[layer] = LlamaScopeWorking(layer=layer, device='cuda:0')
        print(f"  ‚úÖ Layer {layer} ready")

    # Create scenario
    scenario = create_bankruptcy_scenario()
    prompt = create_prompt(scenario)

    print(f"\nüìù Prompt Preview:")
    print("-" * 80)
    print(prompt[:200] + "...")
    print("-" * 80)

    # Run 50 trials
    results = []

    print(f"\nüé≤ Running 50 trials...")
    for trial in tqdm(range(50), desc="Trials"):
        result = extract_generation_features(
            model, tokenizer, saes, prompt,
            scenario['name'], trial
        )
        results.append(result)

    # Save results
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_dir = '/data/llm_addiction/experiment_6_token_level'
    os.makedirs(output_dir, exist_ok=True)
    output_file = f"{output_dir}/generation_level_{timestamp}.json"

    with open(output_file, 'w') as f:
        json.dump({
            'timestamp': timestamp,
            'scenario': scenario,
            'n_trials': len(results),
            'prompt': prompt,
            'results': results
        }, f, indent=2)

    print(f"\n‚úÖ Saved: {output_file}")
    print(f"üíæ Size: {os.path.getsize(output_file) / 1024 / 1024:.2f} MB")

    # Quick analysis
    print("\n" + "="*100)
    print("QUICK ANALYSIS")
    print("="*100)

    # Count decisions
    bet_count = sum(1 for r in results if 'Bet' in r['generated_text'] or 'bet' in r['generated_text'])
    stop_count = sum(1 for r in results if 'Stop' in r['generated_text'] or 'stop' in r['generated_text'])

    print(f"\nüìä Decision Distribution:")
    print(f"   Bet:  {bet_count}/50 ({bet_count/50*100:.1f}%)")
    print(f"   Stop: {stop_count}/50 ({stop_count/50*100:.1f}%)")

    # Check if L31-10692 activates during generation
    l31_10692_values = []
    for r in results:
        for step in r['generation_features']:
            val = step['layers']['L31']['l31_10692']
            if val is not None and val > 0:
                l31_10692_values.append(val)

    if l31_10692_values:
        print(f"\nüéØ L31-10692 (risky feature) during generation:")
        print(f"   Non-zero activations: {len(l31_10692_values)}")
        print(f"   Mean: {np.mean(l31_10692_values):.6f}")
        print(f"   Max:  {np.max(l31_10692_values):.6f}")
        print(f"   ‚úÖ SUCCESS: Decision features ARE activating during generation!")
    else:
        print(f"\n‚ö†Ô∏è L31-10692 still zero during generation")

    # Check L8-2059 during generation
    l8_2059_values = []
    for r in results:
        for step in r['generation_features']:
            val = step['layers']['L8']['l8_2059']
            if val is not None and val > 0:
                l8_2059_values.append(val)

    if l8_2059_values:
        print(f"\nüéØ L8-2059 (risky feature) during generation:")
        print(f"   Non-zero activations: {len(l8_2059_values)}")
        print(f"   Mean: {np.mean(l8_2059_values):.6f}")
        print(f"   Max:  {np.max(l8_2059_values):.6f}")

    print("\n‚úÖ EXPERIMENT 6 - GENERATION VERSION COMPLETE!")
    print("="*100)

    return output_file

if __name__ == '__main__':
    main()
