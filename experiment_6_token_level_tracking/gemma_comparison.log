/data/miniforge3/lib/python3.10/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
====================================================================================================
EXPERIMENT 6C: GEMMA VS LLAMA COMPARISON
====================================================================================================

[1/3] Loading Gemma model...
Loading Gemma 2B model...
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.73s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.10it/s]

[2/3] Loading Gemma Scope SAEs...
Loading Gemma Scope SAEs...
  Loading layer 0 SAE...
  Loading layer 20 SAE...
  Layer 0: 16384 features (width_16k)
  Layer 20: 16384 features (width_16k)

[3/3] Running experiments...
  Trials: 30
  Trial 1
Traceback (most recent call last):
  File "/home/ubuntu/llm_addiction/experiment_6_token_level_tracking/experiment_6_gemma_comparison.py", line 249, in <module>
    main()
  File "/home/ubuntu/llm_addiction/experiment_6_token_level_tracking/experiment_6_gemma_comparison.py", line 202, in main
    result = extract_gemma_features(
  File "/home/ubuntu/llm_addiction/experiment_6_token_level_tracking/experiment_6_gemma_comparison.py", line 136, in extract_gemma_features
    hidden = step_hidden_states[layer_idx][0, -1, :].clone().float().cpu()
RuntimeError: Error: accessing tensor output of CUDAGraphs that has been overwritten by a subsequent run. Stack trace: File "/data/miniforge3/lib/python3.10/site-packages/transformers/utils/generic.py", line 959, in wrapper
    output = func(self, *args, **kwargs)
  File "/data/miniforge3/lib/python3.10/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 548, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/data/miniforge3/lib/python3.10/site-packages/transformers/utils/generic.py", line 1083, in wrapper
    outputs = func(self, *args, **kwargs)
  File "/data/miniforge3/lib/python3.10/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 442, in forward
    hidden_states = hidden_states * normalizer. To prevent overwriting, clone the tensor outside of torch.compile() or call torch.compiler.cudagraph_mark_step_begin() before each model invocation.
