/data/miniforge3/lib/python3.10/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
================================================================================
GEMMA VS LLAMA COMPARISON (SIMPLIFIED)
================================================================================

Loading Gemma 2B model...
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.60s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.19it/s]
/data/miniforge3/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:236: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.
  warnings.warn(

Running 30 trials...
  Progress: 10/30
  Progress: 20/30
  Progress: 30/30

  Results:
    Bet:  9/30 (30.0%)
    Stop: 0/30 (0.0%)

================================================================================
✅ RESULTS SAVED: /data/llm_addiction/experiment_6_token_level/gemma_simple_20251011_020002.json
================================================================================
