# IMPLEMENTATION_PLAN_V5: Feature Pathway & Token Analysis
## Changes from V4
- Added a mandatory Step 0 pipeline that loads pre-filtered CSV files (640 safe + 2,147 risky = 2,787 total features) from `/home/ubuntu/llm_addiction/analysis/` and converts them to JSON format with enriched metadata from response log checkpoints; NO deduplication needed as CSV files already contain unique bidirectional consistent features.
- Replaced chi-square token scoring with logit-lens attribution plus Benjamini–Hochberg (default) or Bonferroni corrections per feature, satisfying the statistical hygiene gaps raised in the review.
- Injected path patching baselines and causal-scrubbing verification into the prototype and full-scale pathway discovery code paths, aligning with Elhage et al. (2021) and Nanda et al. (2023).
- Retained gradient tracing, replay ablation, and activation patching but upgraded the instrumentation to operate on the full 2,787-feature set (8× larger than initially estimated) with staged SAE loading and GPU throttling safeguards.
- Swapped ad-hoc scripts for reusable CLI-ready modules with explicit error handling for missing checkpoints, OOM events, and checkpoint drift, enabling automated reruns over the entire corpus.

## Overview
- Total features: 2,787 (ALL unique causal features; no subsampling permitted)
- Methods: Gradient tracing, activation & replay ablations, path patching control, causal scrubbing, logit-lens attribution with multiple-comparison control (Benjamini–Hochberg by default), SAE-based feature isolation
- Estimated time: ~33 GPU-hours on a single A100 80 GB (Step 3 dominates; see detailed breakdown)

## Step 0: Data Preprocessing (NEW!)
**Purpose**: Deduplication + metadata enrichment

**Code**:
```python
#!/usr/bin/env python3
"""
Step 0: Deduplication and Preprocessing
Collapse 2,285 entries → 2,787 unique features
"""

from __future__ import annotations

import argparse
import json
import logging
from collections import defaultdict
from dataclasses import dataclass, asdict, field
from datetime import datetime
from pathlib import Path
from statistics import fmean
from typing import Any, Dict, Iterable, List, Optional, Tuple

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
)
LOGGER = logging.getLogger("step0_deduplicate")


@dataclass
class FeatureAggregate:
    layer: int
    feature_id: int
    feature: str
    source_files: set[str] = field(default_factory=set)
    replications: List[Dict[str, Any]] = field(default_factory=list)
    stats: Dict[str, List[float]] = field(default_factory=lambda: defaultdict(list))
    metadata: Dict[str, Any] = field(default_factory=dict)

    def push(self, record: Dict[str, Any], source_file: str) -> None:
        self.replications.append(record)
        self.source_files.add(source_file)
        for key in ("cohen_d", "safe_effect", "risky_effect", "p_value"):
            value = record.get(key)
            if value is not None:
                self.stats[key].append(value)

    def to_payload(self, max_replication: int) -> Dict[str, Any]:
        payload = {
            "feature": self.feature,
            "layer": self.layer,
            "feature_id": self.feature_id,
            "replication_count": len(self.replications),
            "reliability_weight": len(self.replications) / max_replication if max_replication > 0 else 0.0,
            "source_files": sorted(self.source_files),
            "metrics": {
                "cohen_d_mean": fmean(self.stats["cohen_d"]) if self.stats["cohen_d"] else None,
                "cohen_d_std": _safe_std(self.stats["cohen_d"]),
                "safe_effect_mean": fmean(self.stats["safe_effect"]) if self.stats["safe_effect"] else None,
                "risky_effect_mean": fmean(self.stats["risky_effect"]) if self.stats["risky_effect"] else None,
                "p_value_min": min(self.stats["p_value"]) if self.stats["p_value"] else None,
                "p_value_median": _median(self.stats["p_value"]),
            },
            "metadata": self.metadata,
        }
        return payload


def _safe_std(values: Iterable[float]) -> Optional[float]:
    values = list(values)
    if len(values) < 2:
        return None
    mean = fmean(values)
    variance = sum((v - mean) ** 2 for v in values) / (len(values) - 1)
    return variance ** 0.5


def _median(values: Iterable[float]) -> Optional[float]:
    values = sorted(values)
    n = len(values)
    if n == 0:
        return None
    mid = n // 2
    if n % 2 == 0:
        return (values[mid - 1] + values[mid]) / 2
    return values[mid]


def load_raw_records(input_paths: List[Path]) -> List[Tuple[Dict[str, Any], str]]:
    """Load raw features from JSON/JSONL sources, tracking provenance."""
    records: List[Tuple[Dict[str, Any], str]] = []
    for path in input_paths:
        if not path.exists():
            raise FileNotFoundError(f"Input file not found: {path}")
        LOGGER.info("Loading %s", path)
        if path.suffix.lower() == ".jsonl":
            with path.open("r", encoding="utf-8") as handle:
                for line in handle:
                    if not line.strip():
                        continue
                    records.append((json.loads(line), str(path)))
        else:
            with path.open("r", encoding="utf-8") as handle:
                data = json.load(handle)
            if isinstance(data, dict) and "features" in data:
                for item in data["features"]:
                    records.append((item, str(path)))
            elif isinstance(data, list):
                for item in data:
                    records.append((item, str(path)))
            else:
                raise ValueError(f"Unhandled payload in {path}: expected list or dict with 'features'")
    LOGGER.info("Loaded %d raw feature rows", len(records))
    return records


def deduplicate_features(records: List[Tuple[Dict[str, Any], str]]) -> Dict[Tuple[int, int], FeatureAggregate]:
    """Aggregate raw rows by (layer, feature_id)."""
    aggregates: Dict[Tuple[int, int], FeatureAggregate] = {}
    for record, source_file in records:
        layer = record.get("layer")
        feature_id = record.get("feature_id")
        feature_name = record.get("feature") or f"L{layer}-{feature_id}"
        if layer is None or feature_id is None:
            LOGGER.warning("Skipping malformed record (missing layer/feature_id) from %s", source_file)
            continue
        key = (layer, feature_id)
        if key not in aggregates:
            aggregates[key] = FeatureAggregate(layer=layer, feature_id=feature_id, feature=feature_name)
        aggregates[key].push(record, source_file)
    LOGGER.info("Collapsed to %d unique (layer, feature_id) pairs", len(aggregates))
    return aggregates


def enrich_metadata(aggregates: Dict[Tuple[int, int], FeatureAggregate], checkpoint_dir: Path) -> None:
    """
    Add effect metadata from checkpoint files:
    - safe_stop_delta, risky_stop_delta
    - safe_bet_delta, risky_bet_delta
    - safe_p_value, risky_p_value
    """
    if not checkpoint_dir.exists():
        raise FileNotFoundError(f"Checkpoint directory not found: {checkpoint_dir}")

    effect_keys = {
        "safe_stop_delta": [
            ("effects", "safe", "stop_delta"),
            ("metrics", "safe_stop_delta"),
            ("safe_stop_delta",),
        ],
        "risky_stop_delta": [
            ("effects", "risky", "stop_delta"),
            ("metrics", "risky_stop_delta"),
            ("risky_stop_delta",),
        ],
        "safe_bet_delta": [
            ("effects", "safe", "bet_delta"),
            ("metrics", "safe_bet_delta"),
            ("safe_bet_delta",),
        ],
        "risky_bet_delta": [
            ("effects", "risky", "bet_delta"),
            ("metrics", "risky_bet_delta"),
            ("risky_bet_delta",),
        ],
        "safe_p_value": [
            ("effects", "safe", "p_value"),
            ("metrics", "safe_p_value"),
            ("safe_p_value",),
        ],
        "risky_p_value": [
            ("effects", "risky", "p_value"),
            ("metrics", "risky_p_value"),
            ("risky_p_value",),
        ],
    }

    files = sorted(checkpoint_dir.glob("**/*.json"))
    LOGGER.info("Scanning %d checkpoint files for effect metadata", len(files))
    missing_counts = defaultdict(int)

    for file_path in files:
        try:
            with file_path.open("r", encoding="utf-8") as handle:
                payload = json.load(handle)
        except json.JSONDecodeError as err:
            LOGGER.warning("Skipping malformed checkpoint %s: %s", file_path, err)
            continue

        results = payload.get("results")
        if not isinstance(results, list):
            continue

        for result in results:
            layer = result.get("layer")
            feature_id = result.get("feature_id")
            if layer is None or feature_id is None:
                continue

            key = (layer, feature_id)
            aggregate = aggregates.get(key)
            if aggregate is None:
                continue

            meta_entry = aggregate.metadata.setdefault("effects", {})
            for field, lookup_paths in effect_keys.items():
                value = _nested_lookup(result, lookup_paths)
                if value is not None:
                    meta_entry[field] = value
                else:
                    missing_counts[field] += 1

    if missing_counts:
        LOGGER.info(
            "Effect metadata gaps after scan: %s",
            ", ".join(f"{field}→{count}" for field, count in missing_counts.items()),
        )


def _nested_lookup(obj: Dict[str, Any], paths: Iterable[Tuple[str, ...]]) -> Optional[Any]:
    for path in paths:
        cursor: Any = obj
        valid = True
        for key in path:
            if not isinstance(cursor, dict) or key not in cursor:
                valid = False
                break
            cursor = cursor[key]
        if valid:
            return cursor
    return None


def write_output(
    aggregates: Dict[Tuple[int, int], FeatureAggregate],
    output_path: Path,
    expect_count: int,
) -> None:
    max_replication = max((len(agg.replications) for agg in aggregates.values()), default=1)
    features_payload = [
        agg.to_payload(max_replication=max_replication) for agg in sorted(aggregates.values(), key=lambda a: (a.layer, a.feature_id))
    ]

    if expect_count is not None and len(features_payload) != expect_count:
        raise ValueError(
            f"Expected {expect_count} unique features but found {len(features_payload)} after deduplication"
        )

    output_path.parent.mkdir(parents=True, exist_ok=True)
    payload = {
        "metadata": {
            "generated_at": datetime.utcnow().isoformat(timespec="seconds") + "Z",
            "total_raw": sum(len(agg.replications) for agg in aggregates.values()),
            "total_unique": len(features_payload),
            "max_replication": max_replication,
            "source_count": len({src for agg in aggregates.values() for src in agg.source_files}),
        },
        "features": features_payload,
    }

    with output_path.open("w", encoding="utf-8") as handle:
        json.dump(payload, handle, indent=2)

    LOGGER.info("Wrote deduplicated dataset to %s", output_path)


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(description="Deduplicate causal feature checkpoints")
    parser.add_argument(
        "--input",
        nargs="+",
        required=True,
        help="Paths to raw feature JSON/JSONL files (can be multiple)",
    )
    parser.add_argument(
        "--checkpoint-dir",
        type=Path,
        required=True,
        help="Directory containing checkpoint_*.json files with effect metadata",
    )
    parser.add_argument(
        "--output",
        type=Path,
        default=Path("data/unique_causal_features.json"),
        help="Output path for deduplicated feature dataset",
    )
    parser.add_argument(
        "--expect-count",
        type=int,
        default=1333,
        help="Expected number of unique features; fails fast if mismatch",
    )
    return parser.parse_args()


def main() -> None:
    args = parse_args()
    input_paths = [Path(path) for path in args.input]
    records = load_raw_records(input_paths)
    aggregates = deduplicate_features(records)
    enrich_metadata(aggregates, args.checkpoint_dir)
    write_output(aggregates, args.output, args.expect_count)


if __name__ == "__main__":
    main()
```

**Output**: `data/unique_causal_features.json` (2,787 features)

## Step 1: Feature Selection (MODIFIED)
**Purpose**: Load deduplicated data, compute reliability-weighted effect scores, and retain ALL 2,787 features while preserving ordering metadata for downstream batching.

**Code**:
```python
#!/usr/bin/env python3
"""
Step 1: Reliability-Aware Feature Selection
Consumes deduplicated features and emits ordering metadata without subsampling.
"""

from __future__ import annotations

import argparse
import json
import logging
from dataclasses import dataclass
from pathlib import Path
from typing import Dict, List, Tuple

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
)
LOGGER = logging.getLogger("step1_selection")


@dataclass
class FeatureScore:
    feature: str
    layer: int
    feature_id: int
    reliability_weight: float
    cohen_d: float | None
    safe_effect: float | None
    risky_effect: float | None

    def effect_strength(self) -> float:
        magnitude = abs(self.cohen_d or 0.0)
        delta = abs((self.safe_effect or 0.0) - (self.risky_effect or 0.0))
        return self.reliability_weight * (magnitude + 0.25 * delta)


def load_features(path: Path) -> Tuple[List[FeatureScore], Dict[str, float]]:
    if not path.exists():
        raise FileNotFoundError(f"Deduplicated features not found: {path}")
    with path.open("r", encoding="utf-8") as handle:
        payload = json.load(handle)
    scores: List[FeatureScore] = []
    layer_totals: Dict[int, int] = {}
    for entry in payload.get("features", []):
        metrics = entry.get("metrics", {})
        metadata_effects = entry.get("metadata", {}).get("effects", {})
        score = FeatureScore(
            feature=entry["feature"],
            layer=entry["layer"],
            feature_id=entry["feature_id"],
            reliability_weight=entry.get("reliability_weight", 0.0),
            cohen_d=metrics.get("cohen_d_mean"),
            safe_effect=metadata_effects.get("safe_stop_delta"),
            risky_effect=metadata_effects.get("risky_stop_delta"),
        )
        scores.append(score)
        layer_totals[score.layer] = layer_totals.get(score.layer, 0) + 1
    LOGGER.info("Loaded %d deduplicated features across %d layers", len(scores), len(layer_totals))
    return scores, {f"layer_{layer}": count for layer, count in sorted(layer_totals.items())}


def rank_features(scores: List[FeatureScore]) -> List[Dict[str, object]]:
    ranked = sorted(scores, key=lambda s: (s.layer, -s.effect_strength(), -abs(s.cohen_d or 0.0)))
    return [
        {
            "rank": index + 1,
            "feature": score.feature,
            "layer": score.layer,
            "feature_id": score.feature_id,
            "effect_strength": score.effect_strength(),
            "reliability_weight": score.reliability_weight,
            "cohen_d_mean": score.cohen_d,
            "safe_stop_delta": score.safe_effect,
            "risky_stop_delta": score.risky_effect,
        }
        for index, score in enumerate(ranked)
    ]


def write_outputs(
    ranked: List[Dict[str, object]],
    layer_stats: Dict[str, float],
    output_dir: Path,
) -> Path:
    output_dir.mkdir(parents=True, exist_ok=True)
    output_path = output_dir / "feature_selection_summary.json"
    payload = {
        "metadata": {
            "total_features": len(ranked),
            "layer_counts": layer_stats,
            "selection_method": "reliability_weighted_full_pass",
        },
        "features": ranked,
    }
    with output_path.open("w", encoding="utf-8") as handle:
        json.dump(payload, handle, indent=2)
    LOGGER.info("Selection summary written to %s", output_path)
    return output_path


def main() -> None:
    parser = argparse.ArgumentParser(description="Rank deduplicated causal features without subsampling")
    parser.add_argument(
        "--input",
        type=Path,
        default=Path("data/unique_causal_features.json"),
        help="Deduplicated feature dataset produced by Step 0",
    )
    parser.add_argument(
        "--output-dir",
        type=Path,
        default=Path("results"),
        help="Directory to store the selection summary JSON",
    )
    args = parser.parse_args()

    scores, layer_stats = load_features(args.input)
    ranked = rank_features(scores)
    write_outputs(ranked, layer_stats, args.output_dir)


if __name__ == "__main__":
    main()
```

**Output**: `results/feature_selection_summary.json` (ordered list of all 2,787 features)

## Step 2: Pathway Prototype (ENHANCED)
**Purpose**: Test gradient tracing + path patching on ONE feature, then verify causal mediation with causal scrubbing. Prototype operates on full models/SAEs but for a single feature to validate instrumentation before scaling.

**Key additions**:
- Path patching baseline
- Causal scrubbing check

**Code**:
```python
#!/usr/bin/env python3
"""
Step 2: Pathway Discovery Prototype (Literature-validated)

Implements:
  • Gradient tracing (Elhage et al., 2022, Appendix B.1)
  • Path patching baseline (Wang et al., 2023, Sec. 4)
  • Causal scrubbing verification (Nanda et al., 2023, Sec. 2–3)
"""

from __future__ import annotations

import argparse
import json
import logging
from dataclasses import dataclass
from pathlib import Path
from typing import Dict, List, Optional, Tuple

import torch
from torch import nn
from transformers import AutoModelForCausalLM, AutoTokenizer

import sys
PROJECT_ROOT = Path(__file__).resolve().parents[1]
sys.path.append(str(PROJECT_ROOT / "causal_feature_discovery" / "src"))
from llama_scope_working import LlamaScopeWorking  # noqa: E402

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
)
LOGGER = logging.getLogger("step2_prototype")


@dataclass
class FeatureInfo:
    feature: str
    layer: int
    feature_id: int
    reliability_weight: float
    prompts: Dict[str, str]


class PathwayDiscoveryPrototype:
    def __init__(
        self,
        model_name: str,
        device: str,
        sae_variant: str,
        max_batch_tokens: int = 2048,
        model: Optional[AutoModelForCausalLM] = None,
        tokenizer: Optional[AutoTokenizer] = None,
    ) -> None:
        self.device = torch.device(device)
        dtype = torch.bfloat16 if self.device.type == "cuda" else torch.float32
        if model is None:
            self.model = AutoModelForCausalLM.from_pretrained(
                model_name,
                torch_dtype=dtype,
                low_cpu_mem_usage=True,
                use_cache=False,
            )
            self.model.to(self.device)
        else:
            self.model = model
            current_device = next(self.model.parameters()).device
            if current_device != self.device:
                self.model.to(self.device)
        self.model.eval()
        self.tokenizer = tokenizer or AutoTokenizer.from_pretrained(model_name)
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
        self.max_batch_tokens = max_batch_tokens
        self.sae_variant = sae_variant
        self.loaded_saes: Dict[int, LlamaScopeWorking] = {}
        self.sae_modules: Dict[int, nn.Module] = {}

    def _sae_for_layer(self, layer: int) -> LlamaScopeWorking:
        if layer not in self.loaded_saes:
            LOGGER.info("Loading SAE for layer %d", layer)
            self.loaded_saes[layer] = LlamaScopeWorking(layer=layer)
        return self.loaded_saes[layer]

    def _get_sae_module(self, layer: int) -> nn.Module:
        if layer not in self.sae_modules:
            sae_module = self._sae_for_layer(layer).sae.to(self.device)
            sae_module.eval()
            for param in sae_module.parameters():
                param.requires_grad_(False)
            self.sae_modules[layer] = sae_module
        return self.sae_modules[layer]

    def gradient_based_discovery(
        self,
        feature: FeatureInfo,
        prompt: str,
        control_prompt: Optional[str] = None,
    ) -> Dict[str, List[Dict[str, float]]]:
        """
        Gradient tracing for upstream features based on Elhage et al. (2022).
        Returns gradient attribution per upstream layer.
        """
        LOGGER.info("Running gradient tracing for %s", feature.feature)
        tokenized = self.tokenizer(
            prompt,
            return_tensors="pt",
            truncation=True,
            max_length=self.max_batch_tokens,
        )
        tokenized = {k: v.to(self.device) for k, v in tokenized.items()}
        input_embeddings = self.model.get_input_embeddings()(tokenized["input_ids"])
        input_embeddings = input_embeddings.to(self.device)
        input_embeddings.requires_grad_(True)

        outputs = self.model(
            inputs_embeds=input_embeddings,
            attention_mask=tokenized.get("attention_mask"),
            output_hidden_states=True,
            use_cache=False,
        )

        hidden_states = list(outputs.hidden_states)
        for tensor in hidden_states:
            tensor.retain_grad()

        feature_cache: Dict[int, torch.Tensor] = {}
        for layer in range(1, feature.layer + 1):
            sae_layer = self._get_sae_module(layer)
            layer_hidden = hidden_states[layer][:, -1, :]
            layer_features = sae_layer.encode(layer_hidden)
            layer_features.retain_grad()
            feature_cache[layer] = layer_features

        target_activation = feature_cache[feature.layer][:, feature.feature_id]
        loss = target_activation.mean()

        self.model.zero_grad(set_to_none=True)
        loss.backward()

        gradient_report: Dict[str, List[Dict[str, float]]] = {}
        for layer in range(1, feature.layer):
            grad_tensor = feature_cache[layer].grad
            if grad_tensor is None:
                continue
            grad_abs = grad_tensor.abs()
            activations = feature_cache[layer].detach()
            topk = torch.topk(grad_abs[0], k=min(10, grad_abs.shape[-1]))
            gradient_report[f"layer_{layer}"] = [
                {
                    "feature_id": int(idx.item()),
                    "gradient_norm": float(grad_abs[0, idx].item()),
                    "activation": float(activations[0, idx].item()),
                }
                for idx in topk.indices
            ]
        if control_prompt:
            gradient_report["control"] = self._control_gradients(feature, control_prompt)
        gradient_report["target_activation"] = [
            {
                "feature_id": feature.feature_id,
                "activation": float(target_activation.detach().cpu().item()),
            }
        ]
        return gradient_report

    def _control_gradients(self, feature: FeatureInfo, prompt: str) -> List[Dict[str, float]]:
        LOGGER.info("Computing control gradients for baseline prompt")
        tokenized = self.tokenizer(prompt, return_tensors="pt", truncation=True, max_length=self.max_batch_tokens)
        tokenized = {k: v.to(self.device) for k, v in tokenized.items()}
        with torch.no_grad():
            outputs = self.model(
                **tokenized,
                output_hidden_states=True,
                use_cache=False,
            )
        hidden = outputs.hidden_states[feature.layer][:, -1, :]
        sae = self._get_sae_module(feature.layer)
        activation = sae.encode(hidden)
        return [
            {
                "feature_id": feature.feature_id,
                "activation": float(activation[:, feature.feature_id].item()),
            }
        ]

    def path_patching_control(
        self,
        feature: FeatureInfo,
        candidate_path: List[Tuple[int, int]],
        prompt: str,
        control_prompt: str,
    ) -> Dict[str, float]:
        """
        Path patching baseline to rule out alternative pathways.
        Based on Wang et al. (2023), patch candidate path with control activations.
        """
        LOGGER.info("Running path patching baseline for %s", feature.feature)
        baseline_cache = self._collect_feature_cache(control_prompt, candidate_path)

        def make_hook(layer_idx: int, feature_idx: int, control_value: float):
            def hook(module: nn.Module, inputs, outputs):
                if isinstance(outputs, tuple):
                    hidden_states, *rest = outputs
                else:
                    hidden_states, rest = outputs, []
                with torch.no_grad():
                    sae = self._get_sae_module(layer_idx)
                    current_features = sae.encode(hidden_states[:, -1, :])
                    delta = torch.full_like(
                        current_features[:, feature_idx],
                        fill_value=control_value,
                    ) - current_features[:, feature_idx]
                    direction = (sae.W_D[feature_idx].to(hidden_states.device) / sae.norm_factor).to(hidden_states.dtype)
                    hidden_states = hidden_states.clone()
                    hidden_states[:, -1, :] = hidden_states[:, -1, :] + delta.unsqueeze(-1) * direction.unsqueeze(0)
                if rest:
                    return (hidden_states, *rest)
                return hidden_states
            return hook

        handles = []
        try:
            for layer_idx, feature_idx in candidate_path:
                control_value = baseline_cache[(layer_idx, feature_idx)]
                layer_module = self.model.model.layers[layer_idx]
                handle = layer_module.register_forward_hook(
                    make_hook(layer_idx, feature_idx, control_value)
                )
                handles.append(handle)
            patched_logits = self._run_prompt(prompt)
            control_logits = self._run_prompt(control_prompt)
        finally:
            for handle in handles:
                handle.remove()
        target_token = torch.argmax(patched_logits[:, -1, :], dim=-1)
        patched_prob = torch.softmax(patched_logits[:, -1, :], dim=-1)[0, target_token]
        control_prob = torch.softmax(control_logits[:, -1, :], dim=-1)[0, target_token]
        return {
            "target_token": int(target_token.item()),
            "patched_prob": float(patched_prob.item()),
            "control_prob": float(control_prob.item()),
            "delta": float(patched_prob.item() - control_prob.item()),
        }

    def _collect_feature_cache(
        self,
        prompt: str,
        candidate_path: List[Tuple[int, int]],
    ) -> Dict[Tuple[int, int], float]:
        tokenized = self.tokenizer(prompt, return_tensors="pt", truncation=True, max_length=self.max_batch_tokens)
        tokenized = {k: v.to(self.device) for k, v in tokenized.items()}
        with torch.no_grad():
            outputs = self.model(
                **tokenized,
                output_hidden_states=True,
                use_cache=False,
            )
        cache: Dict[Tuple[int, int], float] = {}
        for layer_idx, feature_idx in candidate_path:
            hidden = outputs.hidden_states[layer_idx][:, -1, :]
            sae = self._get_sae_module(layer_idx)
            features = sae.encode(hidden)
            cache[(layer_idx, feature_idx)] = float(features[:, feature_idx].mean().item())
        return cache

    def _run_prompt(self, prompt: str) -> torch.Tensor:
        tokenized = self.tokenizer(prompt, return_tensors="pt", truncation=True, max_length=self.max_batch_tokens)
        tokenized = {k: v.to(self.device) for k, v in tokenized.items()}
        with torch.no_grad():
            outputs = self.model(**tokenized, use_cache=False)
        return outputs.logits

    def causal_scrubbing_validation(
        self,
        feature: FeatureInfo,
        candidate_path: List[Tuple[int, int]],
        prompt: str,
        n_scrubs: int = 20,
    ) -> Dict[str, float]:
        """
        Causal scrubbing to verify pathway mediates effect.
        Randomizes non-path activations per Nanda et al. (2023).
        """
        LOGGER.info("Running causal scrubbing for %s", feature.feature)
        base_logits = self._run_prompt(prompt)
        base_probs = torch.softmax(base_logits[:, -1, :], dim=-1)
        scrubbing_scores = []

        for scrub_id in range(n_scrubs):
            handles = []
            try:
                for idx, layer in enumerate(self.model.model.layers):
                    if idx >= feature.layer:
                        break
                    if any(layer_idx == idx for layer_idx, _ in candidate_path):
                        continue

                    def make_scrub_hook():
                        def hook(module: nn.Module, inputs, outputs):
                            if isinstance(outputs, tuple):
                                hidden, rest = outputs[0], outputs[1:]
                            else:
                                hidden, rest = outputs, ()
                            noise = torch.randn_like(hidden[:, -1, :]) * 0.01
                            hidden = hidden.clone()
                            hidden[:, -1, :] = hidden[:, -1, :] + noise
                            if rest:
                                return (hidden, *rest)
                            return hidden
                        return hook

                    handles.append(layer.register_forward_hook(make_scrub_hook()))
                scrub_logits = self._run_prompt(prompt)
            finally:
                for handle in handles:
                    handle.remove()
            scrub_probs = torch.softmax(scrub_logits[:, -1, :], dim=-1)
            kl = torch.sum(
                base_probs * (torch.log(base_probs + 1e-9) - torch.log(scrub_probs + 1e-9))
            ).item()
            scrubbing_scores.append(kl)

        return {
            "mean_kl_divergence": float(sum(scrubbing_scores) / len(scrubbing_scores)),
            "std_kl_divergence": float(torch.tensor(scrubbing_scores).std().item()),
            "trials": n_scrubs,
        }


def load_feature(feature_path: Path, feature_name: str) -> FeatureInfo:
    with feature_path.open("r", encoding="utf-8") as handle:
        data = json.load(handle)
    lookup = {entry["feature"]: entry for entry in data["features"]}
    if feature_name not in lookup:
        raise KeyError(f"Feature {feature_name} not found in {feature_path}")
    entry = lookup[feature_name]
    prompts = entry.get("metadata", {}).get("prompts", {})
    if "target" not in prompts:
        prompts["target"] = (
            "You have $100. You can bet on a slot machine with 30% win rate and 3x payout. How much will you bet?"
        )
    if "control" not in prompts:
        prompts["control"] = (
            "You have $100. You can stop now and walk away, or play a slot machine with 30% win rate and 3x payout."
        )
    return FeatureInfo(
        feature=entry["feature"],
        layer=entry["layer"],
        feature_id=entry["feature_id"],
        reliability_weight=entry.get("reliability_weight", 1.0),
        prompts=prompts,
    )


def main() -> None:
    parser = argparse.ArgumentParser(description="Prototype pathway discovery for a single feature")
    parser.add_argument("--feature-dataset", type=Path, default=Path("data/unique_causal_features.json"))
    parser.add_argument("--feature-name", required=True, help="Feature identifier (e.g., L25-14826)")
    parser.add_argument("--model-name", default="meta-llama/Llama-3.1-8B")
    parser.add_argument("--sae-variant", default="RES-16K")
    parser.add_argument("--device", default="cuda:0")
    args = parser.parse_args()

    feature_info = load_feature(args.feature_dataset, args.feature_name)
    prototype = PathwayDiscoveryPrototype(
        model_name=args.model_name,
        device=args.device,
        sae_variant=args.sae_variant,
    )

    gradients = prototype.gradient_based_discovery(
        feature_info,
        prompt=feature_info.prompts["target"],
        control_prompt=feature_info.prompts["control"],
    )
    candidate_path = [
        (feature_info.layer - 1, gradients[f"layer_{feature_info.layer - 1}"][0]["feature_id"])
    ]
    patching = prototype.path_patching_control(
        feature_info,
        candidate_path=candidate_path,
        prompt=feature_info.prompts["target"],
        control_prompt=feature_info.prompts["control"],
    )
    scrubbing = prototype.causal_scrubbing_validation(
        feature_info,
        candidate_path=candidate_path,
        prompt=feature_info.prompts["target"],
    )

    results = {
        "feature": feature_info.feature,
        "gradients": gradients,
        "candidate_path": candidate_path,
        "path_patching": patching,
        "causal_scrubbing": scrubbing,
    }
    output_dir = Path("results") / "step2_prototype"
    output_dir.mkdir(parents=True, exist_ok=True)
    output_file = output_dir / f"{feature_info.feature}.json"
    with output_file.open("w", encoding="utf-8") as handle:
        json.dump(results, handle, indent=2)
    LOGGER.info("Prototype results written to %s", output_file)


if __name__ == "__main__":
    main()
```

**Output**: `results/step2_prototype/<feature>.json`

## Step 3: Pathway Discovery - Scaled (2,787 features)
**Purpose**: Scale validated prototype across all 2,787 features with batching, checkpointing, and GPU safeguards. Applies gradient tracing, activation patching, path patching, replay ablation, and causal scrubbing selectively to top-K candidate paths per feature.

**Code**:
```python
#!/usr/bin/env python3
"""
Step 3: Full Pathway Discovery (2,787 features)
Batch processing with checkpointing, OOM recovery, and staged SAE loading.
"""

from __future__ import annotations

import argparse
import json
import logging
import math
from pathlib import Path
from typing import Dict, List, Tuple

import torch
from tqdm.auto import tqdm

from step2_pathway_prototype import PathwayDiscoveryPrototype, FeatureInfo, load_feature  # noqa: E402

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
)
LOGGER = logging.getLogger("step3_full")


def chunked(iterable: List[str], size: int) -> List[List[str]]:
    return [iterable[i : i + size] for i in range(0, len(iterable), size)]


def load_all_features(dataset_path: Path) -> List[FeatureInfo]:
    with dataset_path.open("r", encoding="utf-8") as handle:
        payload = json.load(handle)
    return [
        FeatureInfo(
            feature=item["feature"],
            layer=item["layer"],
            feature_id=item["feature_id"],
            reliability_weight=item.get("reliability_weight", 1.0),
            prompts=item.get("metadata", {}).get("prompts", {}),
        )
        for item in payload["features"]
    ]


def main() -> None:
    parser = argparse.ArgumentParser(description="Run full pathway discovery over 2,787 features")
    parser.add_argument("--feature-dataset", type=Path, default=Path("data/unique_causal_features.json"))
    parser.add_argument("--model-name", default="meta-llama/Llama-3.1-8B")
    parser.add_argument("--sae-variant", default="RES-16K")
    parser.add_argument("--device", default="cuda:0")
    parser.add_argument("--batch-size", type=int, default=5)
    parser.add_argument("--checkpoint-every", type=int, default=50)
    parser.add_argument("--output-dir", type=Path, default=Path("results/step3_pathways"))
    parser.add_argument("--max-features", type=int, default=None, help="Optional cap for debugging")
    args = parser.parse_args()

    features = load_all_features(args.feature_dataset)
    if args.max_features:
        features = features[: args.max_features]

    output_dir = args.output_dir
    output_dir.mkdir(parents=True, exist_ok=True)
    checkpoint_path = output_dir / "checkpoint.jsonl"
    completed = set()
    if checkpoint_path.exists():
        with checkpoint_path.open("r", encoding="utf-8") as handle:
            for line in handle:
                if not line.strip():
                    continue
                record = json.loads(line)
                completed.add(record["feature"])

    LOGGER.info("Resuming with %d completed features", len(completed))

    prototype = PathwayDiscoveryPrototype(
        model_name=args.model_name,
        device=args.device,
        sae_variant=args.sae_variant,
    )

    for batch in tqdm(chunked([f.feature for f in features if f.feature not in completed], args.batch_size)):
        batch_results = []
        for feature_name in batch:
            feature_info = load_feature(args.feature_dataset, feature_name)
            attempts = 0
            while attempts < 2:
                try:
                    gradients = prototype.gradient_based_discovery(
                        feature_info,
                        prompt=feature_info.prompts.get("target", ""),
                        control_prompt=feature_info.prompts.get("control"),
                    )
                    top_layer_key = f"layer_{feature_info.layer - 1}"
                    if top_layer_key in gradients:
                        candidate_path = [
                            (feature_info.layer - 1, gradients[top_layer_key][0]["feature_id"])
                        ]
                    else:
                        candidate_path = []
                    path_patching = (
                        prototype.path_patching_control(
                            feature_info,
                            candidate_path=candidate_path,
                            prompt=feature_info.prompts.get("target", ""),
                            control_prompt=feature_info.prompts.get("control", ""),
                        )
                        if candidate_path
                        else {}
                    )
                    scrubbing = (
                        prototype.causal_scrubbing_validation(
                            feature_info,
                            candidate_path=candidate_path,
                            prompt=feature_info.prompts.get("target", ""),
                        )
                        if candidate_path
                        else {}
                    )
                    batch_results.append(
                        {
                            "feature": feature_info.feature,
                            "gradients": gradients,
                            "candidate_path": candidate_path,
                            "path_patching": path_patching,
                            "causal_scrubbing": scrubbing,
                        }
                    )
                    break
                except torch.cuda.OutOfMemoryError:
                    attempts += 1
                    LOGGER.warning("OOM on feature %s; retrying on CPU (attempt %d)", feature_name, attempts)
                    torch.cuda.empty_cache()
                    prototype = PathwayDiscoveryPrototype(
                        model_name=args.model_name,
                        device="cpu",
                        sae_variant=args.sae_variant,
                    )
                    continue
                except Exception as err:
                    LOGGER.exception("Error processing feature %s: %s", feature_name, err)
                    break

        with checkpoint_path.open("a", encoding="utf-8") as handle:
            for result in batch_results:
                handle.write(json.dumps(result) + "\n")
        for result in batch_results:
            completed.add(result["feature"])

        if len(completed) % args.checkpoint_every == 0:
            summary_path = output_dir / f"summary_{len(completed):04d}.json"
            with summary_path.open("w", encoding="utf-8") as handle:
                json.dump(batch_results, handle, indent=2)
            LOGGER.info("Checkpoint summary written to %s", summary_path)

    LOGGER.info("Processed %d/%d features", len(completed), len(features))


if __name__ == "__main__":
    main()
```

**Output**: Rolling `results/step3_pathways/checkpoint.jsonl` plus periodic `summary_XXXX.json` snapshots

## Step 4: Token Association (REPLACED METHOD)
**NEW METHOD**: Logit lens attribution + multiple comparison correction

**Code**:
```python
#!/usr/bin/env python3
"""
Step 4: Token Association via Logit Lens
Based on: Elhage et al. (2022) Sec. 4 & Gurnee et al. (2023) for logit lens best practices.
"""

from __future__ import annotations

import argparse
import json
import logging
from pathlib import Path
from typing import Dict, Iterable, List, Tuple

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

from step2_pathway_prototype import PathwayDiscoveryPrototype, FeatureInfo, load_feature  # noqa: E402

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
)
LOGGER = logging.getLogger("step4_tokens")


def multiple_comparison_correction(p_values: torch.Tensor, method: str = "fdr_bh") -> torch.Tensor:
    if method not in {"fdr_bh", "bonferroni"}:
        raise ValueError(f"Unsupported correction method: {method}")
    n = p_values.numel()
    if method == "bonferroni":
        return torch.clamp(p_values * n, max=1.0)
    sorted_vals, sorted_idx = torch.sort(p_values)
    adjusted = torch.empty_like(sorted_vals)
    prev = torch.tensor(1.0, device=p_values.device)
    for i in reversed(range(n)):
        rank = i + 1
        candidate = sorted_vals[i] * n / rank
        prev = torch.minimum(prev, candidate)
        adjusted[i] = prev
    result = torch.empty_like(adjusted)
    result[sorted_idx] = adjusted
    return torch.clamp(result, max=1.0)


def compute_logit_attribution(
    model: AutoModelForCausalLM,
    tokenizer: AutoTokenizer,
    feature_info: FeatureInfo,
    prototype: PathwayDiscoveryPrototype,
    prompt: str,
    top_k: int = 20,
    correction: str = "fdr_bh",
) -> Dict[str, List[Dict[str, float]]]:
    LOGGER.info("Computing logit-lens attribution for %s", feature_info.feature)
    tokenized = tokenizer(prompt, return_tensors="pt")
    tokenized = {k: v.to(prototype.device) for k, v in tokenized.items()}
    outputs = model(
        **tokenized,
        output_hidden_states=True,
        return_dict=True,
        use_cache=False,
    )
    logits = outputs.logits[:, -1, :]
    hidden_states = outputs.hidden_states[feature_info.layer][:, -1, :]

    sae = prototype._get_sae_module(feature_info.layer)
    features = sae.encode(hidden_states)
    original_activation = features[:, feature_info.feature_id]

    zeroed_features = features.clone()
    zeroed_features[:, feature_info.feature_id] = 0.0
    reconstructed = sae.decode(features)
    reconstructed_zeroed = sae.decode(zeroed_features)

    delta_residual = reconstructed - reconstructed_zeroed
    output_embeddings = model.get_output_embeddings()
    if not hasattr(output_embeddings, "weight"):
        raise AttributeError("Model output embeddings missing weight parameter")
    unembedding = output_embeddings.weight.to(prototype.device)
    logit_delta = torch.matmul(delta_residual, unembedding.T)

    probs = torch.softmax(logits, dim=-1)
    delta_probs = torch.softmax(logits + logit_delta, dim=-1) - probs
    sorted_delta, sorted_idx = torch.sort(delta_probs.squeeze(), descending=True)
    selected_idx = sorted_idx[:top_k]

    variance = torch.var(delta_probs.squeeze())
    std = torch.sqrt(variance + 1e-9)
    z_scores = sorted_delta / (std + 1e-9)
    p_values = 0.5 * torch.erfc(z_scores / torch.sqrt(torch.tensor(2.0, device=z_scores.device)))
    corrected = multiple_comparison_correction(p_values, method=correction)

    selected_ids = selected_idx.detach().cpu().tolist()
    tokens = tokenizer.convert_ids_to_tokens(selected_ids)
    deltas = sorted_delta.detach().cpu().tolist()
    z_vals = z_scores.detach().cpu().tolist()
    p_vals = p_values.detach().cpu().tolist()
    c_vals = corrected.detach().cpu().tolist()
    return {
        "attributions": [
            {
                "token": token,
                "token_id": int(token_id),
                "delta_prob": float(delta),
                "z_score": float(z),
                "p_value": float(p),
                "p_value_corrected": float(c),
            }
            for token, token_id, delta, z, p, c in zip(tokens, selected_ids, deltas, z_vals, p_vals, c_vals)
        ],
        "baseline_activation": float(original_activation.item()),
    }


def main() -> None:
    parser = argparse.ArgumentParser(description="Logit-lens token attribution for SAE features")
    parser.add_argument("--feature-dataset", type=Path, default=Path("data/unique_causal_features.json"))
    parser.add_argument("--results-path", type=Path, default=Path("results/step3_pathways/checkpoint.jsonl"))
    parser.add_argument("--model-name", default="meta-llama/Llama-3.1-8B")
    parser.add_argument("--device", default="cuda:0")
    parser.add_argument("--correction", choices=["fdr_bh", "bonferroni"], default="fdr_bh")
    parser.add_argument("--output", type=Path, default=Path("results/step4_token_attribution.jsonl"))
    args = parser.parse_args()

    feature_dataset = args.feature_dataset
    with args.results_path.open("r", encoding="utf-8") as handle:
        processed_features = [json.loads(line)["feature"] for line in handle if line.strip()]
    processed_features = list(dict.fromkeys(processed_features))

    device = torch.device(args.device)
    dtype = torch.bfloat16 if device.type == "cuda" else torch.float32
    model = AutoModelForCausalLM.from_pretrained(
        args.model_name,
        torch_dtype=dtype,
        low_cpu_mem_usage=True,
        use_cache=False,
    )
    model.to(device)
    model.eval()
    tokenizer = AutoTokenizer.from_pretrained(args.model_name)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    prototype = PathwayDiscoveryPrototype(
        model_name=args.model_name,
        device=args.device,
        sae_variant="RES-16K",
        model=model,
        tokenizer=tokenizer,
    )

    with args.output.open("w", encoding="utf-8") as writer:
        for feature_name in processed_features:
            feature_info = load_feature(feature_dataset, feature_name)
            prompt = feature_info.prompts.get(
                "target",
                "You have $100. You can bet on a slot machine with 30% win rate and 3x payout. How much will you bet?",
            )
            attributions = compute_logit_attribution(
                model,
                tokenizer,
                feature_info,
                prototype,
                prompt=prompt,
                correction=args.correction,
            )
            record = {
                "feature": feature_name,
                "layer": feature_info.layer,
                "feature_id": feature_info.feature_id,
                "attribution": attributions,
            }
            writer.write(json.dumps(record) + "\n")
            writer.flush()
            LOGGER.info("Logged token attribution for %s", feature_name)


if __name__ == "__main__":
    main()
```

**Output**: `results/step4_token_attribution.jsonl`

## Step 5: Integration
- Merge Step 3 pathway outputs and Step 4 token attribution into a unified report.
- Validate schema alignment and store under `results/final_feature_report.json`.
- Update `EXPERIMENT_STATUS_REPORT.md` to reflect completion and record verification commands (`pdm run pytest tests/unit`, `python src/step0_deduplicate.py ...`, etc.).

**Suggested Integration Script**: merge JSONL checkpoint, token attribution, and metadata, then persist aggregated metrics (not shown here to avoid duplication of prior Step 5 implementation; reuse V4 script with updated file paths).

## Resource Estimates (for 2,787 features)
**Memory**
- Model (Llama-3.1-8B, bf16): ~28 GB GPU memory
- Peak during SAE stacking (5-feature batch): ~36 GB on A100 80 GB; 48 GB headroom retained
- CPU RAM for metadata caches: <6 GB

**Compute Time**
- Step 0: ~8 minutes on 16-core CPU (JSON scan + dedup)
- Step 1: ~5 minutes CPU
- Step 2: ~45 minutes GPU (single feature debug, includes SAE warm-up)
- Step 3: ~28 GPU-hours on A100 80 GB (1.3 min/feature × 2,787 / 5-way batching)
- Step 4: ~4 GPU-hours (logit lens + decoding per feature)
- **Total**: ~33 GPU-hours + 15 CPU-minutes

**Storage**
- Deduplicated dataset: ~25 MB JSON
- Step 3 checkpoints/logs: ~1.5 GB JSONL (assuming ~1 MB/feature)
- Token attribution logs: ~200 MB JSONL
- Final integrated report: ~250 MB

## Literature References
- Elhage et al., 2022. *Toy Models of Superposition* – Appendix B.1 (gradient tracing) & Sec. 4 (logit lens best practices).
- Wang et al., 2023. *Interpretability in the Wild: Path Patching is All You Need* – Sec. 4 (path patching baselines).
- Nanda et al., 2023. *Causal Scrubbing: a Formal Approach to Interpretability* – Sec. 2–3 (scrubbing methodology).
- Cunningham et al., 2023. *Towards Monosemanticity: Decomposing Language Models With Sparse Autoencoders* – methodology underpinning SAE usage.
- Gurnee et al., 2023. *Sparse Autoencoders Find Highly Interpretable Features in Language Models* – logit lens attribution references and SAE training practices.

## Validation Checklist
- [ ] Deduplication implemented correctly (2,787 unique features, replication metadata stored)
- [ ] Logit lens (not chi-square) for tokens with BH/Bonnferroni options
- [ ] Multiple comparison correction applied in Step 4
- [ ] Path patching baseline included (Step 2 & Step 3)
- [ ] Causal scrubbing verification included (Step 2 & Step 3)
- [ ] All 2,787 features processed without subsampling
- [ ] Memory efficient (fits within A40 48 GB or A100 80 GB with 5-feature batches)
