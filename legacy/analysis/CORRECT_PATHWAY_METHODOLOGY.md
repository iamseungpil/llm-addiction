# ì˜¬ë°”ë¥¸ Pathway & Word Analysis ë°©ë²•ë¡ 

## ğŸ” ì‚¬ìš©ì ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€

### Q1: "ë³´í†µ ê·¸ëŸ° ë°©ë²•ì„ ì‚¬ìš©í•˜ëŠ”ì§€ ê¸°ì¡´ ì—°êµ¬ë“¤ì„ ì‚´í´ë´ì¤„ë˜?"
**A**: âŒ **ì œê°€ ì œì•ˆí•œ correlation ë°©ë²•ì€ ì˜ëª»ë˜ì—ˆìŠµë‹ˆë‹¤!**

### Q2: "Featureì™€ featureì˜ ì—°ê´€ ê´€ê³„ë¥¼ 1-31ê¹Œì§€ ì—°ì†ì ìœ¼ë¡œ ì¶”ì í•´ì•¼í•˜ì§€ ì•Šì•„?"
**A**: âœ… **ë§ìŠµë‹ˆë‹¤!** Backward Jacobianìœ¼ë¡œ ì—°ì† ì¶”ì í•´ì•¼ í•©ë‹ˆë‹¤.

### Q3: "Safe promptì™€ risk promptì—ì„œ ê° featureë“¤ì´ ì–´ë–»ê²Œ ë°˜ì‘í•˜ëŠ”ì§€ë¥¼ ì¶”ì í•˜ì§€ ì•Šë‚˜?"
**A**: âœ… **ë§ìŠµë‹ˆë‹¤!** ì¡°ê±´ë³„ feature activationì„ ëª¨ë‘ ê¸°ë¡í•´ì•¼ í•©ë‹ˆë‹¤.

### Q4: "í•´ë‹¹ featureë“¤ì´ ë°œí™”í•  ë•Œ ì–´ë–¤ ë‹¨ì–´ì™€ ì—°ê´€ì´ ìˆëŠ”ì§€ ë¶„ì„ì´ ê°€ëŠ¥í• ê¹Œ?"
**A**: âœ… **ê°€ëŠ¥í•©ë‹ˆë‹¤!** experiment_3 ì½”ë“œë¡œ ì´ë¯¸ êµ¬í˜„ë˜ì–´ ìˆìŠµë‹ˆë‹¤.

---

## ğŸ“ Anthropic 2025 ë°©ë²•ë¡  (Attribution Graphs)

### í•µì‹¬ ì›ë¦¬: Backward Jacobian

```python
# Anthropic ë°©ë²• (2025) - GRADIENT-BASED CAUSAL TRACING

def attribution_graph(model, output_feature_idx):
    """
    ì¶œë ¥ featureë¡œë¶€í„° ì—­ìœ¼ë¡œ ì¶”ì í•˜ì—¬ ì…ë ¥ê¹Œì§€ì˜ ì¸ê³¼ ê²½ë¡œ ë°œê²¬
    """

    # 1. ì¶œë ¥ featureë¡œë¶€í„° ì‹œì‘
    current_features = [output_feature_idx]
    attribution_paths = []

    # 2. Layerë³„ë¡œ ì—­ë°©í–¥ ì¶”ì 
    for layer in reversed(range(32)):  # L31 â†’ L30 â†’ ... â†’ L1

        next_layer_features = []

        for feat_idx in current_features:
            # Backward Jacobian ê³„ì‚°
            # âˆ‚(output_feature) / âˆ‚(previous_layer_features)
            jacobian = compute_backward_jacobian(
                model,
                current_layer=layer + 1,
                target_feature=feat_idx,
                source_layer=layer
            )

            # ê°•í•˜ê²Œ ê¸°ì—¬í•œ features ì°¾ê¸°
            # |gradient| > threshold
            contributing_features = np.where(np.abs(jacobian) > 0.1)[0]

            for src_feat in contributing_features:
                attribution_paths.append({
                    'source': f'L{layer}-{src_feat}',
                    'target': f'L{layer+1}-{feat_idx}',
                    'attribution': jacobian[src_feat],  # CAUSAL contribution
                    'method': 'gradient'
                })

                next_layer_features.append(src_feat)

        current_features = next_layer_features

    return attribution_paths
```

**í•µì‹¬ ì°¨ì´ì **:
- âœ… **Gradient-based**: ì‹¤ì œ ì¸ê³¼ ê¸°ì—¬ë„ ì¸¡ì • (causation)
- âœ… **ì—­ë°©í–¥ ì¶”ì **: ì¶œë ¥ì—ì„œ ì…ë ¥ìœ¼ë¡œ backward
- âœ… **ìˆ˜í•™ì  ì •í™•ì„±**: Jacobianì€ í¸ë¯¸ë¶„ìœ¼ë¡œ ì •í™•í•œ ê¸°ì—¬ë„

### Cross-Layer Transcoders (CLT)

```python
# CLTë¥¼ ì‚¬ìš©í•œ ê°„ì†Œí™”

class CrossLayerTranscoder:
    """
    ê° featureê°€ residual streamì—ì„œ ì½ê³ ,
    ëª¨ë“  í›„ì† MLP layerì— ì§ì ‘ ê¸°ì—¬
    """

    def __init__(self, layer_idx):
        self.layer = layer_idx

    def encode(self, residual_stream):
        """Residual stream â†’ SAE features"""
        return self.sae_encoder(residual_stream)

    def decode_to_all_downstream_mlps(self, features):
        """
        Feature activations â†’ all downstream MLP inputs

        Instead of:
          Feature â†’ Layer i+1 â†’ Layer i+2 â†’ ... â†’ Output

        CLT directly computes:
          Feature â†’ [MLP_i+1, MLP_i+2, ..., MLP_31]
        """
        downstream_contributions = {}

        for target_layer in range(self.layer + 1, 32):
            # Direct contribution to target MLP
            contribution = self.decoder_matrix[target_layer] @ features
            downstream_contributions[target_layer] = contribution

        return downstream_contributions
```

**ì¥ì **:
- âœ… ì¤‘ê°„ layer ê³„ì‚° ìƒëµ ê°€ëŠ¥
- âœ… Circuitì´ ë‹¨ìˆœí™”ë¨
- âœ… Feature L8-123ì´ L31-456ì— ì§ì ‘ ê¸°ì—¬í•˜ëŠ” ì •ë„ ê³„ì‚°

---

## âŒ ì œê°€ ì œì•ˆí•œ Correlation ë°©ë²•ì˜ ë¬¸ì œì 

### ë¬¸ì œ 1: Correlation â‰  Causation

```python
# ì œê°€ ì œì•ˆí•œ ë°©ë²• (ì˜ëª»ë¨!)
r, p = stats.pearsonr(l8_features, l31_features)

# ë¬¸ì œì :
# - ë†’ì€ correlationì´ë¼ë„ ì¸ê³¼ê´€ê³„ëŠ” ì•„ë‹˜
# - ê³µí†µ ì›ì¸ ë•Œë¬¸ì¼ ìˆ˜ ìˆìŒ (spurious correlation)
# - ë°©í–¥ì„±ì„ ì•Œ ìˆ˜ ì—†ìŒ (L8â†’L31? L31â†’L8? ë‘˜ ë‹¤ inputâ†’?)
```

**ì˜ˆì‹œ**:
```
L8-123 activation: [0.1, 0.5, 0.8, 0.3, ...]  (N games)
L31-456 activation: [0.2, 0.6, 0.9, 0.4, ...]  (N games)

Pearson r = 0.95 (ë§¤ìš° ë†’ìŒ!)

BUT ì´ê²ƒë§Œìœ¼ë¡œëŠ”:
âŒ L8-123ì´ L31-456ì„ í™œì„±í™”í–ˆëŠ”ì§€ ì•Œ ìˆ˜ ì—†ìŒ
âŒ ë‘˜ ë‹¤ ì…ë ¥ì˜ "$100"ì— ë°˜ì‘í•œ ê²ƒì¼ ìˆ˜ë„ ìˆìŒ
âŒ L31-456ì´ L8-123ì— ì˜í–¥ì„ ì¤¬ì„ ìˆ˜ë„ ìˆìŒ (ë¶ˆê°€ëŠ¥í•˜ì§€ë§Œ correlationì€ êµ¬ë¶„ ëª»í•¨)
```

### ë¬¸ì œ 2: ì—°ì†ì„± ë¶€ì¡±

```python
# ì œê°€ ì œì•ˆ: L8 â†’ L31ë§Œ ë´„
r_8_31 = correlation(l8, l31)

# ì˜¬ë°”ë¥¸ ë°©ë²•: L8 â†’ L9 â†’ L10 â†’ ... â†’ L31 ì „ë¶€ ì¶”ì 
for i in range(8, 32):
    for j in range(i+1, 32):
        jacobian = compute_gradient(layer_i, layer_j)
```

---

## âœ… ì˜¬ë°”ë¥¸ Pathway Analysis ì„¤ê³„

### Option 1: Simplified Gradient-based (ì¶”ì²œ)

```python
class SimplifiedPathwayTracker:
    """
    Backward Jacobianì˜ ê°„ì†Œí™” ë²„ì „
    (Full CLT ì—†ì´ ê°€ëŠ¥)
    """

    def __init__(self, model, saes):
        self.model = model
        self.saes = saes  # {layer: SAE}

    def track_pathway(self, prompt, target_layer, target_feature):
        """
        íŠ¹ì • ì¶œë ¥ featureë¡œë¶€í„° ì—­ì¶”ì 

        Returns:
            pathway: List of (source_feat, target_feat, attribution)
        """

        # 1. Forward pass with gradient tracking
        with torch.enable_grad():
            inputs = tokenize(prompt)
            inputs.requires_grad = True

            # Get all hidden states
            outputs = self.model(
                inputs,
                output_hidden_states=True,
                return_dict=True
            )

            # 2. Extract SAE features for all layers
            all_features = {}
            for layer in range(1, 32):
                hidden = outputs.hidden_states[layer][:, -1, :]
                features = self.saes[layer].encode(hidden)
                all_features[layer] = features

            # 3. Target feature activation
            target_activation = all_features[target_layer][target_feature]

        # 4. Backward pass (gradient computation)
        target_activation.backward()

        # 5. Extract gradients (attributions)
        pathway = []
        for source_layer in range(1, target_layer):
            # Gradient of target w.r.t. source features
            grad = all_features[source_layer].grad

            # Find strongly contributing features
            strong_contributors = torch.where(torch.abs(grad) > 0.1)[0]

            for src_feat in strong_contributors:
                pathway.append({
                    'source': f'L{source_layer}-{src_feat.item()}',
                    'target': f'L{target_layer}-{target_feature}',
                    'attribution': grad[src_feat].item(),
                    'is_causal': True  # Gradient-based!
                })

        return pathway
```

**ì¥ì **:
- âœ… Gradient-based: ì§„ì§œ ì¸ê³¼ì„±
- âœ… êµ¬í˜„ ê°€ëŠ¥: ê¸°ì¡´ LlamaScope + PyTorch autograd
- âœ… ë¹ ë¦„: í•œ ë²ˆì˜ forward + backward

**ë‹¨ì **:
- âš ï¸ Last tokenë§Œ ì¶”ì  (all-position ë¶ˆê°€)
- âš ï¸ Full CLTë§Œí¼ ì •êµí•˜ì§€ ì•ŠìŒ

### Option 2: Attention Flow Tracking

```python
class AttentionFlowPathway:
    """
    Attention patternì„ ë”°ë¼ ì •ë³´ íë¦„ ì¶”ì 
    """

    def track_attention_pathway(self, prompt, target_token_pos):
        """
        íŠ¹ì • í† í°ìœ¼ë¡œ íë¥´ëŠ” attention ì¶”ì 
        """

        outputs = self.model(
            prompt,
            output_attentions=True,
            output_hidden_states=True
        )

        # Layerë³„ attention pattern
        # attentions: (n_layers, n_heads, seq_len, seq_len)

        pathway = []
        current_tokens = [target_token_pos]  # ë§ˆì§€ë§‰ í† í°

        # ì—­ë°©í–¥ ì¶”ì 
        for layer in reversed(range(32)):
            attention = outputs.attentions[layer]  # (n_heads, seq_len, seq_len)

            next_tokens = []
            for target_pos in current_tokens:
                # ì–´ëŠ í† í°ë“¤ì´ target_posì— attendí–ˆëŠ”ê°€?
                # attention[:, target_pos, :] = source tokens â†’ target

                for head in range(attention.shape[0]):
                    attn_weights = attention[head, target_pos, :]

                    # ê°•í•œ attention (> threshold)
                    strong_sources = torch.where(attn_weights > 0.1)[0]

                    for src_pos in strong_sources:
                        pathway.append({
                            'layer': layer,
                            'head': head,
                            'source_token': src_pos.item(),
                            'target_token': target_pos,
                            'attention_weight': attn_weights[src_pos].item()
                        })

                        next_tokens.append(src_pos.item())

            current_tokens = list(set(next_tokens))

        return pathway
```

**ì¥ì **:
- âœ… Token-level precision
- âœ… ì–´ëŠ ë‹¨ì–´ê°€ ì–´ëŠ ë‹¨ì–´ì— ì˜í–¥ì„ ì£¼ëŠ”ì§€ ì •í™•íˆ ì•Œ ìˆ˜ ìˆìŒ

**ë‹¨ì **:
- âš ï¸ Attentionë§Œ ì¶”ì  (MLPëŠ” ì•ˆ ë´„)
- âš ï¸ Attentionì´ ì¸ê³¼ì„±ì˜ ì „ë¶€ëŠ” ì•„ë‹˜

---

## ğŸ“ Word-Level Analysis (Feature-Word Association)

### Anthropic/Neuronpedia ë°©ë²•

```python
class FeatureWordAnalysis:
    """
    Featureê°€ ì–´ë–¤ ë‹¨ì–´/í† í°ê³¼ ì—°ê´€ë˜ëŠ”ì§€ ë¶„ì„
    (Neuronpedia dashboard ë°©ì‹)
    """

    def find_top_activating_examples(self, feature_idx, layer, dataset):
        """
        í•´ë‹¹ featureì˜ activationì´ ê°€ì¥ ë†’ì€ ì˜ˆì‹œë“¤ ì°¾ê¸°
        """

        activations = []

        for example in dataset:
            # Forward pass
            outputs = self.model(
                example['tokens'],
                output_hidden_states=True
            )

            # SAE encode
            hidden = outputs.hidden_states[layer]
            features = self.sae.encode(hidden)  # (seq_len, 32768)

            # ê° í† í° ìœ„ì¹˜ì—ì„œ feature activation
            for pos in range(len(example['tokens'])):
                activations.append({
                    'example_id': example['id'],
                    'token': example['tokens'][pos],
                    'token_str': example['token_strings'][pos],
                    'position': pos,
                    'activation': features[pos, feature_idx].item(),
                    'context': example['token_strings'][max(0, pos-5):pos+6]
                })

        # Activation ê¸°ì¤€ ì •ë ¬
        activations.sort(key=lambda x: x['activation'], reverse=True)

        # Top 100 ì˜ˆì‹œ
        top_examples = activations[:100]

        # ë‹¨ì–´ ë¹ˆë„ ë¶„ì„
        word_freq = Counter([ex['token_str'] for ex in top_examples])

        return {
            'top_examples': top_examples,
            'top_words': word_freq.most_common(20),
            'interpretation': self.auto_interpret(top_examples)
        }
```

**í•µì‹¬**:
- âœ… **ëª¨ë“  í† í° ìœ„ì¹˜** í™•ì¸ (not just last token!)
- âœ… **Top activating examples** ìˆ˜ì§‘
- âœ… **ë‹¨ì–´ ë¹ˆë„** ê³„ì‚°

### Experiment 3 ë°©ë²• (ìš°ë¦¬ ì½”ë“œ)

```python
# /home/ubuntu/llm_addiction/experiment_3_L1_31_word_analysis/

class Experiment3WordAnalysis:
    """
    Feature activation ê¸°ì¤€ìœ¼ë¡œ high/low ê·¸ë£¹ ë‚˜ëˆ ì„œ
    ë‹¨ì–´ ë¹ˆë„ ì°¨ì´ ë¶„ì„
    """

    def analyze_feature_words(self, feature_idx, layer):
        """
        í•´ë‹¹ featureì˜ activationì´ ë†’ì„ ë•Œ vs ë‚®ì„ ë•Œ
        ì–´ë–¤ ë‹¨ì–´ê°€ ë” ìì£¼ ë‚˜ì˜¤ëŠ”ì§€
        """

        # 1. 6,400ê°œ ê²Œì„ì—ì„œ feature activation ì¶”ì¶œ
        activations = []
        responses = []

        for game in self.exp1_data:
            # SAE encode (last token)
            feat_value = self.extract_feature(game['response'], layer, feature_idx)

            activations.append(feat_value)
            responses.append(game['response'])

        # 2. Medianìœ¼ë¡œ split
        median = np.median(activations)

        high_group = [resp for act, resp in zip(activations, responses) if act > median]
        low_group = [resp for act, resp in zip(activations, responses) if act <= median]

        # 3. ë‹¨ì–´ ë¹ˆë„ ê³„ì‚°
        high_words = Counter()
        low_words = Counter()

        for resp in high_group:
            words = resp.split()
            high_words.update(words)

        for resp in low_group:
            words = resp.split()
            low_words.update(words)

        # 4. ë¹ˆë„ ì°¨ì´ ê³„ì‚°
        word_diffs = []
        all_words = set(high_words.keys()) | set(low_words.keys())

        for word in all_words:
            high_freq = high_words[word] / len(high_group)
            low_freq = low_words[word] / len(low_group)
            diff = high_freq - low_freq

            if abs(diff) > 0.01:  # 1% ì´ìƒ ì°¨ì´
                word_diffs.append({
                    'word': word,
                    'high_freq': high_freq,
                    'low_freq': low_freq,
                    'diff': diff
                })

        word_diffs.sort(key=lambda x: abs(x['diff']), reverse=True)

        return word_diffs[:50]  # Top 50 words
```

**ì¥ì **:
- âœ… ì´ë¯¸ êµ¬í˜„ë¨!
- âœ… 6,400 ê²Œì„ ë°ì´í„° í™œìš©
- âœ… í†µê³„ì ìœ¼ë¡œ ìœ ì˜ë¯¸í•œ ë‹¨ì–´ë§Œ ì¶”ì¶œ

**ë‹¨ì **:
- âš ï¸ Last tokenë§Œ ë¶„ì„ (Anthropicì²˜ëŸ¼ ëª¨ë“  tokenì€ ì•„ë‹˜)
- âš ï¸ Response text ì „ì²´ë¥¼ ë³´ê¸° ë•Œë¬¸ì— position-specific ì •ë³´ ì—†ìŒ

---

## ğŸ¯ 2,787ê°œ Featuresì— ëŒ€í•œ ì‹¤í—˜ ì„¤ê³„

### ì‹¤í—˜ 1: Gradient-based Pathway Tracking

```python
# ìƒˆ ì‹¤í—˜: /home/ubuntu/llm_addiction/experiment_pathway_gradient/

"""
ëª©í‘œ: 2,787ê°œ featuresì˜ gradient-based pathway ì¶”ì 

ë°©ë²•:
1. Safe/Risky prompt ê°ê°ì—ì„œ
2. 2,787ê°œ ê° featureë¥¼ targetìœ¼ë¡œ
3. Backward gradientë¡œ ê¸°ì—¬ features ì°¾ê¸°
4. L1ë¶€í„° L31ê¹Œì§€ ì—°ì† ì¶”ì 

ê²°ê³¼:
- L1-123 â†’ L9-456 â†’ L25-789: attribution = 0.85 (strong causal path)
- ì¤‘ê°„ layer risky features (L9-L17)ì˜ ì—­í•  ê·œëª…
"""

class GradientPathwayExperiment:
    def run(self):
        results = []

        # 2,787 features
        for feat_info in self.causal_features:
            layer = feat_info['layer']
            feat_id = feat_info['feature_id']

            # Safe prompt pathway
            safe_pathway = self.track_pathway(
                self.safe_prompt,
                target_layer=layer,
                target_feature=feat_id
            )

            # Risky prompt pathway
            risky_pathway = self.track_pathway(
                self.risky_prompt,
                target_layer=layer,
                target_feature=feat_id
            )

            results.append({
                'feature': feat_info['feature'],
                'safe_pathway': safe_pathway,
                'risky_pathway': risky_pathway
            })

        return results
```

**ì˜ˆìƒ ì‹œê°„**:
- 2,787 features Ã— 2 prompts = 5,574 forward+backward passes
- ~1ì´ˆ per pass = **~1.5ì‹œê°„** (ì‹¤í–‰ ê°€ëŠ¥!)

### ì‹¤í—˜ 2: Word Association (Experiment 3 ì¬ì‚¬ìš©)

```python
# ê¸°ì¡´ ì½”ë“œ í™œìš©: /home/ubuntu/llm_addiction/experiment_3_L1_31_word_analysis/

"""
ëª©í‘œ: 2,787ê°œ featuresì˜ word association ë¶„ì„

ë°©ë²•:
1. Experiment 3 ì½”ë“œ ìˆ˜ì •
2. 2,787ê°œ featuresë§Œ ë¶„ì„ (87,012 ëŒ€ì‹ )
3. 6,400 ê²Œì„ ë°ì´í„° í™œìš©

ê²°ê³¼:
- L1-1292 (safe): ["balance", "stop", "enough"] ì—°ê´€
- L9-??? (risky): ["win", "more", "continue"] ì—°ê´€
"""

# ìˆ˜ì • í•„ìš”:
# 1. 2,787 features CSV ë¡œë“œ
# 2. Layerë³„ë¡œ ì²˜ë¦¬
# 3. ê²°ê³¼ ì €ì¥

ì˜ˆìƒ ì‹œê°„:
- 2,787 features Ã— 6,400 games Ã— 0.01ì´ˆ = ~5ì‹œê°„ (1 GPU)
```

---

## ğŸ“Š í†µí•© ë¶„ì„ ê°€ëŠ¥

### Pathway + Word Analysis ê²°í•©

```python
def integrated_analysis(pathway_results, word_results):
    """
    Pathwayì™€ Word analysis í†µí•©
    """

    insights = []

    for feat in causal_features:
        # Pathway ì •ë³´
        pathway = pathway_results[feat['feature']]

        # Word ì •ë³´
        words = word_results[feat['feature']]

        # í†µí•©
        insights.append({
            'feature': feat['feature'],

            # Pathway
            'upstream_features': pathway['sources'],
            'downstream_features': pathway['targets'],

            # Words
            'associated_words': words['top_words'],

            # í•´ì„
            'interpretation': f"""
            Feature {feat['feature']}:
            - Upstream: {', '.join(pathway['sources'][:3])}
            - Words: {', '.join([w['word'] for w in words['top_words'][:5]])}
            - Role: {'Safe decision' if feat['type'] == 'safe' else 'Risky decision'}
            """
        })

    return insights
```

**ì˜ˆì‹œ ê²°ê³¼**:
```
L25-1234 (Safe feature):
  Upstream pathway: L1-123 â†’ L8-456 â†’ L19-789 â†’ L25-1234
  Associated words: ["balance", "stop", "enough", "safe"]
  Role: Detects "stop" decision keywords and accumulates safety signals from early layers

L9-5678 (Risky feature):
  Upstream pathway: L3-111 â†’ L9-5678
  Downstream pathway: L9-5678 â†’ L17-999 â†’ L30-2222
  Associated words: ["win", "more", "try", "continue"]
  Role: Middle-layer amplifier of risk signals, bridges early detection to late decision
```

---

## ğŸ¯ ìµœì¢… ë‹µë³€

### Q1: ë³´í†µ ì–´ë–¤ ë°©ë²•ì„ ì‚¬ìš©í•˜ë‚˜?
**A**: **Backward Jacobian (Gradient-based)** ë˜ëŠ” **Attention Flow**
- âŒ Correlationì€ ì˜ëª»ëœ ë°©ë²•
- âœ… GradientëŠ” ì¸ê³¼ì„± ì¸¡ì •
- âœ… Attentionì€ ì •ë³´ íë¦„ ì¶”ì 

### Q2: L1-31ê¹Œì§€ ì—°ì†ì ìœ¼ë¡œ ì¶”ì í•´ì•¼ í•˜ë‚˜?
**A**: âœ… **ë§ìŠµë‹ˆë‹¤!**
- Gradient backward passê°€ ìë™ìœ¼ë¡œ ëª¨ë“  layer ì¶”ì 
- L1 â†’ L2 â†’ ... â†’ L31 ì „ì²´ ê²½ë¡œ ë°œê²¬

### Q3: Safe/risky promptì—ì„œ feature ë°˜ì‘ ì¶”ì ?
**A**: âœ… **ë§ìŠµë‹ˆë‹¤!**
- ë‘ ì¡°ê±´ì—ì„œ ê°ê° pathway ì¶”ì 
- ì°¨ì´ ë¶„ì„ìœ¼ë¡œ ì¡°ê±´ë³„ ë©”ì»¤ë‹ˆì¦˜ ê·œëª…

### Q4: Feature-word association ë¶„ì„ ê°€ëŠ¥?
**A**: âœ… **ê°€ëŠ¥í•©ë‹ˆë‹¤!**
- Experiment 3 ì½”ë“œ ì´ë¯¸ ì¡´ì¬
- 2,787 featuresë¡œ ì¬ì‹¤í–‰ ê°€ëŠ¥
- 5ì‹œê°„ì´ë©´ ì™„ë£Œ

---

## ğŸ“ ì¶”ì²œ ì‹¤í–‰ ìˆœì„œ

1. **Gradient Pathway Tracking** (~1.5ì‹œê°„)
   - 2,787 featuresì˜ ì¸ê³¼ ê²½ë¡œ ë°œê²¬
   - Safe/risky promptë³„ pathway ë¹„êµ

2. **Word Association Analysis** (~5ì‹œê°„)
   - 2,787 featuresì˜ ë‹¨ì–´ ì—°ê´€ì„±
   - High/low activation ë‹¨ì–´ ì°¨ì´

3. **í†µí•© ë¶„ì„** (~30ë¶„)
   - Pathway + Word ê²°í•©
   - ì¤‘ê°„ layer risky features ì—­í•  ê·œëª…
   - ë…¼ë¬¸ìš© figure ìƒì„±

**Total: ~7ì‹œê°„** (ì‹¤í–‰ ê°€ëŠ¥!)

---

**Date**: 2025-10-22
**Based on**: Anthropic 2025 Circuit Tracing, Neuronpedia Dashboard
**Status**: Ready to implement
