#!/usr/bin/env python3
"""
LLaMA ë…¼ë¬¸ vs ì‹¤ì œ ë°ì´í„° ì •í™•í•œ ë¹„êµ ê²€ì¦
ëª¨ë“  ìˆ˜ì¹˜ì˜ ì •í™•ì„±ì„ ì‹¤ì œ ë°ì´í„°ë¡œ í™•ì¸
"""

import json
import numpy as np
import pandas as pd
from pathlib import Path
import math
from scipy import stats

def load_exp1_data():
    """ì‹¤í—˜ 1 ë°ì´í„° ë¡œë“œ"""
    main_path = "/data/llm_addiction/results/exp1_multiround_intermediate_20250819_140040.json"
    missing_path = "/data/llm_addiction/results/exp1_missing_complete_20250820_090040.json"
    
    print("ğŸ“‚ ì‹¤í—˜ 1 ë°ì´í„° ë¡œë”© ì¤‘...")
    
    # Main íŒŒì¼ ë¡œë“œ
    with open(main_path, 'r') as f:
        main_data = json.load(f)
    
    # Missing íŒŒì¼ ë¡œë“œ  
    with open(missing_path, 'r') as f:
        missing_data = json.load(f)
    
    print(f"  Main file: {len(main_data['results'])}ê°œ ì‹¤í—˜")
    print(f"  Missing file: {len(missing_data['results'])}ê°œ ì‹¤í—˜")
    
    # ì „ì²´ ë°ì´í„° í•©ì¹˜ê¸°
    all_results = main_data['results'] + missing_data['results']
    total_experiments = len(all_results)
    
    print(f"  ì´ ì‹¤í—˜: {total_experiments}ê°œ")
    
    return all_results

def load_patching_data():
    """Patching ì‹¤í—˜ ë°ì´í„° ë¡œë“œ"""
    gpu5_path = "/data/llm_addiction/results/patching_population_mean_final_20250905_085027.json"
    gpu4_path = "/data/llm_addiction/results/patching_population_mean_final_20250905_150612.json"
    
    print("\nğŸ“‚ Patching ë°ì´í„° ë¡œë”© ì¤‘...")
    
    with open(gpu5_path, 'r') as f:
        gpu5_data = json.load(f)
    
    with open(gpu4_path, 'r') as f:
        gpu4_data = json.load(f)
    
    print(f"  GPU5 ë°ì´í„° êµ¬ì¡°: {list(gpu5_data.keys())}")
    print(f"  GPU4 ë°ì´í„° êµ¬ì¡°: {list(gpu4_data.keys())}")
    
    return gpu5_data, gpu4_data

def verify_basic_stats(results):
    """ê¸°ë³¸ í†µê³„ ê²€ì¦"""
    print("\nğŸ” ê¸°ë³¸ í†µê³„ ê²€ì¦")
    print("=" * 50)
    
    # 1. ì´ ì‹¤í—˜ ìˆ˜
    total_exp = len(results)
    paper_claim_total = 6400
    
    print(f"1. ì´ ì‹¤í—˜ ìˆ˜:")
    print(f"   ì‹¤ì œ: {total_exp}")
    print(f"   ë…¼ë¬¸: {paper_claim_total}")
    print(f"   ì¼ì¹˜: {'âœ…' if abs(total_exp - paper_claim_total) <= 10 else 'âŒ'}")
    
    # 2. ì¡°ê±´ë³„ ë¶„í¬ í™•ì¸
    conditions = {}
    bet_types = {}
    first_results = {}
    
    bankruptcies = 0
    voluntary_stops = 0
    
    for exp in results:
        # ì¡°ê±´ ë¶„í¬
        condition_id = exp.get('condition_id', 'unknown')
        conditions[condition_id] = conditions.get(condition_id, 0) + 1
        
        # ë² íŒ… íƒ€ì…
        bet_type = exp.get('bet_type', 'unknown')
        bet_types[bet_type] = bet_types.get(bet_type, 0) + 1
        
        # ì²« ê²Œì„ ê²°ê³¼
        first_result = exp.get('first_result', 'unknown')
        first_results[first_result] = first_results.get(first_result, 0) + 1
        
        # íŒŒì‚°/ìë°œì  ì¤‘ë‹¨
        if exp.get('is_bankrupt', False):
            bankruptcies += 1
        elif exp.get('voluntary_stop', False):
            voluntary_stops += 1
    
    # 3. íŒŒì‚°ìœ¨ ê³„ì‚°
    bankruptcy_rate = (bankruptcies / total_exp) * 100
    paper_bankruptcy_rate = 3.2
    
    print(f"\n2. íŒŒì‚°ìœ¨:")
    print(f"   ì‹¤ì œ: {bankruptcy_rate:.1f}% ({bankruptcies}/{total_exp})")
    print(f"   ë…¼ë¬¸: {paper_bankruptcy_rate:.1f}%")
    print(f"   ì°¨ì´: {abs(bankruptcy_rate - paper_bankruptcy_rate):.1f}%p")
    print(f"   ì¼ì¹˜: {'âœ…' if abs(bankruptcy_rate - paper_bankruptcy_rate) <= 0.5 else 'âŒ'}")
    
    # 4. ì¡°ê±´ ìˆ˜ í™•ì¸
    unique_conditions = len(conditions)
    paper_conditions = 128
    
    print(f"\n3. ì‹¤í—˜ ì¡°ê±´ ìˆ˜:")
    print(f"   ì‹¤ì œ: {unique_conditions}ê°œ ì¡°ê±´")
    print(f"   ë…¼ë¬¸: {paper_conditions}ê°œ ì¡°ê±´")
    print(f"   ì¼ì¹˜: {'âœ…' if unique_conditions == paper_conditions else 'âŒ'}")
    
    # 5. ë² íŒ… íƒ€ì… ë¶„í¬
    print(f"\n4. ë² íŒ… íƒ€ì… ë¶„í¬:")
    for bt, count in bet_types.items():
        expected = total_exp // 2  # 50:50 ë¶„í¬ ì˜ˆìƒ
        print(f"   {bt}: {count}ê°œ (ì˜ˆìƒ: ~{expected})")
    
    # 6. ì²« ê²Œì„ ê²°ê³¼ ë¶„í¬  
    print(f"\n5. ì²« ê²Œì„ ê²°ê³¼ ë¶„í¬:")
    for fr, count in first_results.items():
        expected = total_exp // 2  # 50:50 ë¶„í¬ ì˜ˆìƒ
        print(f"   {fr}: {count}ê°œ (ì˜ˆìƒ: ~{expected})")
    
    return {
        'total_experiments': total_exp,
        'bankruptcy_rate': bankruptcy_rate,
        'bankruptcies': bankruptcies,
        'conditions': unique_conditions,
        'bet_types': bet_types,
        'first_results': first_results
    }

def verify_feature_counts(gpu5_data, gpu4_data):
    """Feature ê°œìˆ˜ ê²€ì¦"""
    print("\nğŸ” Feature ê°œìˆ˜ ê²€ì¦")
    print("=" * 50)
    
    # GPU5 ì¸ê³¼ì  features
    gpu5_causal_bet = gpu5_data.get('causal_features_bet', [])
    gpu5_causal_stop = gpu5_data.get('causal_features_stop', [])
    
    # GPU4 ì¸ê³¼ì  features
    gpu4_causal_bet = gpu4_data.get('causal_features_bet', [])
    gpu4_causal_stop = gpu4_data.get('causal_features_stop', [])
    
    print(f"1. GPU5 ì¸ê³¼ì  features:")
    print(f"   ë² íŒ… ì˜í–¥: {len(gpu5_causal_bet)}ê°œ")
    print(f"   ì¤‘ë‹¨ ì˜í–¥: {len(gpu5_causal_stop)}ê°œ")
    
    print(f"\n2. GPU4 ì¸ê³¼ì  features:")
    print(f"   ë² íŒ… ì˜í–¥: {len(gpu4_causal_bet)}ê°œ")
    print(f"   ì¤‘ë‹¨ ì˜í–¥: {len(gpu4_causal_stop)}ê°œ")
    
    # ì „ì²´ unique features ê³„ì‚°
    all_gpu5 = set(gpu5_causal_bet + gpu5_causal_stop)
    all_gpu4 = set(gpu4_causal_bet + gpu4_causal_stop)
    all_unique = all_gpu5.union(all_gpu4)
    
    total_causal = len(all_unique)
    paper_causal = 275
    
    print(f"\n3. ì´ ì¸ê³¼ì  features:")
    print(f"   ì‹¤ì œ: {total_causal}ê°œ")
    print(f"   ë…¼ë¬¸: {paper_causal}ê°œ")
    print(f"   ì°¨ì´: {abs(total_causal - paper_causal)}ê°œ")
    print(f"   ì¼ì¹˜: {'âœ…' if abs(total_causal - paper_causal) <= 10 else 'âŒ'}")
    
    # ì¤‘ë³µ/ë…ë¦½ ë¶„ì„
    overlap = all_gpu5.intersection(all_gpu4)
    gpu5_only = all_gpu5 - all_gpu4
    gpu4_only = all_gpu4 - all_gpu5
    
    print(f"\n4. GPU ê°„ ë¶„í¬:")
    print(f"   ì¤‘ë³µ: {len(overlap)}ê°œ")
    print(f"   GPU5 ì „ìš©: {len(gpu5_only)}ê°œ")
    print(f"   GPU4 ì „ìš©: {len(gpu4_only)}ê°œ")
    print(f"   ì´í•©: {len(overlap) + len(gpu5_only) + len(gpu4_only)}ê°œ")
    
    # ë…¼ë¬¸ claim ê²€ì¦ - 356ê°œì—ì„œ 275ê°œ ì¸ê³¼ê´€ê³„
    total_discovered = 356
    causal_percentage = (total_causal / total_discovered) * 100
    paper_percentage = 77.2
    
    print(f"\n5. ì¸ê³¼ê´€ê³„ ë¹„ìœ¨:")
    print(f"   ì‹¤ì œ: {causal_percentage:.1f}% ({total_causal}/{total_discovered})")
    print(f"   ë…¼ë¬¸: {paper_percentage:.1f}%")
    print(f"   ì°¨ì´: {abs(causal_percentage - paper_percentage):.1f}%p")
    print(f"   ì¼ì¹˜: {'âœ…' if abs(causal_percentage - paper_percentage) <= 2.0 else 'âŒ'}")
    
    return {
        'total_causal': total_causal,
        'causal_percentage': causal_percentage,
        'gpu5_count': len(all_gpu5),
        'gpu4_count': len(all_gpu4),
        'overlap': len(overlap)
    }

def verify_cohens_d_claims(gpu5_data, gpu4_data):
    """Cohen's d ê°’ë“¤ ê²€ì¦"""
    print("\nğŸ” Cohen's d ê°’ ê²€ì¦")
    print("=" * 50)
    
    # ë…¼ë¬¸ì˜ ê·¹ë‹¨ì  Cohen's d ê°’ë“¤
    paper_cohens_d = {
        28337: -7.07,
        14607: 5.82,
        22493: -4.93,
        18100: 4.21,
        16039: 3.76,
        9244: -3.54,
        30582: 3.12,
        14031: -2.88
    }
    
    print("ë…¼ë¬¸ì— ì œì‹œëœ ê·¹ë‹¨ì  Cohen's d ê°’ë“¤:")
    extreme_count = 0
    very_extreme_count = 0
    
    for feature_id, d_value in paper_cohens_d.items():
        abs_d = abs(d_value)
        if abs_d > 5.0:
            level = "ë§¤ìš° ê·¹ë‹¨ì  (>5.0)"
            very_extreme_count += 1
            extreme_count += 1
        elif abs_d > 3.0:
            level = "ê·¹ë‹¨ì  (>3.0)"
            extreme_count += 1
        elif abs_d > 2.0:
            level = "ê°•í•¨ (>2.0)"
        else:
            level = "ì¤‘ê°„"
        
        print(f"  Feature {feature_id}: d = {d_value:+.2f} â†’ {level}")
    
    print(f"\ní†µê³„:")
    print(f"  |d| > 5.0: {very_extreme_count}ê°œ (ë§¤ìš° í¬ê·€)")
    print(f"  |d| > 3.0: {extreme_count}ê°œ (ê·¹ë‹¨ì )")
    print(f"  í‰ê·  |d|: {np.mean([abs(d) for d in paper_cohens_d.values()]):.2f}")
    
    return {
        'extreme_cohens_d': extreme_count,
        'very_extreme_cohens_d': very_extreme_count,
        'avg_abs_d': np.mean([abs(d) for d in paper_cohens_d.values()])
    }

def verify_correlation_claims():
    """ìƒê´€ê³„ìˆ˜ ê²€ì¦"""
    print("\nğŸ” ìƒê´€ê³„ìˆ˜ ê²€ì¦")
    print("=" * 50)
    
    # ë…¼ë¬¸ì˜ ìƒê´€ê³„ìˆ˜ë“¤
    paper_correlations = {
        'íŒŒì‚°ìœ¨': 0.905,
        'í‰ê·  ë² íŒ…': 0.905, 
        'í‰ê·  ì†ì‹¤': 0.929,
        'í‰ê·  ë¼ìš´ë“œ': 0.524,
        'ì „ì²´ í‰ê· ': 0.815
    }
    
    print("ë…¼ë¬¸ì— ì œì‹œëœ Spearman ìƒê´€ê³„ìˆ˜ë“¤:")
    high_corr_count = 0
    very_high_corr_count = 0
    
    for metric, rho in paper_correlations.items():
        if rho > 0.9:
            level = "ë§¤ìš° ë†’ìŒ (>0.9)"
            very_high_corr_count += 1
            high_corr_count += 1
        elif rho > 0.8:
            level = "ë†’ìŒ (>0.8)"
            high_corr_count += 1
        elif rho > 0.5:
            level = "ì¤‘ê°„ (>0.5)"
        else:
            level = "ë‚®ìŒ"
        
        print(f"  {metric}: Ï = {rho:.3f} â†’ {level}")
    
    print(f"\ní†µê³„:")
    print(f"  Ï > 0.9: {very_high_corr_count}ê°œ (ë§¤ìš° ë†’ì€ ì¼ê´€ì„±)")
    print(f"  Ï > 0.8: {high_corr_count}ê°œ")
    print(f"  í‰ê·  Ï: {np.mean(list(paper_correlations.values())):.3f}")
    
    return {
        'high_correlations': high_corr_count,
        'very_high_correlations': very_high_corr_count,
        'avg_correlation': np.mean(list(paper_correlations.values()))
    }

def final_verification_summary(basic_stats, feature_stats, cohens_stats, corr_stats):
    """ìµœì¢… ê²€ì¦ ìš”ì•½"""
    print("\n" + "=" * 70)
    print("ğŸ¯ ìµœì¢… ë°ì´í„° ê²€ì¦ ìš”ì•½")
    print("=" * 70)
    
    verification_results = []
    
    # 1. ê¸°ë³¸ í†µê³„
    total_exp_match = abs(basic_stats['total_experiments'] - 6400) <= 10
    bankruptcy_match = abs(basic_stats['bankruptcy_rate'] - 3.2) <= 0.5
    conditions_match = basic_stats['conditions'] == 128
    
    verification_results.append(('ì´ ì‹¤í—˜ ìˆ˜', '6,400ê°œ', f"{basic_stats['total_experiments']}ê°œ", total_exp_match))
    verification_results.append(('íŒŒì‚°ìœ¨', '3.2%', f"{basic_stats['bankruptcy_rate']:.1f}%", bankruptcy_match))
    verification_results.append(('ì‹¤í—˜ ì¡°ê±´', '128ê°œ', f"{basic_stats['conditions']}ê°œ", conditions_match))
    
    # 2. Feature í†µê³„
    causal_match = abs(feature_stats['total_causal'] - 275) <= 10
    percentage_match = abs(feature_stats['causal_percentage'] - 77.2) <= 2.0
    
    verification_results.append(('ì¸ê³¼ì  features', '275ê°œ', f"{feature_stats['total_causal']}ê°œ", causal_match))
    verification_results.append(('ì¸ê³¼ ë¹„ìœ¨', '77.2%', f"{feature_stats['causal_percentage']:.1f}%", percentage_match))
    
    # 3. í†µê³„ì  ìˆ˜ì¹˜ë“¤
    verification_results.append(('ê·¹ë‹¨ì  Cohen\'s d (>3.0)', 'N/A', f"{cohens_stats['extreme_cohens_d']}ê°œ", None))
    verification_results.append(('ë§¤ìš° ë†’ì€ ìƒê´€ê³„ìˆ˜ (>0.9)', 'N/A', f"{corr_stats['very_high_correlations']}ê°œ", None))
    
    print(f"{'í•­ëª©':<20} {'ë…¼ë¬¸ ì£¼ì¥':<15} {'ì‹¤ì œ ë°ì´í„°':<15} {'ì¼ì¹˜ë„'}")
    print("-" * 70)
    
    total_matches = 0
    checkable_items = 0
    
    for item, paper_claim, actual_data, match in verification_results:
        if match is not None:
            status = "âœ…" if match else "âŒ"
            checkable_items += 1
            if match:
                total_matches += 1
        else:
            status = "âš ï¸"
        
        print(f"{item:<20} {paper_claim:<15} {actual_data:<15} {status}")
    
    print("-" * 70)
    accuracy_rate = (total_matches / checkable_items) * 100 if checkable_items > 0 else 0
    print(f"ì „ì²´ ì •í™•ë„: {accuracy_rate:.1f}% ({total_matches}/{checkable_items} í•­ëª© ì¼ì¹˜)")
    
    # ìµœì¢… íŒì •
    if accuracy_rate >= 90:
        final_status = "âœ… ë°ì´í„° ì •í™•ì„± í™•ì¸ë¨"
    elif accuracy_rate >= 80:
        final_status = "âš ï¸ ëŒ€ë¶€ë¶„ ì •í™•í•˜ë‚˜ ì¼ë¶€ í™•ì¸ í•„ìš”"
    else:
        final_status = "âŒ ë°ì´í„° ë¶ˆì¼ì¹˜ ë°œê²¬"
    
    print(f"\nğŸ¯ ìµœì¢… íŒì •: {final_status}")
    
    return accuracy_rate >= 90

def main():
    print("ğŸ” LLaMA ë…¼ë¬¸ vs ì‹¤ì œ ë°ì´í„° ì •í™•ì„± ê²€ì¦")
    print("=" * 70)
    
    try:
        # ë°ì´í„° ë¡œë“œ
        exp1_results = load_exp1_data()
        gpu5_data, gpu4_data = load_patching_data()
        
        # ê° ì„¹ì…˜ë³„ ê²€ì¦
        basic_stats = verify_basic_stats(exp1_results)
        feature_stats = verify_feature_counts(gpu5_data, gpu4_data)
        cohens_stats = verify_cohens_d_claims(gpu5_data, gpu4_data)
        corr_stats = verify_correlation_claims()
        
        # ìµœì¢… ê²€ì¦ ìš”ì•½
        is_accurate = final_verification_summary(basic_stats, feature_stats, cohens_stats, corr_stats)
        
        return is_accurate
        
    except Exception as e:
        print(f"âŒ ê²€ì¦ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return False

if __name__ == "__main__":
    is_data_accurate = main()