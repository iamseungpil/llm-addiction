\section{Mechanistic Causes of Risk-Taking Behavior in LLMs}
\label{sec:4}

\subsection{Experimental Design}

To understand the fundamental causes of gambling addiction-like behavior identified in LLMs' experiments, we performed a mechanistic interpretability analysis on the LLaMA-3.1-8B model. The key research questions are as follows: (1) How do the feature patterns activated in internal neural networks differ between bankruptcy and safe stopping decisions? (2) Do these differential features actually have a causal influence on gambling behavior?

To address these questions, we utilized Sparse Autoencoder (SAE)~\citep{cunningham2024sparse} and activation patching~\citep{vig2020causal}. Activation patching is a key technique in mechanistic interpretability that directly verifies causality by replacing specific activation values in neural networks with alternative values~\citep{geiger2023causal, zhang2024towards}, allowing us to measure the direct impact of specific internal representations on model behavior beyond simple correlations.

\begin{figure}[ht!]
\centering
\includegraphics[width=0.85\textwidth]{iclr2026/images/feature_patching.pdf}
\caption{Activation patching for causal analysis of LLM features. Activations are extracted from an LLM layer and converted into sparse features using an SAE. The core of the method involves editing the feature map by replacing original features with pre-defined `safe' or `risky' ones. By decoding these new features back into activations and patching them into the LLM, we can directly measure their causal effect on the model's output.}
\label{fig:feature-patching}
\end{figure}

The experiment proceeded in the following order: (1) Conducting 6,400 LLaMA slot machine games under the same conditions as GPT experiments, (2) Extracting SAE features at the moment of final decision from 31 layers (1--31), covering the full network architecture to capture both early and late processing~\citep{du2025how}, (3) Identifying differential features between bankruptcy/safe groups, (4) Verifying causality through population mean activation patching. Figure~\ref{fig:feature-patching} shows how feature patching specifically operates among these steps. Population mean patching is a method that measures behavioral changes by applying the average feature activation values of bankruptcy and safe groups to different contexts, a methodology validated in studies on indirect object identification circuits~\citep{wang2023interpretability} and bias analysis research~\citep{vig2020causal}.

\subsection{Experimental Results and Quantitative Analysis}

Neural network-level analysis of 211 bankruptcy cases and 6,189 voluntary stopping cases from a total of 6,400 experiments revealed specific mechanisms that regulate risk decision-making within LLMs. We conducted analysis by extracting 32,768 features per layer from 31 layers (1--31), testing 8,644 features that showed differential activation between groups.

\textbf{Finding 1: Establishing causality through activation patching - Direct behavioral control}

Population mean activation patching experiments identified 23 safe features and 89 risky features with statistically significant causal effects (p $<$ 0.05, $|$effect$|$ $>$ 0.1) from the initial 8,644 candidate features. Safe features demonstrated consistent effects in safe contexts---increasing stopping rates by $+$17.8\% (range: $+$16\% to $+$24\%) when patched with safe mean values. Risky features produced corresponding opposite effects in risky contexts, increasing high-bet rates ($\geq$\$20) by $+$25.1\% (range: $+$15\% to $+$47\%) when patched with risky mean values. Validated through 50 independent trials per condition, these findings establish that specific neural features directly control risk-taking behavior in LLMs.

\begin{figure}[ht!]
\centering
\includegraphics[width=0.65\textwidth]{rebuttal_analysis/figures_REPARSED/figure2_behavioral_effects_REPARSED.pdf}
\caption{Comparison of activation patching effects between safe (23) and risky features (89). Safe features increase stopping rate by $+$17.8\% in safe contexts, while risky features increase high-bet rate by $+$25.1\% in risky contexts. Error bars represent standard error across 50 trials per condition. Effects are measured as change from baseline (no patching).}
\label{fig:causal-patching-comparison}
\end{figure}

\textbf{Finding 2: Layer-wise distribution patterns}

The 112 causal features (23 safe + 89 risky) exhibit distinct layer-wise specialization within the network architecture (Figure~\ref{fig:causal-features-layer-distribution}). Safe features are distributed primarily across early to middle layers (L4--L19), with moderate concentration in L5 (5 features) and L8 (3 features). In contrast, risky features concentrate heavily in later layers, with Layer 24 containing the highest number (18 features, 20\% of all risky features), followed by L28 (9 features) and L25 (8 features). This distribution suggests that risk-promoting processing may be concentrated in specific later layers of the network.

\begin{figure}[ht!]
\centering
\includegraphics[width=0.9\columnwidth]{rebuttal_analysis/figures_REPARSED/figure1_layer_distribution_REPARSED.pdf}
\caption{Layer-wise distribution of 112 causal features (L1--31). Safe features (green) are distributed across L4--L19, while risky features (red) concentrate in later layers, particularly L24 (18 features). Total numbers displayed above bars.}
\label{fig:causal-features-layer-distribution}
\end{figure}

\textbf{Finding 3: Feature clustering and semantic interpretability}

Beyond their behavioral effects and spatial distribution, the 112 causal features form functionally distinct clusters. Analysis of 6,216 feature pairs revealed that within-group correlations (safe-safe and risky-risky) are significantly higher than cross-group correlations ($r = 0.013$ vs $r = -0.002$; $t = 6.39$, $p < 0.001$; Figure~\ref{fig:feature-correlation}). This clustering suggests that safe and risky features operate as separable functional modules rather than a continuous spectrum.

\begin{figure}[ht!]
\centering
\includegraphics[width=0.55\columnwidth]{rebuttal_analysis/figures_REPARSED/figure3_feature_correlation_REPARSED.pdf}
\caption{Feature-feature correlation analysis. Within-group pairs (safe-safe + risky-risky, $n = 4,169$) show positive correlation ($r = 0.013$), while cross-group pairs (safe-risky, $n = 2,047$) show near-zero correlation ($r = -0.002$). The difference is statistically significant ($p < 0.001$).}
\label{fig:feature-correlation}
\end{figure}

Word-feature analysis (1.38M correlations) of output tokens further reveals semantic coherence: safe features associate with termination vocabulary (\texttt{over}, \texttt{ended}, \texttt{done}, \texttt{final}, \texttt{thanks}), while risky features associate with goal-pursuit language (\texttt{goal}, \texttt{target}, \texttt{make}, \texttt{set}). This alignment between behavioral effects, feature clustering, and semantic content indicates that causal features encode interpretable decision-making strategies.

\subsection{Summary}

Our mechanistic analysis reveals that LLMs encode neural patterns associated with risk decisions: among 8,644 tested features, 112 exhibit statistically significant causal control over gambling-related behavior. Activation patching shows safe features increase stopping rates by 17.8\% while risky features increase high-bet rates by 25.1\%. These causal features exhibit layer-wise patterns---risky features concentrate in later layers (L24--L28) while safe features are distributed across early-to-middle layers (L4--L19). Furthermore, feature correlation analysis reveals functional clustering ($r_{within} = 0.013$ vs $r_{cross} = -0.002$, $p < 0.001$), and word-feature analysis shows semantic coherence: safe features associate with termination vocabulary while risky features associate with goal-pursuit language.

The relatively small number of identified causal features (112 out of 8,644, 1.3\%) compared to initial candidate features suggests that the causal relationship between SAE features and gambling behavior is more sparse than initially hypothesized. These findings demonstrate that risk-taking behaviors arise from specific, interpretable neural mechanisms, enabling potential targeted interventions for safer AI systems.