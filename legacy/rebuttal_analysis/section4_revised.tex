\section{Mechanistic Causes of Risk-Taking Behavior in LLMs}
\label{sec:4}

\subsection{Experimental Design}

To understand the fundamental causes of gambling addiction-like behavior identified in LLMs' experiments, we performed a mechanistic interpretability analysis on the LLaMA-3.1-8B model. The key research questions are as follows: (1) How do the feature patterns activated in internal neural networks differ between bankruptcy and safe stopping decisions? (2) Do these differential features actually have a causal influence on gambling behavior?

To address these questions, we utilized Sparse Autoencoder (SAE)~\citep{cunningham2024sparse} and activation patching~\citep{vig2020causal}. Activation patching is a key technique in mechanistic interpretability that directly verifies causality by replacing specific activation values in neural networks with alternative values~\citep{geiger2023causal, zhang2024towards}, allowing us to measure the direct impact of specific internal representations on model behavior beyond simple correlations.


\begin{figure}[ht!]
\centering
\includegraphics[width=0.85\textwidth]{iclr2026/images/feature_patching.pdf}
\caption{Activation patching for causal analysis of LLM features. Activations are extracted from an LLM layer and converted into sparse features using an SAE. The core of the method involves editing the feature map by replacing original features with pre-defined `safe' or `risky' ones. By decoding these new features back into activations and patching them into the LLM, we can directly measure their causal effect on the model's output.}
\label{fig:feature-patching}
\end{figure}

The experiment proceeded in the following order: (1) Conducting 6,400 LLaMA slot machine games under the same conditions as GPT experiments, (2) Extracting SAE features at the moment of final decision from 30 layers (1--30), covering the full network architecture to capture both early and late processing~\citep{du2025how}, (3) Identifying differential features between bankruptcy/safe groups, (4) Verifying causality through population mean activation patching. Figure~\ref{fig:feature-patching} shows how feature patching specifically operates among these steps. Population mean patching is a method that measures behavioral changes by applying the average feature activation values of bankruptcy and safe groups to different contexts, a methodology validated in studies on indirect object identification circuits~\citep{wang2023interpretability} and bias analysis research~\citep{vig2020causal}.

\subsection{Experimental Results and Quantitative Analysis}

Neural network-level analysis of 211 bankruptcy cases and 6,189 voluntary stopping cases from a total of 6,400 experiments revealed specific mechanisms that regulate risk decision-making within LLMs. We conducted analysis by extracting 32,768 features per layer from 30 layers (1--30), testing 9,000 activated features in total.

\textbf{Finding 1: Establishing causality through activation patching - Direct behavioral control}

Population mean activation patching experiments identified 640 safe features and 2,147 risky features with consistent bidirectional effects across L1--30 from the initial 2,787 causal features. Despite being outnumbered, safe features demonstrated protective effects across both safe and risky contexts---increasing stopping rates ($+$9.1\%, $+$8.7\%) and reducing bankruptcy ($-$19.4\%) (Figure~\ref{fig:causal-patching-comparison}). Risky features produced substantially stronger opposite effects, decreasing stopping rates ($-$41.3\%, $-$41.5\%) and increasing bankruptcy ($+$17.0\%). Validated through 30 independent trials per condition, these findings establish that specific neural features directly control risk-taking behavior in LLMs, transcending mere correlational patterns. This causal control suggests that targeted feature interventions could prevent harmful risk-taking behaviors in deployed AI systems.

\begin{figure}[ht!]
\centering
\includegraphics[width=0.65\textwidth]{rebuttal_analysis/figures/figure2_behavioral_effects.pdf}
\caption{Comparison of activation patching effects between safe (640) and risky features (2,147) from 2,787 causal features identified across L1--30. Left: In safe contexts, safe feature patching increases the stopping rate by $+$9.1\%, while risky feature patching decreases it by $-$41.3\%. Middle: In risky contexts, safe features increase stopping rate by $+$8.7\%, while risky features decrease it by $-$41.5\%. Right: In risky contexts, safe features decrease bankruptcy rate by $-$19.4\%, while risky features increase it by $+$17.0\%. Error bars represent standard error. All effects show consistent bidirectional patterns, with safe features promoting cautious behavior and risky features encouraging continued gambling across both contexts.}
\label{fig:causal-patching-comparison}
\end{figure}

\begin{figure}[ht!]
\centering
\includegraphics[width=0.9\columnwidth]{rebuttal_analysis/figures/figure1_layer_distribution.pdf}
\caption{Layer-wise distribution of 2,787 causal features (L1--30). Safe features (green) and risky features (red) are shown stacked, with total numbers displayed above bars. The distribution reveals distinct layer-wise specialization: risky features concentrate in middle layers (L5--L18, peaking at L9 with 272 features), while safe features predominate in late layers (L25--L29, with 108--110 features each). This suggests that risky decision pathways are processed in middle network layers, whereas safe decision-making is primarily handled in later layers.}
\label{fig:causal-features-layer-distribution}
\end{figure}

\textbf{Finding 2: Layer-wise distribution patterns}

The 2,787 causal features exhibit distinct layer-wise specialization within the network architecture (Figure~\ref{fig:causal-features-layer-distribution}). Risky features concentrate in middle layers (5--18), peaking at Layer 9 with 272 features, while safe features predominate in late layers (25--29), with 108--110 features each. Despite being outnumbered overall (640 safe vs 2,147 risky), safe features' concentration in final decision layers suggests a late-stage safety mechanism that can override earlier risk-promoting signals when activated sufficiently.

\textbf{Finding 3: Feature-feature correlation patterns}

Beyond individual feature effects, we analyzed the correlation structure among causal features to understand their collective organization (Figure~\ref{fig:feature-correlation}). Within-group correlations are significantly higher than cross-group correlations: safe features show moderate positive correlation with each other ($r = 0.041$, $p < 0.001$), as do risky features ($r = 0.026$, $p < 0.01$). In contrast, cross-group correlations between safe and risky features are weaker ($r = 0.018$). This pattern suggests that safe and risky features form distinct functional clusters rather than a continuous spectrum, supporting the hypothesis that LLMs encode separable neural pathways for risk-promoting and risk-averse decision-making.

\begin{figure}[ht!]
\centering
\includegraphics[width=0.6\columnwidth]{rebuttal_analysis/figures/figure3_feature_correlation.pdf}
\caption{Feature-feature correlation analysis by category. Within-group correlations (Safe-Safe: $r = 0.041$, Risky-Risky: $r = 0.026$) are higher than cross-group correlations (Safe-Risky: $r = 0.018$), indicating that safe and risky features form distinct functional clusters within the network.}
\label{fig:feature-correlation}
\end{figure}

\textbf{Finding 4: Word-feature association patterns}

To interpret what these causal features represent semantically, we analyzed the association between output words and feature activations (Figure~\ref{fig:word-feature-association}). Safe features show strong activation when the model outputs words related to cautious behavior: ``stop,'' ``\$105'' (moderate amounts), ``see,'' ``easy,'' and ``designed.'' These words reflect conservative decision-making and awareness of game mechanics. In contrast, risky features activate strongly with goal-oriented and loss-chasing language: ``target,'' ``goal,'' ``loss,'' ``make,'' and ``\$200'' (high target amounts). This semantic analysis reveals that the causal features encode interpretable behavioral patterns---safe features capture cautious, observational reasoning while risky features capture goal-fixation and loss-recovery strategies commonly associated with gambling behavior.

\begin{figure}[ht!]
\centering
\includegraphics[width=0.85\columnwidth]{rebuttal_analysis/figures/figure4_word_feature_association.pdf}
\caption{Word-feature associations in LLM responses. Left: Words associated with safe features reflect cautious behavior (``stop,'' moderate amounts). Right: Words associated with risky features reflect goal-fixation and loss-chasing (``target,'' ``goal,'' ``\$200''). These associations provide semantic interpretability for the causal features, showing that safe and risky pathways encode distinct behavioral strategies.}
\label{fig:word-feature-association}
\end{figure}

\subsection{Summary}

Our mechanistic analysis reveals that LLMs encode distinct neural patterns for risk decisions: among 9,000 tested features, 2,787 exhibit consistent bidirectional causal control over gambling behavior. Activation patching shows safe features reduce bankruptcy by 19.4\% while risky features increase it by 17.0\%. These causal features exhibit layer-wise specialization---risky features concentrate in middle layers (5--18) while safe features cluster in late layers (25--29).

Additionally, feature-feature correlation analysis reveals that safe and risky features form distinct functional clusters ($r_{within} > r_{cross}$), suggesting separable neural pathways for risk-promoting and risk-averse processing. Word-feature association analysis provides semantic interpretability: safe features encode cautious behavior (``stop,'' ``see'') while risky features encode goal-fixation and loss-chasing (``target,'' ``goal,'' ``loss'').

The numerical predominance of risky features (2,147 vs 640) combined with their concentration in middle layers suggests that gambling behavior arises from extensive middle-layer processing, which can be overridden by concentrated late-layer safety mechanisms. These findings demonstrate that risk-taking behaviors arise from specific, interpretable neural mechanisms, enabling targeted interventions for safer AI systems.
