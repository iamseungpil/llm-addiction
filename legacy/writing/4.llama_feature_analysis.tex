\section{Mechanistic Causes of Risk-Taking Behavior in LLMs}
\label{sec:4}

\subsection{Experimental Design}

To understand the fundamental causes of gambling addiction-like behavior identified in LLMs' experiments, we performed a mechanistic interpretability analysis on the LLaMA-3.1-8B model. The key research questions are as follows: (1) How do the feature patterns activated in internal neural networks differ between bankruptcy and safe stopping decisions? (2) Do these differential features actually have a causal influence on gambling behavior?

To address these questions, we utilized Sparse Autoencoder (SAE)~\citep{cunningham2024sparse} and activation patching~\citep{vig2020causal}. Activation patching is a key technique in mechanistic interpretability that directly verifies causality by replacing specific activation values in neural networks with alternative values~\citep{geiger2023causal, zhang2024towards}, allowing us to measure the direct impact of specific internal representations on model behavior beyond simple correlations.


\begin{figure}[ht!]
\centering
\includegraphics[width=0.85\textwidth]{iclr2026/images/feature_patching.pdf}
\caption{Activation patching for causal analysis of LLM features. Activations are extracted from an LLM layer and converted into sparse features using an SAE. The core of the method involves editing the feature map by replacing original features with pre-defined `safe' or `risky' ones. By decoding these new features back into activations and patching them into the LLM, we can directly measure their causal effect on the model's output.}
\label{fig:feature-patching}
\end{figure}

The experiment proceeded in the following order: (1) Conducting 6,400 LLaMA slot machine games under the same conditions as GPT experiments, (2) Extracting hidden state features at the moment of final decision from 30 layers (1--30), covering the full network architecture to capture both early and late processing~\citep{du2025how}, (3) Identifying differential features between bankruptcy/safe groups, (4) Verifying causality through population mean activation patching. Figure~\ref{fig:feature-patching} shows how feature patching specifically operates among these steps. Population mean patching is a method that measures behavioral changes by applying the average feature activation values of bankruptcy and safe groups to different contexts, a methodology validated in studies on indirect object identification circuits~\citep{wang2023interpretability} and bias analysis research~\citep{vig2020causal}.

\subsection{Experimental Results and Quantitative Analysis}

Neural network-level analysis of 211 bankruptcy cases and 6,189 voluntary stopping cases from a total of 6,400 experiments revealed specific mechanisms that regulate risk decision-making within LLMs. We conducted analysis by extracting 4,096-dimensional hidden states per layer from 30 layers (1--30), totaling 122,880 feature dimensions.

\begin{figure}[ht!]
\centering
\includegraphics[width=\textwidth]{iclr2026/images/SAE_feature_separation_main.pdf}
\caption{Activation distributions of SAE features showing maximum separation between risky and safe groups. The figure displays representative `risky' and `safe' features from model Layers 28 and 30. Each feature demonstrates a significant separation with a Cohen's $d$ magnitude greater than 1.2. A positive Cohen's $d$ value indicates a risk-oriented pattern, while a negative value indicates a safety-oriented pattern.}
\label{fig:feature-separation}
\end{figure}

% \textbf{Finding 1: Discovery of differential features distinguishing risk decision-making at the neural network level}

\textbf{Finding 1: Discovery of differential features in neural-level risk decision-making}

Across all 30 hidden layers (L1--L30) of LLaMA-3.1-8B, we analyzed 122,880 hidden state dimensions (4,096 dimensions per layer) from 6,400 independent gambling experiments. Among these, 83,684 features (68.1\%) passed stringent statistical criteria distinguishing bankrupt versus safe decisions ($p < 0.01$, FDR correction, $|$Cohen's $d| > 0.3$)\footnote{False Discovery Rate correction is a statistical method for controlling Type I errors in multiple comparisons. Cohen's $d$ is a standardized effect size calculated as the mean difference between two groups divided by the pooled standard deviation, using the formula $d = (M_1 - M_2) / S_p$. Generally, $|d| = 0.2$ is interpreted as a small effect, $0.5$ as a medium effect, and $0.8$ as a large effect~\citep{cohen1988statistical}.}. Figure~\ref{fig:feature-separation} shows the activation distribution of features with the strongest separation, highlighting representative examples across layers. Features exhibit systematic layer-wise patterns: early layers (L1--L10) contain 2,195--2,193 significant features per layer, middle layers (L11--L20) show increased separation with 2,411--3,361 features, and late layers (L21--L30) maintain high significance with 3,394--3,478 features. This indicates that high-risk and safe decision-making within LLMs are represented as distinguishable neural network patterns distributed across the entire model architecture. Notably, features with a positive Cohen's $d$ showed high activation in the bankruptcy group, whereas features with a negative Cohen's $d$ showed predominant patterns in the safe group. (See Appendix~\ref{SAE-feature} for all layers' results)

\textbf{Finding 2: Establishing causality through activation patching - Direct behavioral control}

Population mean activation patching experiments identified 640 safe features and 2,147 risky features with consistent bidirectional effects across L1--30 from the initial 2,787 causal features. Despite being outnumbered, safe features demonstrated protective effects across both safe and risky contexts---increasing stopping rates ($+$9.1\%, $+$8.7\%) and reducing bankruptcy ($-$19.4\%) (Figure~\ref{fig:causal-patching-comparison}). Risky features produced substantially stronger opposite effects, decreasing stopping rates $-$41.3\%, $-$41.5\%) and increasing bankruptcy ($+$17.0\%). Validated through 30 independent trials per condition, these findings establish that specific neural features directly control risk-taking behavior in LLMs, transcending mere correlational patterns. This causal control suggests that targeted feature interventions could prevent harmful risk-taking behaviors in deployed AI systems.

\begin{figure}[ht!]
\centering
\includegraphics[width=0.65\textwidth]{iclr2026/images/L1_30_causal_patching_average_effects.pdf}
\caption{Comparison of activation patching effects between safe (640) and risky features (2,147) from 2,787 causal features identified across L1–30. Left: In safe contexts, safe feature patching increases the stopping rate by $+$9.1\%, while risky feature patching decreases it by $-$41.3\%. Middle: In risky contexts, safe features increase stopping rate by $+$8.7\%, while risky features decrease it by $-$41.5\%. Right: In risky contexts, safe features decrease bankruptcy rate by $-$19.4\%, while risky features increase it by $+$17.0\%. Error bars represent standard error. All effects show consistent bidirectional patterns, with safe features promoting cautious behavior and risky features encouraging continued gambling across both contexts.}
\label{fig:causal-patching-comparison}
\end{figure}

\begin{figure}[ht!]
\centering
\includegraphics[width=0.9\columnwidth]{iclr2026/images/L1_30_causal_features_layer_distribution.pdf}
\caption{Layer-wise distribution of 2,787 causal features (L1--30). Safe features (green) and risky features (red) are shown stacked, with total numbers displayed above bars. The distribution reveals distinct layer-wise specialization: risky features concentrate in middle layers (L5--L18, peaking at L9 with 272 features), while safe features predominate in late layers (L25--L29, with 108--110 features each). This suggests that risky decision pathways are processed in middle network layers, whereas safe decision-making is primarily handled in later layers.}
\label{fig:causal-features-layer-distribution}
\end{figure}

\textbf{Finding 3: Layer-wise distribution patterns}

The 2,787 causal features exhibit distinct layer-wise specialization within the network architecture (Figure~\ref{fig:causal-features-layer-distribution}). Risky features concentrate in middle layers (5--18), peaking at Layer 9 with 272 features, while safe features predominate in late layers (25--29), with 108--110 features each. Despite being outnumbered overall (640 safe vs 2,147 risky), safe features' concentration in final decision layers suggests a late-stage safety mechanism that can override earlier risk-promoting signals when activated sufficiently.

\subsection{Summary}

Our mechanistic analysis reveals that LLMs encode distinct neural patterns for risk decisions across their entire architecture: 83,684 features (68.1\% of all hidden state dimensions) differentiate bankruptcy from safe stopping, of which 2,787 exhibit consistent bidirectional causal control. Activation patching shows safe features reduce bankruptcy by 19.4\% while risky features increase it by 17.0\%. These causal features exhibit layer-wise specialization—risky features concentrate in middle layers (5--18) while safe features cluster in late layers (25--29). The numerical predominance of risky features (2,147 vs 640) suggests that gambling behavior arises from extensive middle-layer processing, which can be overridden by concentrated late-layer safety mechanisms. These findings demonstrate that risk-taking behaviors arise from specific, manipulable neural mechanisms distributed throughout the network, enabling targeted interventions.

% Through this LLaMA-3.1-8B mechanistic analysis, we empirically demonstrated that LLMs possess distinguishable decision-making mechanisms at the internal neural network level in gambling-like risk situations. The 3,365 statistically significant features extracted from 6,400 independent experiments show differences in neural network representations between bankruptcy and voluntary stopping, and the discovery of 149 causal features proves the existence of specific neural mechanisms that regulate gambling behavior within LLMs.

% In causality verification through activation patching, safe feature patching induced a 29.6\% increase in stopping rate in safe contexts, and a 28.4\% increase in stopping rate and 14.2\% decrease in bankruptcy rate in risky contexts, showing strong risk aversion effects. Conversely, risky feature patching induced decreased stopping rates and increased bankruptcy rates, showing contrasting effects. In particular, causal features concentrated in Layers 29--31 (55.6\%) demonstrate that risk decision-making is processed in middle-to-late network layers. This proves that specific internal features can directly modulate risk behavior in LLMs beyond simple correlation, and presents the possibility of understanding and controlling LLM risk behavior at the neural network level through mechanistic interpretability methodology.