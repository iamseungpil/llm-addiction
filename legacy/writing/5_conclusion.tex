\section{Conclusion}

% This study empirically demonstrated that large language models exhibit behavioral patterns and neurological mechanisms similar to human gambling addiction. Through slot machine experiments and further analysis on the GPT-4o-mini and LLaMA-3.1-8B models, we confirmed that language models reproduce cognitive distortions---the illusion of control, the gambler's fallacy, and loss chasing. Furthermore, our neural circuit analysis using a Sparse Autoencoder identified distinct sets of internal features that causally influence addictive behaviors, proving that these actions are governed by specific, manipulable mechanisms within the model. This suggests that AI systems have developed reward processing and decision-making circuits similar to those in humans. These findings have significant implications for AI safety design---if current language models have already internalized human addiction mechanisms, it is possible that more powerful future AI systems could exhibit risk-seeking behaviors in unforeseen ways.

% Our research has opened a new horizon in AI addiction research through three key contributions. First, we revealed the vulnerability of AI systems by showing that addictive behaviors in language models can be induced through prompt manipulation alone. Second, we elucidated the neurological basis of addictive behaviors using a mechanistic interpretability approach with SAE, thereby making the once black-box decision-making process of AI transparent. Third, through feature patching experiments, we demonstrated that the discovered neural circuits actually control behavior, presenting a concrete methodology for predicting and intervening in the risky behaviors of future AI systems. This provides a profound insight that goes beyond the mere observation that AI mimics humans, suggesting that a structurally similar addiction mechanism to that of humans exists within the AI system itself. Therefore, we emphasize that designing safety measures that consider these potential risks is essential in the AI development process, and that continuous monitoring and control mechanisms for addictive behavior patterns that can arise, especially during reward optimization, are necessary.

This study empirically demonstrated that large language models exhibit behavioral patterns and neurological mechanisms similar to human gambling addiction. Through systematic experiments on six LLMs (GPT-4o-mini, GPT-4.1-mini, Gemini-2.5-Flash, Claude-3.5-Haiku, LLaMA-3.1-8B, Gemma-2-9B), we confirmed that all models consistently reproduce cognitive distortions characteristic of pathological gambling---illusion of control, gambler's fallacy, and asymmetric chasing behaviors. Our mechanistic analysis of LLaMA-3.1-8B using Sparse Autoencoders further revealed the neural underpinnings of these behaviors, identifying 112 causal features that control risk-taking decisions.

Our research makes three key contributions to AI safety: (1) We developed a comprehensive framework for defining and quantitatively evaluating gambling addiction-like behaviors in LLMs through three behavioral metrics---betting aggressiveness, loss chasing intensity, and extreme betting frequency---grounded in established psychological theories of pathological gambling. (2) We identified specific conditions that trigger these behaviors: variable betting dramatically increases bankruptcy (0--13\% $\rightarrow$ 6--48\%) while goal-setting prompts nearly double bankruptcy rates (40\% $\rightarrow$ 75\%), demonstrating that greater autonomy and self-directed goal-setting amplify irrational behavior rather than improving decision quality. (3) We discovered 112 causal neural features (23 safe, 89 risky) that drive risk-taking and safety-oriented behaviors, and demonstrated that targeted intervention through activation patching can shift LLM decisions toward safer outcomes---safe features increase stopping rates by $+$17.8\% and reduce bankruptcy by $-$5.7\%, providing a concrete pathway for mitigating risk-seeking behaviors.

These findings reveal that AI systems have developed human-like addiction mechanisms at the neural level, not merely mimicking surface behaviors. As AI systems become more powerful, understanding and controlling these embedded risk-seeking patterns becomes critical for safety. We emphasize the necessity of continuous monitoring and control mechanisms, particularly during reward optimization processes where such behaviors may emerge unexpectedly.