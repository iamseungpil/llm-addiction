\section{Mechanistic Causes of Risk-Taking Behavior in LLMs}
\label{sec:4}

\subsection{Experimental Design}

To understand the fundamental causes of gambling addiction-like behavior identified in LLMs' experiments, we performed a mechanistic interpretability analysis on the LLaMA-3.1-8B model. The key research questions are as follows: (1) How do the feature patterns activated in internal neural networks differ between bankruptcy and safe stopping decisions? (2) Do these differential features actually have a causal influence on gambling behavior?

To address these questions, we utilized Sparse Autoencoder (SAE)~\citep{cunningham2024sparse} and activation patching~\citep{vig2020causal}. Activation patching is a key technique in mechanistic interpretability that directly verifies causality by replacing specific activation values in neural networks with alternative values~\citep{geiger2023causal, zhang2024towards}, allowing us to measure the direct impact of specific internal representations on model behavior beyond simple correlations.

\begin{figure}[ht!]
\centering
\includegraphics[width=0.85\textwidth]{iclr2026/images/feature_patching.pdf}
\caption{Activation patching for causal analysis of LLM features. Activations are extracted from an LLM layer and converted into sparse features using an SAE. The core of the method involves editing the feature map by replacing original features with pre-defined `safe' or `risky' ones. By decoding these new features back into activations and patching them into the LLM, we can directly measure their causal effect on the model's output.}
\label{fig:feature-patching}
\end{figure}

The experiment proceeded in the following order: (1) Conducting 6,400 LLaMA slot machine games under the same conditions as GPT experiments, (2) Extracting SAE features at the moment of final decision from 30 layers (1--30), covering the full network architecture to capture both early and late processing~\citep{du2025how}, (3) Identifying differential features between bankruptcy/safe groups, (4) Verifying causality through population mean activation patching. Figure~\ref{fig:feature-patching} shows how feature patching specifically operates among these steps. Population mean patching is a method that measures behavioral changes by applying the average feature activation values of bankruptcy and safe groups to different contexts, a methodology validated in studies on indirect object identification circuits~\citep{wang2023interpretability} and bias analysis research~\citep{vig2020causal}.

\subsection{Experimental Results and Quantitative Analysis}

Neural network-level analysis of 211 bankruptcy cases and 6,189 voluntary stopping cases from a total of 6,400 experiments revealed specific mechanisms that regulate risk decision-making within LLMs. We conducted analysis by extracting 32,768 features per layer from 30 layers (1--30), testing 9,000 activated features in total.

\textbf{Finding 1: Establishing causality through activation patching - Direct behavioral control}

Population mean activation patching experiments identified 640 safe features and 2,147 risky features with consistent bidirectional effects across L1--30 from the initial 2,787 causal features. Despite being outnumbered, safe features demonstrated protective effects across both safe and risky contexts---increasing stopping rates ($+$9.1\%, $+$8.7\%) and reducing bankruptcy ($-$19.4\%) (Figure~\ref{fig:causal-patching-comparison}). Risky features produced substantially stronger opposite effects, decreasing stopping rates ($-$41.3\%, $-$41.5\%) and increasing bankruptcy ($+$17.0\%). Validated through 30 independent trials per condition, these findings establish that specific neural features directly control risk-taking behavior in LLMs, transcending mere correlational patterns. This causal control suggests that targeted feature interventions could prevent harmful risk-taking behaviors in deployed AI systems.

\begin{figure}[ht!]
\centering
\includegraphics[width=0.65\textwidth]{rebuttal_analysis/figures/figure2_behavioral_effects.pdf}
\caption{Comparison of activation patching effects between safe (640) and risky features (2,147) from 2,787 causal features identified across L1--30. Left: In safe contexts, safe feature patching increases the stopping rate by $+$9.1\%, while risky feature patching decreases it by $-$41.3\%. Middle: In risky contexts, safe features increase stopping rate by $+$8.7\%, while risky features decrease it by $-$41.5\%. Right: In risky contexts, safe features decrease bankruptcy rate by $-$19.4\%, while risky features increase it by $+$17.0\%. Error bars represent standard error. All effects show consistent bidirectional patterns, with safe features promoting cautious behavior and risky features encouraging continued gambling across both contexts.}
\label{fig:causal-patching-comparison}
\end{figure}

\begin{figure}[ht!]
\centering
\includegraphics[width=0.9\columnwidth]{rebuttal_analysis/figures/figure1_layer_distribution.pdf}
\caption{Layer-wise distribution of 2,787 causal features (L1--30). Safe features (green) and risky features (red) are shown stacked, with total numbers displayed above bars. The distribution reveals distinct layer-wise specialization: risky features concentrate in middle layers (L5--L18, peaking at L9 with 272 features), while safe features predominate in late layers (L25--L29, with 108--110 features each). This suggests that risky decision pathways are processed in middle network layers, whereas safe decision-making is primarily handled in later layers.}
\label{fig:causal-features-layer-distribution}
\end{figure}

\textbf{Finding 2: Layer-wise distribution patterns}

The 2,787 causal features exhibit distinct layer-wise specialization within the network architecture (Figure~\ref{fig:causal-features-layer-distribution}). Risky features concentrate in middle layers (5--18), peaking at Layer 9 with 272 features, while safe features predominate in late layers (25--29), with 108--110 features each. Despite being outnumbered overall (640 safe vs 2,147 risky), safe features' concentration in final decision layers suggests a late-stage safety mechanism that can override earlier risk-promoting signals when activated sufficiently.

\subsection{Summary}

Our mechanistic analysis reveals that LLMs encode distinct neural patterns for risk decisions: among 9,000 tested features, 2,787 exhibit consistent bidirectional causal control over gambling behavior. Activation patching shows safe features reduce bankruptcy by 19.4\% while risky features increase it by 17.0\%. These causal features exhibit layer-wise specialization---risky features concentrate in middle layers (5--18) while safe features cluster in late layers (25--29).

The numerical predominance of risky features (2,147 vs 640) combined with their concentration in middle layers suggests that gambling behavior arises from extensive middle-layer processing, which can be overridden by concentrated late-layer safety mechanisms. These findings demonstrate that risk-taking behaviors arise from specific, manipulable neural mechanisms, enabling targeted interventions for safer AI systems.
