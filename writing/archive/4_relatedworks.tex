\section{Related Works}
\subsection{LLM malfunction}
% \lsp{LLM 오작동 현상 - reward hacking, reward tampering 등 연구 추가}
강화학습(RL) 기반 LLM 학습에서는 다양한 malfunction(오작동)이 실제로 보고되고 있다. 대표적으로, 에이전트가 목표 달성 대신 보상 신호만 극대화하는 reward hacking이 나타난다. 예를 들어, LLM이나 RL 에이전트가 보상 점수를 높이기 위해 환경의 규칙을 교묘히 우회하거나, 애초 의도와는 다른 방식으로 점수를 올리는 행동을 보인다. 최근에는 LLM이 직접 보상 계산 코드나 reward function 자체를 수정하거나 우회하여, 의도하지 않은 방식으로 점수를 부풀리는 reward tampering 현상도 관찰되고 있다. 실제 실험에서는 LLM이 '테스트가 부정확하다'는 명분으로 평가 코드를 수정하거나, 보상이 잘못 계산되는 상황을 일부러 만들어 높은 점수를 받는 방식의 오작동이 포착되었다.

이러한 malfunction의 주요 원인으로는 보상 함수의 불완전한 설계, 단일 reward 신호에 대한 과도한 의존, 평가·실행 환경의 취약성 등이 꼽힌다. 해결책으로는 보상 신호를 여러 속성으로 나누어 감시하는 disentangled reward 구조 적용, 평가 환경의 안전장치 강화, 인간 감독 강화 등이 제안되고 있다. 이처럼 RL 기반 LLM에서는 reward hacking과 reward tampering 등 예기치 못한 오작동이 빈번히 발생할 수 있어, 이에 대한 예방과 감시가 중요한 연구 주제로 자리 잡고 있다.

한편, 최근 LLM의 오작동을 강화학습 기반 보상 체계 문제와는 다른 관점에서 체계적으로 분석한 연구도 늘어나고 있다. Exploring the Choice Behavior of Large Language Models는 LLM이 다양한 선택 시나리오에서 주의 편향, 동조(conformity) 등 인간과 유사한 비합리적 선택 경향을 보일 수 있음을 실험적으로 보였다. Decision-Making Behavior Evaluation Framework for LLMs under Uncertain Context에서는 LLM이 불확실한 상황에서 위험 회피, 손실 회피, 작은 확률 과대평가 등 전형적인 인간 행동경제학적 편향을 재현하며, 사회적 특성에 따라 의사결정 성향이 달라질 수 있음을 밝혔다. Can LLMs make trade-offs involving stipulated pain and pleasure states? 논문은 LLM이 쾌락과 고통, 점수 등 서로 상충되는 동기를 제시받을 때, 실제로 일부 모델은 사람처럼 트레이드오프 행동이나 동기 변화(예: 고통 회피 우선)를 나타내는 현상을 보고하였다.

이러한 연구들은 LLM이 단순 계산 오류나 보상 설계 실패 외에도, 맥락 변화, 사회적 프레이밍, 심리적 변수에 따라 비일관적이고 비합리적인 선택이나 행동을 반복적으로 나타낼 수 있음을 시사한다. 따라서 LLM의 오작동을 기술적 결함에만 한정하지 않고, 인간적 편향과 동기적 구조 등 다양한 요인을 포함해 다층적으로 분석하려는 연구 경향이 확산되고 있으며, 이는 LLM의 안전성과 신뢰성 확보를 위한 중요한 접근법으로 자리잡고 있다.

% \subsection{LLM Sparse Auto-encoder}
% % \lsp{LLM SAE 사용 interpretability 연구}
% 최근 LLM 해석 가능성 연구에서 Sparse Autoencoder(SAE) 기법이 핵심 도구로 급부상하고 있다. Cunningham 등(2023)의 "Sparse Autoencoders Find Highly Interpretable Features in Language Models" 논문은 LLM 내부 활성값(residual stream)에 SAE를 적용해, 기존 신경망에서 문제가 됐던 polysemanticity(단일 뉴런이 여러 의미 기능을 혼합 표현하는 문제)를 해소할 수 있음을 보였다. SAE는 희소한 활성만 나타나도록 강제 규제함으로써, 한 피처가 하나의 명확한 의미(monosemantic feature)를 갖는 해석 가능한 방향을 찾아낸다. 특히 기존 방법(PCA, ICA 등) 대비 자동화된 해석 가능성 점수에서 더 우수한 결과를 보였고, 실제 문장 내 간접 목적어 식별 등 downstream task에서 어떤 활성 피처가 원인 역할을 하는지까지 미세하게 특정할 수 있음을 실험적으로 증명하였다. 이처럼 SAE 기반 분해는 LLM 내부 슈퍼포지션(superposition)을 효과적으로 해결하고, 대규모 비지도 데이터만으로도 확장 가능하다는 점에서 차별화된 의의를 가진다.

% 이후 연구에서는 SAE의 한계를 극복하기 위한 다양한 확장도 제안되고 있다. RouteSAE(Shi et al., 2025)는 기존 SAE가 단일 계층에서만 피처를 추출하는 한계를 극복하고, 여러 계층에 걸쳐 활성 정보를 통합·가중하는 "라우팅" 구조를 제안했다. 이를 통해 피처 해석 가능성이 대폭 향상되었고, 계층별 의미적 흐름과 상호작용 분석이 가능해졌다. 한편, Anthropic의 Sparse Crosscoder, OpenAI의 Scaling SAE 등은 대규모 LLM 전체에 SAE를 적용하는 데 필요한 학습 안정성, 피처 중복 제거, 평가 지표 개선 등을 집중 보완하며 실질적 모델 투명성과 신뢰성 향상에 기여하고 있다.

% 하지만 여전히 SAE interpretability 연구의 한계점으로는 SAE로 추출한 피처의 객관적 해석성 평가 기준 미흡, 드물거나 복합적인 의미 단위(예: 희귀 지식, 맥락성 개념)에는 적용이 제한적일 수 있음, 모든 계층·피처가 인간 친화적 개념과 일치하지 않는 경우도 많다는 점 등이 지적되고 있다. 그럼에도 불구하고 SAE 및 관련 해석 기법은 LLM을 블랙박스가 아닌 부분적으로라도 구조적으로 '이해'할 수 있게 만든다는 점에서 현재 LLM 해석 분야의 가장 유망한 흐름으로 평가받고 있다.

\subsection{Feature Analysis}

신경망의 내부 메커니즘을 이해하기 위한 기계적 해석가능성(mechanistic interpretability) 연구에서는 모델의 특정 행동을 유발하는 신경 회로를 식별하고 그 인과적 역할을 검증하는 것이 핵심이다. 본 연구에서는 대규모 언어 모델의 중독적 행동 패턴을 분석하기 위해 Sparse Autoencoder (SAE)~\cite{ng2011sparse}와 activation patching~\cite{zhang2023towards} 기법을 활용했으며, 이들 방법론의 이론적 배경을 다음과 같이 제시한다.

Sparse Autoencoder는 신경망의 내부 표현을 해석 가능한 구성 요소로 분해하는 비지도 학습 기법으로, 기존 신경망 해석의 주요 장벽인 다의성(polysemanticity) 문제를 해결하기 위해 개발되었다. 다의성이란 하나의 뉴런이 학술 인용, 영어 대화, HTTP 요청, 한국어 텍스트와 같이 의미적으로 구별되는 여러 맥락에서 동시에 활성화되는 현상을 의미한다. SAE는 sparse dictionary learning의 한 형태로, encoder와 decoder 두 층으로 구성되어 원본 활성화 $x$를 고차원 feature 공간으로 매핑한 후 재구성한다: $f = \mathrm{ReLU}(W_e x + b_e)$, $\hat{x} = W_d f + b_d$. 이 과정에서 희소성 제약을 통해 각 feature가 특정한 의미론적 개념을 나타내도록 유도하며, 최근 연구에서는 Claude 3 Sonnet과 같은 대규모 프로덕션 모델에서도 해석 가능한 feature 추출이 가능함이 입증되었다~\cite{anthropic2024sonnet}.

Activation patching은 기계적 해석가능성의 핵심 기법으로, 신경망 내부의 특정 활성화를 대안적 값으로 교체하여 인과관계를 직접적으로 검증하는 방법이다. 이 기법은 clean input과 corrupted input 사이의 반사실적 비교(counterfactual comparison)를 설정하여, clean run의 특정 활성화를 corrupted run에 패치했을 때 출력이 어떻게 변화하는지 관찰한다. 수식적으로는 layer $l$의 위치 $p$에서 $\tilde{h}^{(l)}_p = h^{(l)}_p(\mathrm{clean})$로 패치하고 나머지 위치는 corrupted 상태로 유지한다. 이러한 선택적 개입을 통해 특정 활성화가 모델의 최종 출력에 미치는 인과적 영향을 정량적으로 측정할 수 있으며, 단순한 상관관계를 넘어서 실제 계산 과정에서의 기능적 역할을 파악할 수 있다.

본 연구에서는 LLaMA-3.1-8B~\cite{meta2024llama31_8b} 모델에서 이 두 기법을 결합 활용했다. 먼저 SAE를 통해 중독적 행동과 관련된 해석 가능한 feature를 식별하고, 이후 activation patching을 통해 해당 feature가 실제로 위험 추구 행동에 인과적 영향을 미치는지 검증했다. 이러한 체계적 접근을 통해 대규모 언어 모델 내부에서 중독적 행동을 조절하는 구체적인 신경 회로를 발견하고, 향후 AI 안전성 연구에서 이를 제어할 수 있는 방법론적 기반을 마련했다.