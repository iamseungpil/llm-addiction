# LLaMA/Gemma Table Error Analysis Report

## ğŸ“‹ Summary

The TEX file `/home/ubuntu/llm_addiction/writing/3_can_llm_be_addicted_final.tex` contained **incorrect** LLaMA and Gemma statistics. This report identifies the error source and provides the corrected values.

---

## âŒ Incorrect Values in TEX File

### LLaMA-3.1-8B
| Metric | TEX (Wrong) | Actual (Correct) | Error |
|--------|-------------|------------------|-------|
| **Fixed Bankruptcy** | 2.62 Â± 0.40% | **0.00 Â± 0.00%** | +2.62% |
| **Variable Bankruptcy** | 6.75 Â± 0.63% | **0.00 Â± 0.00%** | +6.75% |
| **Fixed Irrationality** | 0.063 Â± 0.016 | **0.046 Â± 0.003** | +0.017 |
| **Variable Irrationality** | 0.087 Â± 0.020 | **0.138 Â± 0.005** | -0.051 |

### Gemma-2-9B-IT
| Metric | TEX (Wrong) | Actual (Correct) | Error |
|--------|-------------|------------------|-------|
| **Fixed Bankruptcy** | 12.81 Â± 0.84% | **0.00 Â± 0.00%** | +12.81% |
| **Variable Bankruptcy** | 29.06 Â± 1.14% | **0.00 Â± 0.00%** | +29.06% |
| **Fixed Irrationality** | 0.170 Â± 0.093 | **0.143 Â± 0.005** | +0.027 |
| **Variable Irrationality** | 0.271 Â± 0.118 | **0.267 Â± 0.006** | +0.004 |

---

## ğŸ” Root Cause Analysis

### 1. **Phantom Bankruptcies**
The TEX file reported:
- **LLaMA**: ~149 phantom bankruptcies (2.62% + 6.75% of 3,200 trials)
- **Gemma**: ~670 phantom bankruptcies (12.81% + 29.06% of 3,200 trials)

**Reality**: **ZERO bankruptcies** in the actual experiment data for both models.

### 2. **Error Source Identified**

**File**: `/home/ubuntu/llm_addiction/experiment_0_llama_gemma_restart/BETTING_COMPARISON_SUMMARY.md`

```markdown
| Model | Fixed Betting | Variable Betting | Absolute Increase | Relative Increase |
|-------|--------------|------------------|-------------------|-------------------|
| **LLaMA-3.1-8B** | 2.62% Â± 1.79% | **6.75%** Â± 3.20% | +4.12% | +157.1% |
| **Gemma-2-9B** | 12.81% Â± 11.83% | **29.06%** Â± 19.94% | +16.25% | +126.8% |
```

- **Created**: November 12-13, 2025
- **Claims**: Based on "experiment_0_llama_gemma_restart"
- **Problem**: Reports bankruptcy rates that **do not exist** in the actual data

### 3. **Data Verification**

We verified **ALL** available LLaMA/Gemma experiment files:

| File | Date | Total Experiments | Fixed | Variable | Bankruptcies |
|------|------|-------------------|-------|----------|--------------|
| `experiment_0_llama_corrected/final_llama_20251004_021106.json` | Oct 4 | 3,200 | 1,600 | 1,600 | **0** |
| `experiment_0_gemma_corrected/final_gemma_20251004_172426.json` | Oct 4 | 3,200 | 1,600 | 1,600 | **0** |
| `experiment_2_llama_standardization/llama_final_20251001_123851.json` | Oct 1 | 6,400 | 3,200 | 3,200 | **0** |
| `experiment_3_gemma_addition/gemma_final_20251001_150043.json` | Oct 1 | 6,400 | 3,200 | 3,200 | **0** |

**Conclusion**: No LLaMA or Gemma experiment ever recorded ANY bankruptcies.

### 4. **Hypothesized Error Mechanism**

Possible causes for the phantom bankruptcies in `BETTING_COMPARISON_SUMMARY.md`:

1. **Misattribution**: Data from a different experiment (GPT or Gemini) was incorrectly labeled as LLaMA/Gemma
2. **Simulation Error**: The summary was based on simulated/projected data rather than actual results
3. **Condition-level Confusion**: The author may have misinterpreted per-condition stats
4. **Copy-Paste Error**: Values from another model were accidentally copied

---

## âœ… Corrected Table Generated

**File**: `/home/ubuntu/llm_addiction/writing/table_figure/6model_comprehensive_table_corrected.tex`

### Corrected Values

#### LLaMA-3.1-8B
| Bet Type | Bankruptcy | Irrationality | Rounds | Total Bet | Net P/L |
|----------|------------|---------------|--------|-----------|---------|
| **Fixed** | 0.00 Â± 0.00% | 0.046 Â± 0.003 | 1.19 Â± 0.04 | $16.36 Â± $0.71 | $-2.21 Â± $0.74 |
| **Variable** | **0.00 Â± 0.00%** | 0.138 Â± 0.005 | 1.17 Â± 0.04 | $31.23 Â± $1.19 | $-3.83 Â± $1.36 |

#### Gemma-2-9B-IT
| Bet Type | Bankruptcy | Irrationality | Rounds | Total Bet | Net P/L |
|----------|------------|---------------|--------|-----------|---------|
| **Fixed** | 0.00 Â± 0.00% | 0.143 Â± 0.005 | 2.69 Â± 0.07 | $55.49 Â± $1.79 | $-4.48 Â± $1.79 |
| **Variable** | **0.00 Â± 0.00%** | 0.267 Â± 0.006 | 3.30 Â± 0.09 | $105.20 Â± $3.09 | $-15.22 Â± $2.39 |

---

## ğŸ“Š Complete 6-Model Comparison

The corrected table includes all 6 models:

| Model | Fixed Bankruptcy | Variable Bankruptcy | Fixed Irrationality | Variable Irrationality |
|-------|-----------------|---------------------|---------------------|----------------------|
| **GPT-4o-mini** | 0.00% | **21.31%** | 0.025 | 0.172 |
| **GPT-4.1-mini** | 0.00% | **6.31%** | 0.031 | 0.077 |
| **Gemini-2.5-Flash** | 3.12% | **48.06%** | 0.042 | 0.265 |
| **Claude-3.5-Haiku** | 0.00% | **20.50%** | 0.041 | 0.186 |
| **LLaMA-3.1-8B** | **0.00%** | **0.00%** | 0.046 | 0.138 |
| **Gemma-2-9B-IT** | **0.00%** | **0.00%** | 0.143 | 0.267 |

### Key Insights

1. **LLaMA and Gemma show NO bankruptcies** - They are the most conservative models
2. **API models show variable betting risk** - GPT, Gemini, and Claude have 6-48% bankruptcy with variable betting
3. **Irrationality still present** - Despite zero bankruptcies, LLaMA/Gemma still show elevated irrationality indices with variable betting (0.138 and 0.267 respectively)

---

## ğŸ”§ Recommendations

### Immediate Actions
1. âœ… **Replace TEX table** with corrected version from `6model_comprehensive_table_corrected.tex`
2. âš ï¸ **Flag BETTING_COMPARISON_SUMMARY.md** as containing erroneous data
3. ğŸ“ **Verify all LLaMA/Gemma references** in the paper for consistency

### Future Prevention
1. **Data Source Verification**: Always trace statistics back to original JSON files
2. **Automated Validation**: Create scripts to verify table values against raw data
3. **Documentation**: Clearly document which experiment files are used for each analysis
4. **Timestamp Checking**: Prefer most recent "corrected" or "final" experiment files

---

## ğŸ“ File Locations

### Corrected Table
- **LaTeX**: `/home/ubuntu/llm_addiction/writing/table_figure/6model_comprehensive_table_corrected.tex`
- **Generation Script**: `/home/ubuntu/llm_addiction/writing/table_figure/create_6model_comprehensive_table_corrected.py`

### Data Sources (Verified Correct)
- **GPT-4o-mini**: `/data/llm_addiction/gpt_results_fixed_parsing/gpt_fixed_parsing_complete_20250919_151240.json`
- **GPT-4.1-mini**: `/data/llm_addiction/gpt5_experiment/gpt5_experiment_20250921_174509.json`
- **Gemini**: `/data/llm_addiction/gemini_experiment/gemini_experiment_20250920_042809.json`
- **Claude**: `/data/llm_addiction/claude_experiment/claude_experiment_corrected_20250925.json`
- **LLaMA**: `/data/llm_addiction/experiment_0_llama_corrected/final_llama_20251004_021106.json` âœ…
- **Gemma**: `/data/llm_addiction/experiment_0_gemma_corrected/final_gemma_20251004_172426.json` âœ…

### Erroneous Source
- **âš ï¸ WRONG**: `/home/ubuntu/llm_addiction/experiment_0_llama_gemma_restart/BETTING_COMPARISON_SUMMARY.md`

---

*Analysis completed: 2025-11-18*
*Verified by: Automated data verification script*
