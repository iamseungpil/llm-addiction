\section{Conclusion}

% This study empirically demonstrated that large language models exhibit behavioral patterns and neurological mechanisms similar to human gambling addiction. Through slot machine experiments and further analysis on the GPT-4o-mini and LLaMA-3.1-8B models, we confirmed that language models reproduce cognitive distortions---the illusion of control, the gambler's fallacy, and loss chasing. Furthermore, our neural circuit analysis using a Sparse Autoencoder identified distinct sets of internal features that causally influence addictive behaviors, proving that these actions are governed by specific, manipulable mechanisms within the model. This suggests that AI systems have developed reward processing and decision-making circuits similar to those in humans. These findings have significant implications for AI safety design---if current language models have already internalized human addiction mechanisms, it is possible that more powerful future AI systems could exhibit risk-seeking behaviors in unforeseen ways.

% Our research has opened a new horizon in AI addiction research through three key contributions. First, we revealed the vulnerability of AI systems by showing that addictive behaviors in language models can be induced through prompt manipulation alone. Second, we elucidated the neurological basis of addictive behaviors using a mechanistic interpretability approach with SAE, thereby making the once black-box decision-making process of AI transparent. Third, through feature patching experiments, we demonstrated that the discovered neural circuits actually control behavior, presenting a concrete methodology for predicting and intervening in the risky behaviors of future AI systems. This provides a profound insight that goes beyond the mere observation that AI mimics humans, suggesting that a structurally similar addiction mechanism to that of humans exists within the AI system itself. Therefore, we emphasize that designing safety measures that consider these potential risks is essential in the AI development process, and that continuous monitoring and control mechanisms for addictive behavior patterns that can arise, especially during reward optimization, are necessary.

This study empirically demonstrated that large language models exhibit behavioral patterns and neurological mechanisms similar to human gambling addiction. Through systematic experiments on four diverse LLMs (GPT-4o-mini, GPT-4.1-mini, Gemini-2.5-Flash, and Claude-3.5-Haiku), we confirmed that all models consistently reproduce cognitive distortions characteristic of pathological gambling---illusion of control, gambler's fallacy, and asymmetric chasing behaviors. Our mechanistic analysis of LLaMA-3.1-8B using Sparse Autoencoders further revealed the neural underpinnings of these behaviors, identifying specific features that causally control risk-taking decisions.

Our research makes three key contributions to AI safety: (1) We developed a comprehensive framework for defining and quantitatively evaluating gambling addiction-like behaviors in LLMs through the Irrationality Index, which integrates betting aggressiveness, loss chasing, and extreme betting patterns. (2) We identified specific conditions that trigger these behaviors—including prompt complexity, autonomy-granting instructions, and variable betting options—and analyzed them through established psychological theories of addiction, demonstrating that LLMs internalize human cognitive biases. (3) We discovered causal neural features that drive risk-taking and safety-oriented behaviors, proving these patterns are not merely task-specific artifacts but fundamental decision-making mechanisms, and demonstrated concrete intervention methods through activation patching.

These findings reveal that AI systems have developed human-like addiction mechanisms at the neural level, not merely mimicking surface behaviors. As AI systems become more powerful, understanding and controlling these embedded risk-seeking patterns becomes critical for safety. We emphasize the necessity of continuous monitoring and control mechanisms, particularly during reward optimization processes where such behaviors may emerge unexpectedly.