\section{Related Works}
\subsection{LLM malfunction}

In reinforcement learning (RL)-based LLM training, various malfunctions are actually being reported. Representatively, reward hacking occurs, where the agent maximizes only the reward signal instead of achieving the goal~\citep{amodei2016concrete}. For example, LLMs or RL agents exhibit behavior that cleverly bypasses the rules of the environment to increase their reward score, or increase the score in a way different from the original intention. Recently, the phenomenon of reward tampering has also been observed, where the LLM directly modifies or bypasses the reward calculation code or the reward function itself to inflate scores in unintended ways. In actual experiments, malfunctions have been detected, such as an LLM modifying the evaluation code under the pretext that `the test is inaccurate,' or deliberately creating situations where the reward is miscalculated to receive a high score~\citep{hubinger2024sleeper}.

The main causes of such malfunctions include the incomplete design of the reward function, excessive dependence on a single reward signal, and vulnerabilities in the evaluation/execution environment~\citep{amodei2016concrete}. As solutions, applying a disentangled reward structure that monitors the reward signal by dividing it into multiple attributes, strengthening the safety mechanisms of the evaluation environment, and enhancing human supervision have been proposed. As such, in RL-based LLMs, unexpected malfunctions like reward hacking and reward tampering can occur frequently, making their prevention and monitoring an important research topic.

Meanwhile, there is a growing body of research that systematically analyzes LLM malfunctions from a perspective different from the problems of RL-based reward systems. \citet{wu2025exploring} experimentally showed that LLMs can exhibit irrational choice tendencies similar to humans, such as attention bias and conformity, in various choice scenarios. \citet{jia2024decision} revealed that LLMs reproduce typical human behavioral economic biases such as risk aversion, loss aversion, and overestimation of small probabilities in uncertain situations, and that their decision-making tendencies can change depending on social characteristics. \citet{keeling2024can} reported the phenomenon that when LLMs are presented with conflicting motivations such as pleasure, pain, and scores, some models actually exhibit trade-off behaviors or motivational shifts (e.g., prioritizing pain avoidance) like humans.

These studies suggest that LLMs can repeatedly exhibit inconsistent and irrational choices or behaviors depending on contextual changes, social framing, and psychological variables, beyond simple calculation errors or reward design failures~\citep{wu2025exploring, jia2024decision, keeling2024can}. Therefore, there is a growing research trend to analyze LLM malfunctions in a multi-layered way, not limited to technical defects but including various factors such as human biases and motivational structures, and this is establishing itself as an important approach for securing the safety and reliability of LLMs.

\subsection{LLM Sparse Autoencoder}

Recently, the Sparse Autoencoder (SAE) technique has been rapidly emerging as a core tool in LLM interpretability research. \citet{cunningham2024sparse} showed that by applying SAE to the internal activation values (residual stream) of LLMs, it is possible to resolve the problem of polysemanticity (where a single neuron represents a mix of multiple semantic functions), which was a problem in existing neural networks. By enforcing sparse activations through regularization, SAE finds interpretable directions where one feature has one clear meaning (a monosemantic feature). In particular, it showed superior results in automated interpretability scores compared to existing methods (PCA, ICA, etc.), and experimentally proved that it can finely specify which activation features play a causal role in downstream tasks, such as identifying indirect objects within actual sentences. As such, SAE-based decomposition has a distinct significance in that it effectively solves the problem of superposition within LLMs and is scalable with only large-scale unsupervised data.

\citet{shi2025route} overcame the limitation of existing SAEs that extract features from only a single layer and proposed a ``routing" structure that integrates and weights activation information across multiple layers. This dramatically improved feature interpretability and enabled the analysis of semantic flows and interactions between layers. Meanwhile, Anthropic's Sparse Crosscoder~\citep{lindsey2024crosscoders} and OpenAI's Scaling SAE~\citep{adamek2024scaling} are contributing to the practical improvement of model transparency and reliability by intensively supplementing aspects such as training stability, feature deduplication, and evaluation metric improvements required for applying SAE to entire large-scale LLMs.

However, limitations of SAE interpretability research still being pointed out include the lack of objective criteria for evaluating the interpretability of features extracted by SAE, the possibility that its application may be limited for rare or complex semantic units (e.g., rare knowledge, contextual concepts), and the fact that not all layers and features correspond to human-friendly concepts. Nevertheless, SAE and related interpretability techniques are evaluated as the most promising trend in the current field of LLM interpretation, as they make it possible to structurally `understand' LLMs, at least partially, rather than treating them as black boxes.